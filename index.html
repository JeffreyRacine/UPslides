<!DOCTYPE html>
<html lang="en"><head>
<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/tabby.min.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.433">

  <meta name="author" content="Jeffrey S. Racine">
  <title>Nonparametric Workshop</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="custom.css">
  <link href="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script>
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
  </script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Nonparametric Workshop</h1>
  <p class="subtitle">University of Pretoria</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Jeffrey S. Racine 
</div>
        <p class="quarto-title-affiliation">
            McMaster University
          </p>
    </div>
</div>

  <p class="date">Monday, August 14, 2023</p>
</section>
<section id="slide-pro-tips" class="title-slide slide level1 smaller center">
<h1>Slide Pro-Tips</h1>
<div>
<ul>
<li><p>Link to slides - <a href="https://jeffreyracine.github.io/UPslides">jeffreyracine.github.io/UPslides</a> (case sensitive) (<a href="https://jeffreyracine-github-io.translate.goog/UPslides/?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en&amp;_x_tr_pto=wapp#/title-slide">Google Translate</a>)</p></li>
<li><p>View <strong>full screen</strong> by pressing the F key (press the Esc key to revert)</p></li>
<li><p>Access <strong>navigation menu</strong> by pressing the M key (navigation menu X to close)</p></li>
<li><p><strong>Advance</strong> using arrow keys</p></li>
<li><p><strong>Zoom</strong> in by holding down the Alt key in Windows, Opt key in macOS or Ctrl key in Linux, and clicking on any screen element (Alt/Opt/Ctrl click again to zoom out)</p></li>
<li><p>Use <strong>copy to clipboard</strong> button for R code blocks (upper right in block) to copy and paste into R/RStudio</p></li>
<li><p><strong>Export to a PDF</strong> by pressing the E key (wait a few seconds, then print [or print using system dialog], enable landscape layout, then save as PDF - press the E key to revert)</p></li>
<li><p>Enable drawing tools - chalk <strong>board</strong> by pressing the B key (B to revert), notes <strong>canvas</strong> by pressing the C key (C to revert), press the Del key to erase, press the D key to <strong>download drawings</strong></p></li>
</ul>
</div>
<aside class="notes">
<p>Encourage participants to print/save a PDF copy of the slides as there is no guarantee that this material will be there when they realize it might be useful.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section>
<section id="welcome" class="title-slide slide level1 center">
<h1>Welcome!</h1>
<ul>
<li class="fragment"><p>The website with install info for R, RStudio, and <span class="math inline">\(\rm\TeX\)</span> is</p>
<p><a href="https://jeffreyracine.github.io/UPworkshop" class="uri">https://jeffreyracine.github.io/UPworkshop</a></p></li>
<li class="fragment"><p>The GitHub repository with example code is</p>
<p><a href="https://github.com/JeffreyRacine/UPworkshop" class="uri">https://github.com/JeffreyRacine/UPworkshop</a></p></li>
<li class="fragment"><p>Don’t be shy, feel free to ask questions.</p></li>
<li class="fragment"><p>Let’s get going!</p></li>
</ul>
</section>
<section id="welcome-1" class="slide level2 center">
<h2>Welcome!</h2>
<ul>
<li class="fragment"><p>You will be guided through basic nonparametric kernel methods using R</p></li>
<li class="fragment"><p>You will also, if you are patient, be introduced to recently released tools for conducting reproducible research</p></li>
<li class="fragment"><p>No background knowledge of either nonparametric analysis or the R programming language is required</p></li>
<li class="fragment"><p>All software is available for <em>free</em> and is <em>open source</em></p></li>
<li class="fragment"><p>For more detailed descriptions see <span class="citation" data-cites="racine_2019">Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2019</a>)</span>, <span class="citation" data-cites="10.2307/41337225">Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2012</a>)</span>, and <span class="citation" data-cites="np">Hayfield and Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2008</a>)</span></p></li>
</ul>
</section></section>
<section id="overview" class="title-slide slide level1 center">
<h1>Overview</h1>
<ul>
<li class="fragment"><p>I hope that you leave this workshop armed with a modern set of data-analytic tools</p></li>
<li class="fragment"><p>Let’s briefly discuss the following:</p>
<ul>
<li class="fragment"><p><a href="https://www.r-project.org/about.html">What is R?</a></p></li>
<li class="fragment"><p><a href="https://posit.co/products/open-source/rstudio/">What is RStudio?</a></p></li>
<li class="fragment"><p><a href="https://en.wikipedia.org/wiki/Kernel_regression">What is Kernel Regression?</a></p></li>
<li class="fragment"><p><a href="https://quarto.org">What is Quarto?</a></p></li>
</ul></li>
</ul>
</section>

<section>
<section id="background-data-types" class="title-slide slide level1 center">
<h1>Background <br> Data Types</h1>

</section>
<section id="r-and-data-types" class="slide level2 center">
<h2>R and Data Types</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p><em>Pre-cast</em> data <em>prior to analysis</em> so R functions can do their job</p>
</div>
</div>
</div>
<ul>
<li class="fragment"><p>It is <em>crucial</em> to understand different data types in R</p></li>
<li class="fragment"><p>There are three R functions we will use: <code>numeric()</code>, <code>factor()</code>, and <code>ordered()</code></p></li>
<li class="fragment"><p>These correspond to data that are <em>numbers</em>, <em>unordered categories</em>, and <em>ordered categories</em>, respectively</p></li>
<li class="fragment"><p>Pay careful attention to the following example</p></li>
</ul>
</section>
<section id="r-and-data-types-1" class="slide level2 smaller center">
<h2>R and Data Types</h2>
<ul>
<li class="fragment"><p>Below is an example of how we <em>cast</em> our data in R <em>prior</em> to analyzing it</p></li>
<li class="fragment"><p>Copy and paste this into a new R file in RStudio, then run it line-by-line</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/datatype_d83fa5d5695131b8b9ef814f385ca613">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="do">## Generate some data: sex (unordered categorical), income (ordered categorical),</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="do">## and height (numeric)</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>sex <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">"Female"</span>,<span class="st">"Male"</span>),n,<span class="at">replace=</span><span class="cn">TRUE</span>,<span class="at">prob=</span><span class="fu">c</span>(.<span class="dv">4</span>,.<span class="dv">6</span>))</span>
<span id="cb1-5"><a href="#cb1-5"></a>income <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">"Low"</span>,<span class="st">"Middle"</span>,<span class="st">"High"</span>),n,<span class="at">replace=</span><span class="cn">TRUE</span>,<span class="at">prob=</span><span class="fu">c</span>(.<span class="dv">3</span>,.<span class="dv">5</span>,.<span class="dv">2</span>))</span>
<span id="cb1-6"><a href="#cb1-6"></a>height <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n,<span class="at">mean=</span><span class="dv">150</span>,<span class="at">sd=</span><span class="dv">20</span>)</span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="do">## Note - by default these variables may not be the data types we desire</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="fu">class</span>(sex);<span class="fu">class</span>(income);<span class="fu">class</span>(height)</span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="do">## income is already numeric(), but sex and height are character()</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="do">## sex is categorical and unordered, so cast as type factor()</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>sex <span class="ot">&lt;-</span> <span class="fu">factor</span>(sex)</span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="do">## income is categorical and ordered, but we need to ensure intended order (it</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="do">## will assume alphabetical ordering otherwise). Suppose you ignore it - let's</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="do">## see what happens when we just cast as type ordered() using defaults</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>income <span class="ot">&lt;-</span> <span class="fu">ordered</span>(income)</span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="do">## The levels are in alphabetical order, which we don't want</span></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="fu">levels</span>(income)</span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="do">## We shall reorder the ordered factor levels as intended using levels=...</span></span>
<span id="cb1-19"><a href="#cb1-19"></a>income <span class="ot">&lt;-</span> <span class="fu">ordered</span>(income,<span class="at">levels=</span><span class="fu">c</span>(<span class="st">"Low"</span>,<span class="st">"Middle"</span>,<span class="st">"High"</span>))</span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="fu">levels</span>(income)</span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="do">## Check data types again</span></span>
<span id="cb1-22"><a href="#cb1-22"></a><span class="fu">class</span>(sex);<span class="fu">class</span>(income);<span class="fu">class</span>(height)</span>
<span id="cb1-23"><a href="#cb1-23"></a><span class="do">## Note that with integers the default ordered() works fine</span></span>
<span id="cb1-24"><a href="#cb1-24"></a>x <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">1</span>),n,<span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb1-25"><a href="#cb1-25"></a>x <span class="ot">&lt;-</span> <span class="fu">ordered</span>(x)</span>
<span id="cb1-26"><a href="#cb1-26"></a><span class="fu">levels</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
</ul>
<aside class="notes">
<p>Pull up RStudio and patiently explain the interface then run the example - don’t waste time but realize this could be their first exposure to R/RStudio.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="rstudio-r-editor-interface" class="slide level2 center">
<h2>RStudio R Editor Interface</h2>

<img data-src="images/rstudio_rcode.png" class="r-stretch"></section></section>
<section>
<section id="nonparametric-essentials-parametric-vs.-nonparametric" class="title-slide slide level1 center">
<h1>Nonparametric Essentials <br> Parametric vs.&nbsp;Nonparametric</h1>

</section>
<section id="parametric-or-nonparametric" class="slide level2 center">
<h2>Parametric or Nonparametric?</h2>
<ul>
<li class="fragment"><p>Suppose we need to estimate the density <span class="math inline">\(f(x)\)</span> of some numeric random variable <span class="math inline">\(X\)</span></p></li>
<li class="fragment"><p>Suppose that we naively presume that our data is generated from the normal parametric family of distributions, namely <span class="math display">\[\begin{equation*}
    f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}
    \exp\left\{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right\}
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>This distributional family depends on two unknown parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, which can be estimated given data</p></li>
<li class="fragment"><p>That is, given estimates of the mean and standard deviation, then assuming the data is generated from this family <em>prior to estimation</em> we can construct an estimate of the density</p></li>
</ul>
</section>
<section id="parametric-or-nonparametric-1" class="slide level2 smaller center">
<h2>Parametric or Nonparametric?</h2>
<p>Let’s simulate some data where we know the distribution that generated the sample (we don’t have this luxury in applied settings), then estimate the density using the true density function (which is not known in applied settings)</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/parnpeval_cd08a24e84a865bb3b7e2e7ed9fef982">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="do">## Let's simulate a random numeric sample from the normal distribution</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb2-3"><a href="#cb2-3"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="do">## Let's sort the data so we can graph x versus dnorm(x,...) using lines (type="l")</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(x)</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="do">## Since we simulated the data, let's plot the true, known, parametric density</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="do">## (we can't do this with actual data because the density of such data is, in</span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="do">## general, unknown)</span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="fu">plot</span>(x,<span class="fu">dnorm</span>(x,<span class="at">mean=</span><span class="fu">mean</span>(x),<span class="at">sd=</span><span class="fu">sd</span>(x)),<span class="at">type=</span><span class="st">"l"</span>,<span class="at">ylab=</span><span class="st">"Parametric Density Estimate"</span>,<span class="at">xlab=</span><span class="st">"X"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/parnpeval-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="parametric-or-nonparametric-2" class="slide level2 center">
<h2>Parametric or Nonparametric?</h2>
<ul>
<li class="fragment"><p>The R function <code>shapiro.test(x)</code> tests for normality</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/shapiro_fabb9a0ef6d36ed89c18eac8d97b3d55">
<div class="cell-output-display">
<table style="width:38%;">
<caption>Shapiro-Wilk normality test: <code>x</code></caption>
<colgroup>
<col style="width: 23%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Test statistic</th>
<th style="text-align: center;">P value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0.9988</td>
<td style="text-align: center;">0.767</td>
</tr>
</tbody>
</table>
</div>
</div></li>
<li class="fragment"><p>This is <em>simulated normal</em> data, and we fail to reject the null</p>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>What if we got the null density wrong (i.e., what if we rejected the null)?</p>
</div>
</div>
</div></li>
</ul>
</section>
<section id="parametric-or-nonparametric-3" class="slide level2 center">
<h2>Parametric or Nonparametric?</h2>
<ul>
<li class="fragment"><p>Now let’s test normality for actual data, <code>eruptions</code> (we will discuss this data in detail shortly)</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/shapiroeruptions_01665f21088d57ed1830ce4fb9056600">
<div class="cell-output-display">
<table style="width:49%;">
<caption>Shapiro-Wilk normality test: <code>eruptions</code></caption>
<colgroup>
<col style="width: 23%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Test statistic</th>
<th style="text-align: center;">P value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0.8459</td>
<td style="text-align: center;">9.036e-16 * * *</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>Oh oh… I guess we rule out the normal parametric model… what next?</p>
</div>
</div>
</div></li>
</ul>
</section>
<section id="parametric-or-nonparametric-4" class="slide level2 center">
<h2>Parametric or Nonparametric?</h2>
<ul>
<li class="fragment"><p>Actual data (<span class="math inline">\(X_i\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>) is rarely drawn from the simple parametric distributions used in <em>sampling theory</em></p></li>
<li class="fragment"><p>Sampling theory describes <em>summary statistics</em>, e.g., <em>averages</em> of actual data, or <em>averages</em> of squared deviations, such as <span class="math display">\[\begin{align*}
\hat\mu&amp;=\frac{1}{n}\sum_{i=1}^n X_i\quad\text{ or }\quad\hat\sigma^2=\frac{1}{n-1}\sum_{i=1}^n (X_i -\hat\mu)^2&amp;
\end{align*}\]</span></p></li>
<li class="fragment"><p>But we need to estimate the <em>density of the actual data</em> (i.e., <span class="math inline">\(f(x)\)</span>), not the <em>density of some summary statistic</em> (i.e., <span class="math inline">\(f(\hat\mu)\)</span>)</p></li>
</ul>
</section>
<section id="parametric-or-nonparametric-5" class="slide level2 center">
<h2>Parametric or Nonparametric?</h2>
<ul>
<li class="fragment"><p>Below is a nonparametric density estimate and data <em>rug</em> for the <code>eruptions</code> data (it is bi-modal <em>and</em> asymmetric - we will discuss this estimator shortly)</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/densityeruptions_3116cbdc3e7b131e1595ed2c55514aba">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="index_files/figure-revealjs/densityeruptions-1.png" width="960"></p>
</figure>
</div>
</div>
</div></li>
</ul>
</section>
<section id="parametric-or-nonparametric-6" class="slide level2 center">
<h2>Parametric or Nonparametric?</h2>
<ul>
<li class="fragment"><p>Below is a nonparametric and <em>normal</em> parametric estimate (recall the normal parametric model has been <em>rejected</em>)</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/densityeruptionscomp_7fc2e612129f48dd1273d6f2efc34b70">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="index_files/figure-revealjs/densityeruptionscomp-1.png" width="960"></p>
</figure>
</div>
</div>
</div></li>
</ul>
</section>
<section id="parametric-or-nonparametric-7" class="slide level2 center">
<h2>Parametric or Nonparametric?</h2>
<ul>
<li class="fragment"><p>Here lies the crux of the parametric problem</p></li>
<li class="fragment"><p>We write down some parametric model that is drawn from a <em>dense</em> space of functions (their number is <em>uncountable</em>)</p></li>
<li class="fragment"><p>One very important assumption is <em>this parametric model is the true model</em>, and all model properties (unbiasedness, consistency etc.) depend <em>crucially</em> on this being the case</p></li>
<li class="fragment"><p>But if you are serious, you immediately <em>test</em> your model for correct specification</p></li>
<li class="fragment"><p>What if the parametric model is rejected, as it often is?</p></li>
</ul>
</section></section>
<section>
<section id="nonparametric-essentials-numeric-data" class="title-slide slide level1 center">
<h1>Nonparametric Essentials <br> Numeric Data</h1>

</section>
<section id="numeric-data" class="slide level2 center">
<h2>Numeric Data</h2>
<ul>
<li class="fragment"><p>Parametric methods require the user to make <em>very strong assumptions</em> about the <em>data generating process</em> (DGP)</p></li>
<li class="fragment"><p>We just considered density estimation, but the exact same issue plagues all parametric analysis (i.e., regression, etc.)</p></li>
<li class="fragment"><p>We will consider, instead, nonparametric <em>kernel</em> estimators</p></li>
<li class="fragment"><p>A <em>kernel</em> is simply a weight function, which for <em>numeric</em> data we denote by <span class="math inline">\(K(Z_i)\)</span> where <span class="math inline">\(Z_i=(x-X_i)/h\)</span></p></li>
<li class="fragment"><p>The kernel function assigns <em>higher</em> weight to observations close to <span class="math inline">\(x\)</span> (i.e., small <span class="math inline">\(Z_i\)</span>) than to those lying further away (i.e., large <span class="math inline">\(Z_i\)</span>)</p></li>
<li class="fragment"><p>A kernel estimator is <em>nonparametric</em> since it does not presume the data is from some known parametric family prior to estimation</p></li>
</ul>
</section>
<section id="non-smooth-or-smooth" class="slide level2 center">
<h2>Non-Smooth or Smooth?</h2>
<ul>
<li class="fragment"><p>Our choice of kernel functions affects the nonparametric estimate</p></li>
<li class="fragment"><p>Below are non-smooth and smooth nonparametric density estimates for the <em>numeric</em> variable <code>eruptions</code> (we will define each estimator shortly)</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/histdensityeruptions_09811c3efcfe760aa78ff53657eea973">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="index_files/figure-revealjs/histdensityeruptions-1.png" width="960"></p>
</figure>
</div>
</div>
</div></li>
</ul>
</section>
<section id="non-smooth-or-smooth-1" class="slide level2 center">
<h2>Non-Smooth or Smooth?</h2>
<ul>
<li class="fragment"><p>The non-smooth nonparametric histogram density estimator is <span class="math display">\[\begin{equation*}
f_H(x)=\frac{1}{nh}\sum_{i=1}^n\mathbf{1}(X_i\text{ is in the same bin as }x)
\end{equation*}\]</span></p></li>
<li class="fragment"><p>This estimator has drawbacks, including</p>
<ul>
<li class="fragment"><p>it is not particularly <em>efficient</em> in the statistical sense</p></li>
<li class="fragment"><p>the estimator’s discontinuity presents obstacles if derivatives are required (<span class="math inline">\(df_H(x)/dx\)</span> is 0 or undefined)</p></li>
</ul></li>
</ul>
</section>
<section id="smooth-univariate-density-estimation" class="slide level2 center">
<h2>Smooth Univariate Density Estimation</h2>
<ul>
<li class="fragment"><p>Consider a symmetric smooth kernel function that satisfies <span class="math inline">\(K(z)\ge 0\)</span> and <span class="math inline">\(\int_{-\infty}^{\infty}K(z)\,dz=1\)</span></p></li>
<li class="fragment"><p>The smooth nonparametric kernel density estimator is <span class="math display">\[\begin{equation*}
\hat f(x)=\frac{1}{nh}\sum_{i=1}^n K\left(\frac{x-X_i}{h}\right)
\end{equation*}\]</span></p></li>
<li class="fragment"><p>Note that we can easily switch between non-smooth and smooth estimators by simply changing the kernel function used (see the <code>ckertype=</code> option in the <code>np</code> package)</p></li>
<li class="fragment"><p>The smooth estimator is dominant in applied settings, though <em>boundary bias</em> can be an issue (simple corrections exist, see <code>npuniden.boundary()</code> in the <code>np</code> package)</p></li>
</ul>
</section>
<section id="smooth-univariate-density-estimation-1" class="slide level2 center">
<h2>Smooth Univariate Density Estimation</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment"><p>The kernel function <span class="math inline">\(K(z)\)</span> is relatively unimportant (it imparts <em>smoothness</em> on the estimate, where smooth functions are <em>continuously differentiable</em> functions)</p></li>
<li class="fragment"><p>The bandwidth <span class="math inline">\(h\)</span> is crucial (it governs the bias-variance trade-off)</p></li>
</ul>
</div>
</div>
</div>
<ul>
<li class="fragment"><p><em>Closeness</em> is determined by a <em>bandwidth</em>, denoted <span class="math inline">\(h\)</span></p></li>
<li class="fragment"><p>We choose <span class="math inline">\(h\)</span> to minimize <em>square error risk</em> and trade off <em>bias</em> for <em>variance</em> for the sample at hand</p></li>
<li class="fragment"><p>To accomplish this we use <em>data-driven</em> methods for selecting <span class="math inline">\(h\)</span> (e.g., least-squares cross-validation and likelihood cross-validation)</p></li>
</ul>
</section>
<section id="smooth-univariate-density-estimation-2" class="slide level2 center">
<h2>Smooth Univariate Density Estimation</h2>
<ul>
<li class="fragment"><p>The pointwise bias (based on a <em>second order kernel</em>, i.e., <span class="math inline">\(\kappa_2\ne0\)</span>) is <span class="math display">\[\begin{equation*}
    \operatorname{bias}\hat f(x)\approx \frac{h^2}{2}f^{''}(x)\kappa_2,\qquad \kappa_2=\int z^2K(z)\,dz&lt;\infty
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>Note that the bias falls as <span class="math inline">\(h\)</span> decreases</p></li>
<li class="fragment"><p>The pointwise variance, used to construct asymptotic confidence intervals by replacing <span class="math inline">\(f(x)\)</span> with <span class="math inline">\(\hat f(x)\)</span>, is <span class="math display">\[\begin{equation*}
    \operatorname{var}\hat f(x)\approx\frac{f(x)\kappa}{nh},\qquad \kappa=\int K^2(z)dz
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>Note that the variance rises as <span class="math inline">\(h\)</span> decreases</p></li>
</ul>
</section>
<section id="smooth-univariate-density-estimation-3" class="slide level2 center">
<h2>Smooth Univariate Density Estimation</h2>
<ul>
<li class="fragment"><p>The integrated mean square error, denoted IMSE, aggregates the pointwise mean square error over the domain of the density yielding a global error measure, and can be defined as <span class="math display">\[\begin{align*}
    \operatorname{IMSE}{\hat f(x)} &amp;= \int \operatorname{mse}{\hat f(x)} dx\cr
   &amp;=\int \operatorname{var}{\hat f(x)}\, dx + \int \left\{\operatorname{bias}{\hat f(x)}\right\}^2dx
  \end{align*}\]</span></p></li>
<li class="fragment"><p>The IMSE is used to derive optimal (but infeasible) bandwidths</p></li>
<li class="fragment"><p>The IMSE also helps assess data-driven bandwidth selection methods (i.e., how close to optimal is a data-driven method)</p></li>
</ul>
</section>
<section id="smooth-univariate-density-estimation-4" class="slide level2 center">
<h2>Smooth Univariate Density Estimation</h2>
<ul>
<li class="fragment"><p>We obtain the infeasible optimal bandwidth balancing bias and variance by minimizing IMSE with respect to <span class="math inline">\(h\)</span>, i.e. <span class="math display">\[\begin{align*}
      h_{opt}&amp;=\kappa^{1/5} \kappa_2^{-2/5}\Phi^{-1/5}n^{-1/5}\cr
      &amp;=\left\{\frac{\int K^2(z)dz}{\left(\int
            z^2K(z)dz\right)^2\int
          \left\{f^{''}(x)\right\}^2dx}\right\}^{1/5} n^{-1/5}\cr
      &amp;=cn^{-1/5}
  \end{align*}\]</span></p></li>
<li class="fragment"><p>Note that the constant <span class="math inline">\(c\)</span> depends on <span class="math inline">\(f^{''}(x)\)</span> and <span class="math inline">\(K(z)\)</span> (the latter we know and can compute, the former is unknown)</p></li>
</ul>
</section>
<section id="data-driven-bandwidth-selection" class="slide level2 center">
<h2>Data-Driven Bandwidth Selection</h2>
<ul>
<li class="fragment"><p>Likelihood cross validation is a popular data-driven method that chooses <span class="math inline">\(h\)</span> to <em>maximize</em> the (leave-one-out) log likelihood function <span class="math display">\[\begin{equation*}
    \mathcal{L} = \log\, L = \sum_{i=1}^n \log\, \hat f_{-i}(X_i)
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>Here, <span class="math inline">\(\hat f_{-i}(X_i)\)</span> is the leave-one-out kernel estimator of <span class="math inline">\(f(X_i)\)</span> that uses all points except <span class="math inline">\(X_i\)</span> to construct the density estimate, that is, <span class="math display">\[\begin{equation*}
    \hat f_{-i}(X_i)=\frac{1}{(n-1)h}\sum_{j=1,j\ne i}^n K\left(\frac{X_i-X_j}{h}\right)
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>See the option <code>bwmethod="cv.ml"</code> in the R package np</p></li>
</ul>
</section>
<section id="data-driven-bandwidth-selection-1" class="slide level2 center">
<h2>Data-Driven Bandwidth Selection</h2>
<ul>
<li class="fragment"><p>Least squares cross validation is another data-driven method based on the principle of choosing <span class="math inline">\(h\)</span> to <em>minimize</em> the integrated square error of the resulting estimate</p></li>
<li class="fragment"><p>The integrated squared difference between <span class="math inline">\(\hat f\)</span> and <span class="math inline">\(f\)</span>, i.e., <span class="math inline">\(\int (\hat f(x) - f(x))^2\, dx\)</span>, is <span class="math display">\[\begin{equation*}
   \int \hat f(x)^2\, dx - 2\int \hat f(x) f(x)\, dx + \int f(x)^2\, dx
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>We can insert the formula for <span class="math inline">\(\hat f(x)\)</span> into these three terms, adjust for bias (delete-one trick), and obtain an objective function that can be minimized using numerical search algorithms</p></li>
<li class="fragment"><p>See the option <code>bwmethod="cv.ls"</code> in the R package np</p></li>
</ul>
</section>
<section id="smooth-univariate-density-estimation-5" class="slide level2 center">
<h2>Smooth Univariate Density Estimation</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/npudenseruptions_41604006ce9a865d7a5ef2dc936d408c">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="fu">library</span>(np)</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="fu">data</span>(faithful)</span>
<span id="cb3-3"><a href="#cb3-3"></a>fhat <span class="ot">&lt;-</span> <span class="fu">npudens</span>(<span class="sc">~</span>eruptions,<span class="at">data=</span>faithful)</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="fu">plot</span>(fhat,<span class="at">neval=</span><span class="dv">250</span>,<span class="at">plot.errors.method=</span><span class="st">"bootstrap"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/npudenseruptions-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="smooth-joint-density-estimation" class="slide level2 center">
<h2>Smooth Joint Density Estimation</h2>
<ul>
<li class="fragment"><p>So far we have considered univariate densities (<span class="math inline">\(X\in\mathbb{R}^1\)</span>)</p></li>
<li class="fragment"><p>Let <span class="math inline">\(X\in \mathbb{R}^q\)</span> denote a numeric vector of dimension <span class="math inline">\(q\)</span></p></li>
<li class="fragment"><p>Let <span class="math inline">\(f(x)=f(x_1,x_2,\dots,x_q)\)</span> denote a joint PDF evaluated at <span class="math inline">\(x=(x_1,x_2,\dots,x_q)'\)</span></p></li>
<li class="fragment"><p>Let <span class="math inline">\(\{X_1,X_2,\dots,X_n\}\)</span> represent <span class="math inline">\(n\)</span> draws of a numeric random vector <span class="math inline">\(X\)</span>, where the <span class="math inline">\(i\)</span>th draw is denoted by <span class="math inline">\(X_i=(X_{i1},X_{i2},\dots,X_{iq})\)</span></p></li>
</ul>
</section>
<section id="smooth-joint-density-estimation-1" class="slide level2 center">
<h2>Smooth Joint Density Estimation</h2>
<ul>
<li class="fragment"><p>The multivariate kernel density estimator is <span class="math display">\[\begin{equation*}
\hat f(x)=\frac{1}{nh_1\dots h_q}\sum_{i=1}^n K\left(\frac{x_1-X_{i1}}{h_1},\dots,\frac{x_q-X_{iq}}{h_q}\right)
\end{equation*}\]</span></p></li>
<li class="fragment"><p><span class="math inline">\(K(\cdot)\)</span> is a <em>multivariate</em> kernel, and one popular and tractable choice of kernel function is the product kernel given by the simple product of univariate kernels, <span class="math display">\[\begin{equation*}
    K\left(\frac{x_1-X_{i1}}{h_1}\right)\times\dots\times K\left(\frac{x_q-X_{iq}}{h_q}\right)
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>Typically this the <em>product</em> of the same univariate kernel we already used - we have lots of flexibility here</p></li>
</ul>
</section>
<section id="smooth-joint-density-estimation-2" class="slide level2 center">
<h2>Smooth Joint Density Estimation</h2>
<ul>
<li class="fragment"><p>In the multivariate case, we can show that <span class="math display">\[\begin{equation*}
    \operatorname{bias}\hat f(x) \approx \sum_{s=1}^q \frac{h_s^2}{2}f_{ss}(x)\kappa_2
  \end{equation*}\]</span> based on a <em>second order kernel</em> (<span class="math inline">\(0&lt; \kappa_2 = \int z^2 K(z)\, dz&lt;\infty\)</span>) where <span class="math inline">\(f_{ss}(x)\)</span> is the second-order derivative of <span class="math inline">\(f(x)\)</span> with respect to <span class="math inline">\(x_s\)</span></p></li>
<li class="fragment"><p>We can also show that <span class="math display">\[\begin{equation*}
    \operatorname{var} \hat f(x)\approx \frac{f(x)\kappa^q}{n\prod_{s=1}^qh_s}
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>Bias falls and variance increases as <span class="math inline">\(h_s\)</span> decreases, <span class="math inline">\(s=1,\dots,q\)</span> (just as when <span class="math inline">\(q=1\)</span>, the univariate case)</p></li>
</ul>
</section>
<section id="smooth-joint-density-estimation-3" class="slide level2 center">
<h2>Smooth Joint Density Estimation</h2>
<ul>
<li class="fragment"><p>The smooth multivariate estimator is a straightforward extension of the smooth univariate estimator</p></li>
<li class="fragment"><p>The main differences to be aware of are that</p>
<ul>
<li class="fragment"><p>there is one bandwidth to be computed for each variable (i.e., <span class="math inline">\(h_1,h_2,\dots,h_q\)</span>)</p></li>
<li class="fragment"><p>the rate of convergence depends on the number of numeric variables involved (the “curse-of-dimensionality”)</p></li>
</ul></li>
<li class="fragment"><p>We continue to use data-driven methods for bandwidth selection when <span class="math inline">\(q&gt;1\)</span> (e.g., likelihood and least-squares cross-validation)</p></li>
<li class="fragment"><p>Since optimal bandwidths typically differ across variables (<span class="math inline">\(h_1\ne h_2\)</span>, etc.) we conduct <span class="math inline">\(q\)</span>-dimensional numerical optimization</p></li>
</ul>
</section>
<section id="smooth-joint-density-estimation-4" class="slide level2 center">
<h2>Smooth Joint Density Estimation</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/npudenseruptionswaiting_4b0d58fa08b9e8a35542c02ce8241c6f">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="fu">library</span>(np)</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="fu">data</span>(faithful)</span>
<span id="cb4-3"><a href="#cb4-3"></a>fhat <span class="ot">&lt;-</span> <span class="fu">npudens</span>(<span class="sc">~</span>eruptions<span class="sc">+</span>waiting,<span class="at">data=</span>faithful)</span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="fu">plot</span>(fhat,<span class="at">theta=</span><span class="dv">330</span>,<span class="at">xtrim=</span><span class="sc">-</span><span class="fl">0.05</span>,<span class="at">neval=</span><span class="dv">75</span>,<span class="at">view=</span><span class="st">"fixed"</span>,<span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/npudenseruptionswaiting-1.png" width="960" class="r-stretch quarto-figure-center"></section></section>
<section>
<section id="nonparametric-essentials-categorical-data" class="title-slide slide level1 center">
<h1>Nonparametric Essentials <br> Categorical Data</h1>

</section>
<section id="categorical-variables" class="slide level2 center">
<h2>Categorical Variables</h2>
<ul>
<li class="fragment"><p>We considered the density of continuously distributed random variables <span class="math inline">\(X\in\mathbb{R}\)</span>, but we also deal with the probability function of categorical variables <span class="math inline">\(X\in\mathcal{D}\)</span> where <span class="math inline">\(\mathcal{D}\)</span> is a discrete set</p></li>
<li class="fragment"><p>We might presume a parametric model for the probability function of <span class="math inline">\(X\in\mathcal{D}\)</span> but we face exactly the same issue we faced before</p></li>
<li class="fragment"><p>The non-smooth categorical counterpart to the histogram is called the <em>frequency</em> or <em>empirical</em> probability estimator and, like the histogram, has some drawbacks</p></li>
</ul>
</section>
<section id="univariate-probability-estimation" class="slide level2 center">
<h2>Univariate Probability Estimation</h2>
<ul>
<li class="fragment"><p>Let <span class="math inline">\(X\in\mathcal{D}=\{0,1,\dots,c-1\}\)</span> be categorical</p></li>
<li class="fragment"><p><span class="math inline">\(X\)</span> can be <em>unordered</em> (<code>factor()</code> in R) or <em>ordered</em> (<code>ordered()</code> in R)</p></li>
<li class="fragment"><p>The nonparametric <em>frequency</em> estimator of <span class="math inline">\(p(x)\)</span> is <span class="math display">\[\begin{align*}
p_n(x)&amp;=\frac{\# X_i\text{ equal to }x}{n}\\
&amp;=\frac{1}{n}\sum_{i=1}^n \mathbf{1}(X_i=x)
\end{align*}\]</span></p></li>
</ul>
</section>
<section id="univariate-probability-estimation-1" class="slide level2 center">
<h2>Univariate Probability Estimation</h2>
<ul>
<li class="fragment"><p>The <em>unordered</em> nonparametric kernel estimator of <span class="math inline">\(p(x)\)</span> is <span class="math display">\[\begin{equation*}
\hat p(x)=\frac{1}{n}\sum_{i=1}^n L(X_i,x,\lambda)
\end{equation*}\]</span></p></li>
<li class="fragment"><p><span class="math inline">\(L(\cdot)\)</span> is an <em>unordered</em> kernel function given by <span class="math display">\[\begin{equation*}
L(X_i,x,\lambda)=\left\{
  \begin{array}{ll}
    1-\lambda &amp; \mbox{ if } X_i=x\\
    \lambda/(c-1) &amp; \mbox{ otherwise}
  \end{array}
\right.
\end{equation*}\]</span></p></li>
<li class="fragment"><p><span class="math inline">\(\lambda\)</span> is a <em>smoothing parameter</em> (counterpart to <em>bandwidth</em> <span class="math inline">\(h\)</span>) that can be selected using the same data-driven methods already considered</p></li>
</ul>
</section>
<section id="univariate-probability-estimation-2" class="slide level2 center">
<h2>Univariate Probability Estimation</h2>
<ul>
<li class="fragment"><p>The pointwise bias is <span class="math display">\[\begin{equation*}
    \operatorname{bias}\hat p(x)=\lambda\left\{\frac{1-cp(x)}{c-1}\right\}
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>Note that the bias falls as <span class="math inline">\(\lambda\)</span> decreases</p></li>
<li class="fragment"><p>The pointwise variance, used to construct asymptotic confidence intervals by replacing <span class="math inline">\(p(x)\)</span> with <span class="math inline">\(\hat p(x)\)</span>, is <span class="math display">\[\begin{equation*}
    \operatorname{var}\hat p(x)=\frac{p(x)(1-p(x))}{n}\left(1-\lambda\frac{c}{(c-1)}\right)^2
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>Note that the variance rises as <span class="math inline">\(\lambda\)</span> decreases</p></li>
</ul>
</section>
<section id="example---unordered-probability" class="slide level2 center">
<h2>Example - Unordered Probability</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/npudensfactor_1c04c1cfffea203e712139578dcc48c3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a>n <span class="ot">&lt;-</span> <span class="dv">250</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb5-3"><a href="#cb5-3"></a>sex <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">"Female"</span>,<span class="st">"Male"</span>),n,<span class="at">replace=</span><span class="cn">TRUE</span>,<span class="at">prob=</span><span class="fu">c</span>(.<span class="dv">4</span>,.<span class="dv">6</span>))</span>
<span id="cb5-4"><a href="#cb5-4"></a>sex <span class="ot">&lt;-</span> <span class="fu">factor</span>(sex)</span>
<span id="cb5-5"><a href="#cb5-5"></a>phat <span class="ot">&lt;-</span> <span class="fu">npudens</span>(<span class="sc">~</span>sex)</span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="fu">plot</span>(phat,<span class="at">plot.errors.method=</span><span class="st">"bootstrap"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/npudensfactor-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="univariate-probability-estimation-3" class="slide level2 center">
<h2>Univariate Probability Estimation</h2>
<ul>
<li class="fragment"><p>Let <span class="math inline">\(X\in\mathcal{D}=\{0,1,\dots,c-1\}\)</span>, <span class="math inline">\(c\ge 2\)</span>, be <em>ordered</em></p></li>
<li class="fragment"><p>The <em>ordered</em> nonparametric kernel estimator of <span class="math inline">\(p(x)\)</span> is <span class="math display">\[\begin{equation*}
\hat p(x)=\frac{1}{n}\sum_{i=1}^n l(X_i,x,\lambda)
\end{equation*}\]</span></p></li>
<li class="fragment"><p><span class="math inline">\(l(X_i,x,\lambda)\)</span> is an <em>ordered</em> kernel function given by <span class="math display">\[\begin{equation*}
l(X_i,x,\lambda)=\frac{\lambda^{d_{xi}}}{\Lambda_i}
\end{equation*}\]</span></p></li>
<li class="fragment"><p>See <span class="citation" data-cites="racine_2019">Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2019</a>)</span> for technical details (e.g., bias and variance formulas)</p></li>
</ul>
</section>
<section id="example---ordered-probability" class="slide level2 center">
<h2>Example - Ordered Probability</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/npudensordered_98e52e67016768fcbed918c04e311320">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>n <span class="ot">&lt;-</span> <span class="dv">250</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb6-3"><a href="#cb6-3"></a>income <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">"Low"</span>,<span class="st">"Middle"</span>,<span class="st">"High"</span>),n,<span class="at">replace=</span><span class="cn">TRUE</span>,<span class="at">prob=</span><span class="fu">c</span>(.<span class="dv">3</span>,.<span class="dv">5</span>,.<span class="dv">2</span>))</span>
<span id="cb6-4"><a href="#cb6-4"></a>income <span class="ot">&lt;-</span> <span class="fu">ordered</span>(income,<span class="at">levels=</span><span class="fu">c</span>(<span class="st">"Low"</span>,<span class="st">"Middle"</span>,<span class="st">"High"</span>))</span>
<span id="cb6-5"><a href="#cb6-5"></a>phat <span class="ot">&lt;-</span> <span class="fu">npudens</span>(<span class="sc">~</span>income)</span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="fu">plot</span>(phat,<span class="at">plot.errors.method=</span><span class="st">"bootstrap"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/npudensordered-1.png" width="960" class="r-stretch quarto-figure-center"></section></section>
<section>
<section id="nonparametric-essentials-mixed-data" class="title-slide slide level1 center">
<h1>Nonparametric Essentials <br> Mixed Data</h1>

</section>
<section id="mixed-data-density-estimation" class="slide level2 center">
<h2>Mixed Data Density Estimation</h2>
<ul>
<li class="fragment"><p>Statisticians know that <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(p(x)\)</span> are both called <em>density</em> functions (the difference lies in their <em>measure</em> - the latter uses <em>counting</em> measure), so we adopt <span class="math inline">\(f(x)\)</span></p></li>
<li class="fragment"><p>Suppose you have a joint density defined over mixed data types, say, one numeric (<span class="math inline">\(X^c\in\mathbb{R}\)</span>) and one unordered (<span class="math inline">\(X^d\in\mathcal{D}\)</span> with cardinality <span class="math inline">\(c\)</span>)</p></li>
<li class="fragment"><p>We would like to model their joint density function <span class="math inline">\(f(x)=f(x^c,x^d)\)</span>, where the superscripts <span class="math inline">\(^c\)</span> and <span class="math inline">\(^d\)</span> denote continuous and discrete data types, respectively, and where <span class="math inline">\(x=(x^c,x^d)\in\mathbb{R}\times\mathcal{D}\)</span></p></li>
</ul>
</section>
<section id="mixed-data-density-estimation-1" class="slide level2 center">
<h2>Mixed Data Density Estimation</h2>
<ul>
<li class="fragment"><p>The kernel estimator of <span class="math inline">\(f(x^c,x^d)\)</span> is <span class="math display">\[\begin{equation*}
\hat f(x^c,x^d)=\frac{1}{n}\sum_{i=1}^n \frac{1}{h}K\left(\frac{X_i^c-x^c}{h}\right)L(X_i^d,x^d,\lambda)
\end{equation*}\]</span></p></li>
<li class="fragment"><p><span class="math inline">\(K(\cdot)\)</span> and <span class="math inline">\(L(\cdot)\)</span> are (univariate) <em>numeric</em> and <em>unordered</em> kernel functions, respectively</p></li>
<li class="fragment"><p>If you had one numeric and one <em>ordered</em> variable you would use the ordered kernel <span class="math inline">\(l(X_i^d,x^d,\lambda)\)</span> above</p></li>
<li class="fragment"><p>Now let’s consider the <em>general</em> multivariate mixed data density case</p></li>
</ul>
</section>
<section id="mixed-data-density-estimation-2" class="slide level2 center">
<h2>Mixed Data Density Estimation</h2>
<ul>
<li class="fragment"><p>In general multivariate settings, the probability density function <span class="math inline">\(f(x)\)</span> might use some combination of <span class="math inline">\(q\)</span> numeric, <span class="math inline">\(r\)</span> unordered, and <span class="math inline">\(s\)</span> ordered variable types (<span class="math inline">\(x\)</span> and <span class="math inline">\(X_i\)</span> are <span class="math inline">\(q+r+s\)</span>-vectors)</p></li>
<li class="fragment"><p>The (product) kernel for estimating the joint density function is <span class="math display">\[\begin{equation*}
\prod_{j=1}^qh^{-1}_jK\left(\frac{x^c_j-X^c_{ij}}{h_j}\right)\prod_{j=1}^r L(X^u_{ij},x^u_j,\lambda^u_j)\prod_{j=1}^s l(X^o_{ij},x^o_j,\lambda^o_j)
\end{equation*}\]</span></p></li>
<li class="fragment"><p>All we are doing here is using the <em>appropriate kernel function</em> for each data type</p></li>
</ul>
</section>
<section id="mixed-data-density-estimation-3" class="slide level2 center">
<h2>Mixed Data Density Estimation</h2>
<ul>
<li class="fragment"><p>Let <span class="math inline">\(\gamma=(h_1,\dots,h_q,\lambda^u_1,\dots,\lambda^u_r,\lambda^o_1,\dots,\lambda^o_s)\)</span></p></li>
<li class="fragment"><p>Call the product kernel function on the previous slide the <em>generalized</em> kernel, i.e., let the expression be written as <span class="math display">\[\begin{equation*}
K_\gamma(X_i,x)=\prod_{j=1}^q[\dots]\prod_{j=1}^r[\dots]\prod_{j=1}^s[\dots]
\end{equation*}\]</span></p></li>
<li class="fragment"><p>With <span class="math inline">\(x\)</span> and <span class="math inline">\(X_i\)</span> being <span class="math inline">\(q+r+s\)</span> vectors, we write <span class="math inline">\(\hat f(x)\)</span> as <span class="math display">\[\begin{equation*}
\hat f(x)=\frac{1}{n}\sum_{i=1}^nK_{\gamma}(X_i,x)
\end{equation*}\]</span></p></li>
<li class="fragment"><p>See <span class="citation" data-cites="racine_2019">Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2019</a>)</span> for technical details (variance is the same multivariate formula given on a previous slide, use that for CIs)</p></li>
</ul>
</section>
<section id="mixed-data-density-estimation-4" class="slide level2 center">
<h2>Mixed Data Density Estimation</h2>
<ul>
<li class="fragment"><p>The key point is that <em>once you cast your data</em>, then the R functions in the <code>np</code> package know <em>exactly</em> what to do</p></li>
<li class="fragment"><p>They <em>automatically</em> use the appropriate kernel for the appropriate data type (numeric, factor, ordered)</p></li>
<li class="fragment"><p>The data-driven bandwidth methods adjust automatically to the data type</p></li>
<li class="fragment"><p>Methods for inference (confidence intervals, significance testing in regression) do the same</p></li>
<li class="fragment"><p>Let’s consider a quick example</p></li>
</ul>
</section>
<section id="example---mixed-data-density" class="slide level2 center">
<h2>Example - Mixed Data Density</h2>
<ul>
<li class="fragment"><p>We use Wooldridge’s <code>wage1</code> data and consider two variables, one numeric (<code>lwage</code>) and one ordered (<code>numdep</code>)</p></li>
<li class="fragment"><p><code>lwage</code> is the logarithm of an individual’s average hourly earnings, and <code>numdep</code> is their number of dependents</p></li>
<li class="fragment"><p>The number of observations in each <em>cell</em> is tabulated in <a href="#/example---mixed-data-density">Table&nbsp;1</a></p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/tbl-wage1mixedtable_684c1cc91e7e28e0a4be2baaa4c1333b">
<div class="cell-output-display">
<div id="tbl-wage1mixedtable">
<table>
<caption>Table&nbsp;1: Counts of number of dependants present in 526 households by cell</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">numdep</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="even">
<td style="text-align: left;">counts</td>
<td style="text-align: right;">252</td>
<td style="text-align: right;">105</td>
<td style="text-align: right;">99</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">2</td>
</tr>
</tbody>
</table>
</div>
</div>
</div></li>
<li class="fragment"><p>We estimate <span class="math inline">\(\hat f(lwage,numdep)\)</span> and plot it in <a href="#/fig-wage1mixeddensity">Figure&nbsp;1</a></p></li>
</ul>
</section>
<section id="example---mixed-data-density-1" class="slide level2 center">
<h2>Example - Mixed Data Density</h2>
<ul>
<li class="fragment"><p>Below is the R code to estimate and plot the joint density (we use the <code>plot3D</code> package and reformat the data to render this plot)</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1mixeddensitycode_8deb324eeb940548eadd96c93e4af389">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="fu">library</span>(np)</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="fu">library</span>(plot3D)</span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="fu">data</span>(wage1)</span>
<span id="cb7-4"><a href="#cb7-4"></a>numdep.seq <span class="ot">&lt;-</span> <span class="fu">with</span>(wage1,<span class="fu">sort</span>(<span class="fu">unique</span>(numdep)))</span>
<span id="cb7-5"><a href="#cb7-5"></a>lwage.seq <span class="ot">&lt;-</span> <span class="fu">with</span>(wage1,<span class="fu">seq</span>(<span class="fu">min</span>(lwage),<span class="fu">max</span>(lwage),<span class="at">length=</span><span class="dv">50</span>))</span>
<span id="cb7-6"><a href="#cb7-6"></a>wage1.eval <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">numdep=</span><span class="fu">ordered</span>(numdep.seq),<span class="at">lwage=</span>lwage.seq)</span>
<span id="cb7-7"><a href="#cb7-7"></a>bw <span class="ot">&lt;-</span> <span class="fu">npudensbw</span>(<span class="sc">~</span>lwage<span class="sc">+</span><span class="fu">ordered</span>(numdep),<span class="at">data=</span>wage1)</span>
<span id="cb7-8"><a href="#cb7-8"></a>fhat <span class="ot">&lt;-</span> <span class="fu">fitted</span>(<span class="fu">npudens</span>(<span class="at">bws=</span>bw,<span class="at">newdata=</span>wage1.eval))</span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="do">## Hack since scatter3D converts ordered 0-6 to numeric 1-7</span></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="fu">scatter3D</span>(<span class="fu">as.numeric</span>(wage1.eval[,<span class="dv">1</span>])<span class="sc">-</span><span class="dv">1</span>,wage1.eval[,<span class="dv">2</span>],fhat,</span>
<span id="cb7-11"><a href="#cb7-11"></a>          <span class="at">ylab=</span><span class="st">"Log-Wage"</span>,</span>
<span id="cb7-12"><a href="#cb7-12"></a>          <span class="at">xlab=</span><span class="st">"Number of Dependants"</span>,</span>
<span id="cb7-13"><a href="#cb7-13"></a>          <span class="at">zlab=</span><span class="st">"Joint Density"</span>,</span>
<span id="cb7-14"><a href="#cb7-14"></a>          <span class="at">ticktype=</span><span class="st">"detailed"</span>,</span>
<span id="cb7-15"><a href="#cb7-15"></a>          <span class="at">angle=</span><span class="dv">15</span>,</span>
<span id="cb7-16"><a href="#cb7-16"></a>          <span class="at">box=</span><span class="cn">TRUE</span>,</span>
<span id="cb7-17"><a href="#cb7-17"></a>          <span class="at">type=</span><span class="st">"h"</span>,</span>
<span id="cb7-18"><a href="#cb7-18"></a>          <span class="at">grid=</span><span class="cn">TRUE</span>,</span>
<span id="cb7-19"><a href="#cb7-19"></a>          <span class="at">col=</span><span class="st">"blue"</span>,</span>
<span id="cb7-20"><a href="#cb7-20"></a>          <span class="at">colkey=</span><span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
</ul>
</section>
<section id="example---mixed-data-density-2" class="slide level2 center">
<h2>Example - Mixed Data Density</h2>

<img data-src="index_files/figure-revealjs/fig-wage1mixeddensity-1.png" width="960" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;1: Mixed-data bivariate kernel density estimate for the joint PDF of lwage (numeric) and numdeps (ordered)</p></section></section>
<section>
<section id="nonparametric-regression" class="title-slide slide level1 center">
<h1>Nonparametric Regression</h1>

</section>
<section id="nonparametric-regression-1" class="slide level2 center">
<h2>Nonparametric Regression</h2>
<ul>
<li class="fragment"><p>The conditional mean function <span class="math inline">\(g(x):=\mathbb{E}(Y\vert X=x)\)</span> for numeric <span class="math inline">\(Y\in\mathbb{R}\)</span> is <em>defined</em> as <span class="math display">\[\begin{equation}
\label{g(x)}
g(x)=\int y\frac{f(y,x)}{f(x)}\,dy=\frac{m(x)}{f(x)}
\end{equation}\]</span></p></li>
<li class="fragment"><p>Let’s start with <span class="math inline">\(X\in\mathbb{R}^1\)</span> (i.e., one numeric predictor)</p></li>
<li class="fragment"><p>We aim to estimate the unknown regression model <span class="math inline">\(y=g(x)+\varepsilon\)</span> without assuming that, e.g., <span class="math inline">\(g(x)=\beta_0+\beta_1x\)</span></p></li>
</ul>
<aside class="notes">
<ul>
<li><p>After “We aim”, mention the following:</p></li>
<li><p>People often use the same simple model for different datasets</p></li>
<li><p>Is it realistic to assume <span class="math inline">\(\int y\frac{f(y,x)}{f(x)}\,dy=\beta_0+\beta_1x\)</span> for <em>every</em> dataset?</p></li>
<li><p>Think of just the denominator, <span class="math inline">\(f(x)\)</span> - the <em>same</em> for all data?</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="nonparametric-regression-2" class="slide level2 center">
<h2>Nonparametric Regression</h2>
<ul>
<li class="fragment"><p>Regression is a <em>hard</em> problem because <span class="math inline">\(f(y,x)\)</span> and <span class="math inline">\(f(x)\)</span> are unknown</p></li>
<li class="fragment"><p>They can be <em>consistently</em> estimated using <span class="math inline">\(\hat f(y,x)\)</span> and <span class="math inline">\(\hat f(x)\)</span></p></li>
<li class="fragment"><p>For continuously distributed <span class="math inline">\(Y\in\mathbb{R}\)</span>, the <em>local constant</em> kernel regression estimator replaces the <em>unknown</em> <span class="math inline">\(f(y,x)\)</span> and <span class="math inline">\(f(x)\)</span> with <span class="math inline">\(known\)</span> <span class="math inline">\(\hat f(y,x)\)</span> and <span class="math inline">\(\hat f(x)\)</span> in <span class="math inline">\(\eqref{g(x)}\)</span>, hence <span class="math display">\[\begin{equation}
\hat g(x)=\int y \frac{\hat f(y,x)}{\hat f(x)}\,dy=\frac{\hat m(x)}{\hat f(x)}
\end{equation}\]</span></p></li>
</ul>
</section>
<section id="nonparametric-regression-3" class="slide level2 center">
<h2>Nonparametric Regression</h2>
<ul>
<li class="fragment"><p>For <span class="math inline">\(Y\in\mathbb{R}\)</span> and <span class="math inline">\(X\in\mathbb{R}^1\)</span> (i.e., <span class="math inline">\(q=1\)</span>), we estimate <span class="math inline">\(g(x)\)</span> by</p>
<ol type="1">
<li class="fragment"><p>replacing the unknown <span class="math inline">\(f(x)\)</span> in <span class="math inline">\(\eqref{g(x)}\)</span> with <span class="math display">\[\begin{equation*}
\hat f(x)=\frac{1}{nh}\sum_{i=1}^n K\left(\frac{x-X_i}{h}\right)
\end{equation*}\]</span></p></li>
<li class="fragment"><p>replacing the unknown <span class="math inline">\(f(y,x)\)</span> in <span class="math inline">\(\eqref{g(x)}\)</span> with <span class="math display">\[\begin{equation*}
\hat f(y,x)=\frac{1}{nh_yh}\sum_{i=1}^n K\left(\frac{y-Y_i}{h_y}\right)K\left(\frac{x-X_i}{h}\right)
\end{equation*}\]</span></p></li>
</ol></li>
</ul>
</section>
<section id="nonparametric-regression-4" class="slide level2 center">
<h2>Nonparametric Regression</h2>
<ul>
<li class="fragment"><p>Some mathematical simplification of <span class="math inline">\(\hat g(x)\)</span> then leads to <span class="math display">\[\begin{equation*}
  \hat g(x)=\sum_{i=1}^n Y_i\left\{\frac{K\left(\frac{X_i-x}{h}\right)}{\sum_{i=1}^n K\left(\frac{X_i-x}{h}\right)}\right\}=\sum_{i=1}^n Y_i W_i(x)
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>With mixed data multivariate <span class="math inline">\(X\)</span> (i.e., <span class="math inline">\(x\)</span> and <span class="math inline">\(X_i\)</span> are <span class="math inline">\(q+r+s\)</span>-vectors) we replace <span class="math inline">\(K((x-X_i)/h)\)</span> with <span class="math inline">\(K_\gamma(X_i,x)\)</span> defined previously (we will return to this case shortly)</p></li>
<li class="fragment"><p>The estimated nonparametric regression function is <span class="math display">\[\begin{equation}\label{eq:ghat}
y=\hat g(x)+\hat\epsilon
\end{equation}\]</span></p></li>
</ul>
<aside class="notes">
<ul>
<li><p>Point out that <span class="math inline">\(\sum_{i=1}^n Y_i W_i(x)\)</span> is simply a <em>weighted average</em> of <span class="math inline">\(Y_i\)</span> where the weights are <em>local</em> (i.e., local to <span class="math inline">\(x\)</span>)</p></li>
<li><p>So the weights change with <span class="math inline">\(x\)</span></p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="nonparametric-regression-5" class="slide level2 center">
<h2>Nonparametric Regression</h2>
<ul>
<li class="fragment"><p>The pointwise bias (using a <em>second order kernel</em>, i.e., <span class="math inline">\(\kappa_2\ne0\)</span>, when <span class="math inline">\(X\in\mathbb{R}^1\)</span>) is <span class="math display">\[\begin{equation*}
    \operatorname{bias}\hat g(x)\approx \frac{h^2}{2f(x)}\left\{2g'(x)f'(x)+g^{''}(x)f(x)\right\}\kappa_2
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>The pointwise variance (univariate <span class="math inline">\(X\)</span>) is <span class="math display">\[\begin{equation*}
    \operatorname{var}\hat g(x)
    \approx\frac{\sigma^2(x)}{nhf(x)}\kappa
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>The bias once again falls as <span class="math inline">\(h\)</span> decreases, while variance again rises as <span class="math inline">\(h\)</span> decreases</p></li>
</ul>
</section>
<section id="data-driven-bandwidth-selection-2" class="slide level2 center">
<h2>Data-Driven Bandwidth Selection</h2>
<ul>
<li class="fragment"><p>How one selects the bandwidth <span class="math inline">\(h\)</span> is a key aspect to sound nonparametric analysis, and cross-validation methods are powerful data-driven solutions</p></li>
<li class="fragment"><p>See <code>bwmethod="cv.ls"</code> in the functions <code>npreg()</code> and <code>npregbw()</code></p></li>
<li class="fragment"><p>Least-squares cross-validation chooses <span class="math inline">\(h\)</span> to minimize the following objective function <span class="math display">\[\begin{equation*}
  CV_{lc}(h) = n^{-1}\sum_{i=1}^n \left(Y_i - \hat g_{-i}(X_i)\right)^2
  M(X_i),
\end{equation*}\]</span> where <span class="math inline">\(\hat g_{-i}(X_i) =\sum_{l\neq i}^n Y_l K_\gamma(X_i,x)/\sum_{l\neq i}^n K_\gamma(X_i,x)\)</span> is the leave-one-out kernel estimator of <span class="math inline">\(g(X_i)\)</span>, and <span class="math inline">\(0\leq M(\cdot)\leq 1\)</span> is a weight function which serves to avoid difficulties caused by dividing by zero, or by the slow convergence rate caused by boundary effects</p></li>
</ul>
</section>
<section id="data-driven-bandwidth-selection-3" class="slide level2 center">
<h2>Data-Driven Bandwidth Selection</h2>
<ul>
<li class="fragment"><p>Generalized cross-validation was proposed by <span class="citation" data-cites="CRAVEN_WAHBA:1979">Craven and Wahba (<a href="#/references-scrollable" role="doc-biblioref" onclick="">1979</a>)</span></p></li>
<li class="fragment"><p>The kernel estimator can be expressed as <span class="math display">\[\begin{equation*}
  (\hat g(X_1),\dots,\hat g(X_n))'=H(h)Y,
\end{equation*}\]</span> where <span class="math inline">\(H(h)\)</span> is an <span class="math inline">\(n\times n\)</span> hat matrix which depends on the <span class="math inline">\(X\)</span>s, <span class="math inline">\(h\)</span>, and <span class="math inline">\(K(\cdot)\)</span>, and <span class="math inline">\(Y=(Y_1,\dots,Y_n)'\)</span></p></li>
<li class="fragment"><p>The GCV approach selects <span class="math inline">\(h\)</span> that minimizes <span class="math display">\[\begin{equation*}
  GCV(h)=[n^{-1}{\rm tr}\{I-H(h)\}]^{-2}n^{-1}\sum_{i=1}^n\left(Y_i-\hat g(X_i)\right)^2
\end{equation*}\]</span></p></li>
<li class="fragment"><p>Both <span class="math inline">\(CV(h)\)</span> and <span class="math inline">\(GCV(h)\)</span> are consistent estimates of the MISE of <span class="math inline">\(\hat g(x)\)</span></p></li>
</ul>
</section>
<section id="data-driven-bandwidth-selection-4" class="slide level2 center">
<h2>Data-Driven Bandwidth Selection</h2>
<ul>
<li class="fragment"><p><span class="citation" data-cites="HURVICH_SIMONOFF_TSAI:1998">Hurvich, Simonoff, and Tsai (<a href="#/references-scrollable" role="doc-biblioref" onclick="">1998</a>)</span> proposed an improved information criterion based on minimization of <span class="math display">\[\begin{equation*}
  AIC_c(h)=\log\left[n^{-1}\sum_{i=1}^n\left(Y_i-\hat
      g(X_i)\right)^2\right]
  +1+\left[\frac{2\left({\rm tr}H(h)+1\right)}
    {n-{\rm tr}H(h)-2}
  \right]
\end{equation*}\]</span></p></li>
<li class="fragment"><p>See <code>bwmethod="cv.aic"</code> in the functions <code>npreg()</code> and <code>npregbw()</code></p></li>
<li class="fragment"><p>To assess how various bandwidth selection methods perform on actual data, we consider the following example using data from <span class="citation" data-cites="FOX:2011">Fox and Weisberg (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2011</a>)</span> <code>car</code> package and the local linear estimator</p></li>
<li class="fragment"><p>The dataset consists of <span class="math inline">\(n=102\)</span> observations. The dependent variable is the prestige of Canadian occupations (`Pineo-Porter score’) and the explanatory variable is average income for each occupation measured in 1971 Canadian dollars</p></li>
</ul>
</section>
<section id="data-driven-bandwidth-selection-5" class="slide level2 center">
<h2>Data-Driven Bandwidth Selection</h2>

<img data-src="index_files/figure-revealjs/prestige-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="nonparametric-regression-6" class="slide level2 center">
<h2>Nonparametric Regression</h2>
<ul>
<li class="fragment"><p>As noted previously, the multivariate mixed-data kernel regression estimator is obtained by simply replacing the univariate continuous kernel function in <span class="math inline">\(\eqref{eq:ghat}\)</span> (i.e., <span class="math inline">\(K((X_i-x)/h)\)</span>) with its multivariate mixed-data counterpart (i.e., <span class="math inline">\(K_\gamma(X_i,x)\)</span>)</p></li>
<li class="fragment"><p>This estimator is known as the “local constant” estimator because it can also be obtained by minimizing</p>
<p><span class="math display">\[\begin{equation*}
  \mathcal{S}=\sum_{i=1}^n(Y_t-\alpha)^2K_\gamma(X_i,x)
  \end{equation*}\]</span></p>
<p>with respect to the <em>local constant</em> <span class="math inline">\(\alpha\)</span></p></li>
<li class="fragment"><p>For technical details (bias, variance etc.) see <span class="citation" data-cites="racine_2019">Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2019</a>)</span></p></li>
</ul>
</section>
<section id="nonparametric-regression-7" class="slide level2 center">
<h2>Nonparametric Regression</h2>
<ul>
<li class="fragment"><p>A popular alternative of the local constant estimator is obtained by replacing the <em>constant</em> <span class="math inline">\(\alpha\)</span> in <span class="math inline">\(\mathcal{S}\)</span> by a linear regression function <span class="math inline">\(\alpha+\beta X\)</span> (this is called “local linear” regression - we simply call <code>npreg()</code> with the option <code>regtype="ll"</code>)</p></li>
<li class="fragment"><p>The popularity of this method arises from the fact that it has lower “boundary bias” than the local constant counterpart</p></li>
<li class="fragment"><p>However, it lacks one important feature of the local constant estimator, namely the ability to automatically remove irrelevant predictors without the need for pre-testing</p></li>
<li class="fragment"><p>A generalization can be found in the function <code>npglpreg()</code> in the R package <code>crs</code> that overcomes this limitation</p></li>
<li class="fragment"><p>See <span class="citation" data-cites="racine_2019">Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2019</a>)</span> for further details</p></li>
</ul>
</section>
<section id="simulated-data-illustration" class="slide level2 center">
<h2>Simulated Data Illustration</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/npregcos_40ca0c881d89c0dfb42fe6866f3e85b6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="fu">library</span>(np)</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb8-3"><a href="#cb8-3"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n))</span>
<span id="cb8-5"><a href="#cb8-5"></a>dgp <span class="ot">&lt;-</span> <span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>x)</span>
<span id="cb8-6"><a href="#cb8-6"></a>y <span class="ot">&lt;-</span> dgp <span class="sc">+</span> <span class="fu">rnorm</span>(n,<span class="at">sd=</span><span class="fl">0.25</span><span class="sc">*</span><span class="fu">sd</span>(dgp))</span>
<span id="cb8-7"><a href="#cb8-7"></a>ghat <span class="ot">&lt;-</span> <span class="fu">npreg</span>(y<span class="sc">~</span>x)</span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="fu">plot</span>(x,y,<span class="at">cex=</span><span class="fl">0.5</span>,<span class="at">col=</span><span class="st">"grey"</span>,<span class="at">xlab=</span><span class="st">"X"</span>,<span class="at">ylab=</span><span class="st">"Y"</span>)</span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="fu">lines</span>(x,dgp)</span>
<span id="cb8-10"><a href="#cb8-10"></a><span class="fu">lines</span>(x,<span class="fu">fitted</span>(ghat),<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="fu">abline</span>(ghat.ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x),<span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="fu">legend</span>(<span class="st">"top"</span>,<span class="fu">c</span>(<span class="st">"DGP"</span>,<span class="st">"Kernel"</span>,<span class="st">"OLS"</span>),<span class="at">col=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">1</span>,<span class="at">bty=</span><span class="st">"n"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/npregcos-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="simulated-data-illustration-ols" class="slide level2 smaller center">
<h2>Simulated Data Illustration: OLS</h2>
<p>How does the simple parametric linear model <code>ghat.ols</code> do for this simulated data?</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/cosolssummary_2f6033dc3c0d359af91e2cb59fbd6754">
<div class="cell-output-display">
<table style="width:88%;">
<colgroup>
<col style="width: 25%">
<col style="width: 15%">
<col style="width: 18%">
<col style="width: 13%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">&nbsp;</th>
<th style="text-align: center;">Estimate</th>
<th style="text-align: center;">Std. Error</th>
<th style="text-align: center;">t value</th>
<th style="text-align: center;">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>(Intercept)</strong></td>
<td style="text-align: center;">0.03148</td>
<td style="text-align: center;">0.04529</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">0.4872</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>x</strong></td>
<td style="text-align: center;">-0.04666</td>
<td style="text-align: center;">0.07966</td>
<td style="text-align: center;">-0.5858</td>
<td style="text-align: center;">0.5582</td>
</tr>
</tbody>
</table>
<table style="width:92%;">
<caption>Fitting linear model: y ~ x</caption>
<colgroup>
<col style="width: 20%">
<col style="width: 30%">
<col style="width: 16%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Observations</th>
<th style="text-align: center;">Residual Std. Error</th>
<th style="text-align: center;"><span class="math inline">\(R^2\)</span></th>
<th style="text-align: center;">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1000</td>
<td style="text-align: center;">0.7338</td>
<td style="text-align: center;">0.0003437</td>
<td style="text-align: center;">-0.000658</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>Oh oh… this linear parametric model truly sucks… the nonparametric model has an <span class="math inline">\(R^2\)</span> of <em>0.9411</em> (the parametric model has an <span class="math inline">\(R^2\)</span> of <em>0.0003437</em> and a <em>negative</em> adjusted <span class="math inline">\(R^2\)</span>)</p>
</div>
</div>
</div>
</section>
<section id="marginal-effects" class="slide level2 center">
<h2>Marginal Effects</h2>
<ul>
<li class="fragment"><p>The univariate <span class="math inline">\(X\)</span> <em>marginal effects function</em> is simply the first partial derivative function, and is defined as <span class="math display">\[\begin{align}
\beta(x)=\frac{d g(x)}{dx}
&amp;=\frac{f(x)m'(x)- m(x)f'(x)}{f^2(x)}\notag\\
&amp;=\frac{m'(x)}{f(x)}-g(x)\frac{f'(x)}{f(x)}
\label{beta(x)}
\end{align}\]</span></p></li>
<li class="fragment"><p>We construct <span class="math inline">\(\hat\beta(x)\)</span> by replacing <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(m'(x)\)</span>, <span class="math inline">\(m(x)\)</span>, and <span class="math inline">\(f'(x)\)</span> in <span class="math inline">\(\eqref{beta(x)}\)</span> with <span class="math inline">\(\hat f(x)\)</span>, <span class="math inline">\(\hat m'(x)\)</span>, <span class="math inline">\(\hat m(x)\)</span>, and <span class="math inline">\(\hat f'(x)\)</span></p></li>
</ul>
</section>
<section id="marginal-effects-1" class="slide level2 center">
<h2>Marginal Effects</h2>
<ul>
<li class="fragment"><p>The pointwise approximate bias (univariate <span class="math inline">\(X\)</span>), <span class="math inline">\(\operatorname{bias}(\hat\beta(x))\)</span>, is <span class="math display">\[\begin{equation*}\small
    \frac{h^2\kappa_2}{2}\left\{ g^{'''}(x) +2
      \left\{\frac{g'(x)f^{''}(x)}{f(x)}
        +\frac{g^{''}(x)f'(x)}{f(x)}
        -g'(x)\left(\frac{f'(x)}{f(x)}\right)^2 \right\}
    \right\}
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>The pointwise approximate variance (univariate <span class="math inline">\(X\)</span>) is <span class="math display">\[\begin{equation*}
    \operatorname{var}\hat\beta(x)=\frac{\sigma^2(x)}{nh^3f(x)}\int K^{'2}(z)\,dz
  \end{equation*}\]</span></p></li>
<li class="fragment"><p>We can use the asymptotic variance to construct pointwise confidence intervals replacing <span class="math inline">\(\sigma^2(x)\)</span> and <span class="math inline">\(f(x)\)</span> with their kernel estimates</p></li>
</ul>
</section>
<section id="marginal-effects-2" class="slide level2 center">
<h2>Marginal Effects</h2>
<ul>
<li class="fragment"><p>Recall the definitions of the conditional mean and marginal effects functions, <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(\beta(x)\)</span></p></li>
<li class="fragment"><p>They involve unknown joint and marginal densities and their derivatives, which are functions of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></p></li>
<li class="fragment"><p>If you were tasked with estimating <span class="math inline">\(\beta(x)\)</span>, it is hard to justify the assumption that <span class="math inline">\(\beta(x)\)</span> is <em>constant</em> (i.e., some constant <span class="math inline">\(\beta\)</span> that is <em>not</em> a function of <span class="math inline">\(x\)</span> like <span class="math inline">\(\beta(x)\)</span>)</p></li>
<li class="fragment"><p>But this is <em>exactly</em> what is assumed for the popular linear regression model (i.e., <span class="math inline">\(\beta(x)=d y/d x=\beta_1\)</span>, a <em>constant</em>)</p></li>
<li class="fragment"><p>Consider the following example that compares simple linear regression (<code>lm(y~x)</code>) with its kernel counterpart (<code>npreg(y~x)</code>)</p></li>
</ul>
</section>
<section id="example---simulated-marginal-effects" class="slide level2 center">
<h2>Example - Simulated Marginal Effects</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/cosgradient_54d2000d4b384eb8695890b1546d21db">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="fu">plot</span>(ghat,<span class="at">gradients=</span><span class="cn">TRUE</span>,<span class="at">neval=</span><span class="dv">250</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="fu">lines</span>(x,<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span><span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>x),<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">coef</span>(ghat.ols)[<span class="dv">2</span>],<span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>,<span class="fu">c</span>(<span class="st">"Kernel ME"</span>,<span class="st">"DGP ME"</span>,<span class="st">"Linear ME"</span>),<span class="at">col=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">1</span>,<span class="at">bty=</span><span class="st">"n"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/cosgradient-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="mixed-data-marginal-effects" class="slide level2 center">
<h2>Mixed Data Marginal Effects</h2>
<ul>
<li class="fragment"><p>Consider a multivariate conditional mean function <span class="math inline">\(g(x)\)</span> where <span class="math inline">\(x\)</span> is composed of <span class="math inline">\(q\)</span> continuous, <span class="math inline">\(r\)</span> unordered, and <span class="math inline">\(s\)</span> ordered predictors</p></li>
<li class="fragment"><p>As in the previous univariate example, if the <span class="math inline">\(j\)</span>th predictor is continuous, i.e., <span class="math inline">\(x_{j}\in\mathbb{R}\)</span>, then <span class="math inline">\(\hat\beta_j(x)\)</span> is the first partial derivative function of <span class="math inline">\(\hat g(x)\)</span> with respect to the <span class="math inline">\(j\)</span>th predictor, i.e., <span class="math display">\[\begin{equation*}
\hat\beta_j(x) = \frac{\partial\hat g(x)}{\partial x_j} = \frac{\hat m^{(j)}(x)}{\hat f(x)}-\hat g(x)\frac{\hat f^{(j)}(x)}{\hat f(x)}
\end{equation*}\]</span></p></li>
<li class="fragment"><p>See <span class="citation" data-cites="racine_2019">Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2019</a>)</span> for technical details</p></li>
</ul>
</section>
<section id="multivariate-mixed-data-marginal-effects" class="slide level2 center">
<h2>Multivariate Mixed-Data Marginal Effects</h2>
<ul>
<li class="fragment"><p>If the <span class="math inline">\(j\)</span>th predictor is unordered in <span class="math inline">\(\mathcal{D}=\{a,b,c\}\)</span>, then <span class="math inline">\(\hat\beta_j(x)\)</span> is the difference between <span class="math inline">\(\hat g(x)\)</span> when <span class="math inline">\(x^u_j=b\)</span> versus <span class="math inline">\(\hat g(x)\)</span> when <span class="math inline">\(x^u_j=a\)</span>, and between <span class="math inline">\(\hat g(x)\)</span> evaluated at <span class="math inline">\(x^u_j=c\)</span> versus at <span class="math inline">\(x^u_j=a\)</span> (<span class="math inline">\(a\)</span> is the <em>base</em> category) <span class="math display">\[\begin{equation*}
\hat\beta_j(x)=\hat g(x_{(-j)},x_{j}=l) - \hat g(x_{(-j)},x_{j}=a),\, l=b,c
\end{equation*}\]</span></p></li>
<li class="fragment"><p>If the <span class="math inline">\(j\)</span>th predictor is ordered we have two options, namely, to take differences as in the unordered case, or to take differences between <em>successive</em> elements of the ordered set (i.e., between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> then between <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span>)</p></li>
<li class="fragment"><p>See <span class="citation" data-cites="racine_2019">Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2019</a>)</span> for technical details</p></li>
</ul>
</section>
<section id="multivariate-regression" class="slide level2 center">
<h2>Multivariate Regression</h2>
<ul>
<li class="fragment"><p>We use Wooldridge’s <code>wage1</code> data containing numeric and categorical predictors</p></li>
<li class="fragment"><p>We regress <code>lwage</code> on categorical predictors <code>female</code> and <code>married</code> and numeric predictors <code>educ</code>, <code>exper</code> and <code>tenure</code></p></li>
<li class="fragment"><p>The formula for the regression models is</p>
<p><code>lwage~female+married+educ+exper+tenure</code></p></li>
<li class="fragment"><p>For the nonparametric model this just lists the predictors</p></li>
<li class="fragment"><p>For the parametric model it imposes linear structure</p></li>
</ul>
</section>
<section id="multivariate-nonparametric-regression" class="slide level2 smaller center">
<h2>Multivariate Nonparametric Regression</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1summary_5c5a1c6d78a1f77ef2ac9c028396d9a9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="fu">library</span>(np)</span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="fu">data</span>(wage1)</span>
<span id="cb10-3"><a href="#cb10-3"></a>?wage1</span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="do">## Help on topic 'wage1' was found in the following packages:</span></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="do">## </span></span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="do">##   Package               Library</span></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="do">##   crs                   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library</span></span>
<span id="cb10-8"><a href="#cb10-8"></a><span class="do">##   np                    /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library</span></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="do">## </span></span>
<span id="cb10-10"><a href="#cb10-10"></a><span class="do">## </span></span>
<span id="cb10-11"><a href="#cb10-11"></a><span class="do">## Using the first match ...</span></span>
<span id="cb10-12"><a href="#cb10-12"></a>ghat <span class="ot">&lt;-</span> <span class="fu">npreg</span>(lwage <span class="sc">~</span> female <span class="sc">+</span> married <span class="sc">+</span> educ <span class="sc">+</span> exper <span class="sc">+</span> tenure, <span class="at">data=</span>wage1, <span class="at">regtype=</span><span class="st">"ll"</span>)</span>
<span id="cb10-13"><a href="#cb10-13"></a><span class="fu">summary</span>(ghat)</span>
<span id="cb10-14"><a href="#cb10-14"></a><span class="do">## </span></span>
<span id="cb10-15"><a href="#cb10-15"></a><span class="do">## Regression Data: 526 training points, in 5 variable(s)</span></span>
<span id="cb10-16"><a href="#cb10-16"></a><span class="do">##                     female  married     educ    exper   tenure</span></span>
<span id="cb10-17"><a href="#cb10-17"></a><span class="do">## Bandwidth(s): 1.613829e-07 0.288114 5.148766 7.185586 28.00388</span></span>
<span id="cb10-18"><a href="#cb10-18"></a><span class="do">## </span></span>
<span id="cb10-19"><a href="#cb10-19"></a><span class="do">## Kernel Regression Estimator: Local-Linear</span></span>
<span id="cb10-20"><a href="#cb10-20"></a><span class="do">## Bandwidth Type: Fixed</span></span>
<span id="cb10-21"><a href="#cb10-21"></a><span class="do">## Residual standard error: 0.3682301</span></span>
<span id="cb10-22"><a href="#cb10-22"></a><span class="do">## R-squared: 0.5204168</span></span>
<span id="cb10-23"><a href="#cb10-23"></a><span class="do">## </span></span>
<span id="cb10-24"><a href="#cb10-24"></a><span class="do">## Continuous Kernel Type: Second-Order Gaussian</span></span>
<span id="cb10-25"><a href="#cb10-25"></a><span class="do">## No. Continuous Explanatory Vars.: 3</span></span>
<span id="cb10-26"><a href="#cb10-26"></a><span class="do">## </span></span>
<span id="cb10-27"><a href="#cb10-27"></a><span class="do">## Unordered Categorical Kernel Type: Aitchison and Aitken</span></span>
<span id="cb10-28"><a href="#cb10-28"></a><span class="do">## No. Unordered Categorical Explanatory Vars.: 2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="partial-regression-plots" class="slide level2 smaller center">
<h2>Partial Regression Plots</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1plot_f5a9cc6c5b6279a64cbf955cf1971983">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="do">## We run out of graph axis dimensions with &gt; 2 predictors, so it is common to</span></span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="do">## construct partial plots that plot the fitted model versus each predictor</span></span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="do">## separately holding the off-axis predictors at, say, their median value (you</span></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="do">## can change this - see ?npplot and the argument xq)</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb11-6"><a href="#cb11-6"></a><span class="fu">plot</span>(ghat,<span class="at">plot.errors.method=</span><span class="st">"bootstrap"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/wage1plot-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="marginal-effects-gradient-plots" class="slide level2 smaller center">
<h2>Marginal Effects (Gradient) Plots</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1gradientplot_b9441ff4d6839685535d7c21b248efd0">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="fu">plot</span>(ghat,<span class="at">gradients=</span><span class="cn">TRUE</span>,<span class="at">plot.errors.method=</span><span class="st">"bootstrap"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/wage1gradientplot-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="testing-h_0colonbeta_j0-ols-significance" class="slide level2 smaller center">
<h2>Testing <span class="math inline">\(H_0\colon\beta_j=0\)</span> (OLS Significance)</h2>
<p>A simple <span class="math inline">\(t\)</span>-test is used to test <span class="math inline">\(H_0\colon \beta_j=0\)</span></p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1sigtestols_2b371712bff225ce6326e6bdf1d0d42e">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>ghat.ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(lwage <span class="sc">~</span> female <span class="sc">+</span> married <span class="sc">+</span> educ <span class="sc">+</span> exper <span class="sc">+</span> tenure, <span class="at">data=</span>wage1)</span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="fu">summary</span>(ghat.ols)</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="do">## </span></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="do">## Call:</span></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="do">## lm(formula = lwage ~ female + married + educ + exper + tenure, </span></span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="do">##     data = wage1)</span></span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="do">## </span></span>
<span id="cb13-8"><a href="#cb13-8"></a><span class="do">## Residuals:</span></span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="do">##      Min       1Q   Median       3Q      Max </span></span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="do">## -1.87254 -0.27256 -0.03779  0.25349  1.23666 </span></span>
<span id="cb13-11"><a href="#cb13-11"></a><span class="do">## </span></span>
<span id="cb13-12"><a href="#cb13-12"></a><span class="do">## Coefficients:</span></span>
<span id="cb13-13"><a href="#cb13-13"></a><span class="do">##                    Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb13-14"><a href="#cb13-14"></a><span class="do">## (Intercept)        0.330268   0.106391   3.104  0.00201 ** </span></span>
<span id="cb13-15"><a href="#cb13-15"></a><span class="do">## femaleMale         0.285530   0.037264   7.662 9.00e-14 ***</span></span>
<span id="cb13-16"><a href="#cb13-16"></a><span class="do">## marriedNotmarried -0.125739   0.039986  -3.145  0.00176 ** </span></span>
<span id="cb13-17"><a href="#cb13-17"></a><span class="do">## educ               0.083905   0.006973  12.033  &lt; 2e-16 ***</span></span>
<span id="cb13-18"><a href="#cb13-18"></a><span class="do">## exper              0.003134   0.001682   1.863  0.06300 .  </span></span>
<span id="cb13-19"><a href="#cb13-19"></a><span class="do">## tenure             0.016867   0.002955   5.707 1.93e-08 ***</span></span>
<span id="cb13-20"><a href="#cb13-20"></a><span class="do">## ---</span></span>
<span id="cb13-21"><a href="#cb13-21"></a><span class="do">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span id="cb13-22"><a href="#cb13-22"></a><span class="do">## </span></span>
<span id="cb13-23"><a href="#cb13-23"></a><span class="do">## Residual standard error: 0.4125 on 520 degrees of freedom</span></span>
<span id="cb13-24"><a href="#cb13-24"></a><span class="do">## Multiple R-squared:  0.4036, Adjusted R-squared:  0.3979 </span></span>
<span id="cb13-25"><a href="#cb13-25"></a><span class="do">## F-statistic: 70.38 on 5 and 520 DF,  p-value: &lt; 2.2e-16</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="testing-h_0colonbeta_jx0-significance" class="slide level2 center">
<h2>Testing <span class="math inline">\(H_0\colon\beta_j(x)=0\)</span> (Significance)</h2>
<ul>
<li class="fragment"><p>The null and alternative hypotheses are <span class="math display">\[\begin{align*}
H_0\colon\quad&amp; \beta_j(x) = 0\hbox{ for all } x \hbox{ (a.e.)}\\
H_A\colon\quad&amp; \beta_j(x) \ne 0 \hbox{ for some }x\hbox{ on a set
with + measure}
\end{align*}\]</span></p></li>
<li class="fragment"><p>A feasible test statistic <span class="math inline">\(\hat\lambda\ge 0\)</span> is given by <span class="math display">\[\begin{equation*}
\hat\lambda=\left\{
  \begin{array}{ll}
     n^{-1}\sum_{i=1}^n \hat\beta_j(X_i)^2&amp;\text{ if }x_j\in\mathbb{R}\\
     n^{-1}\sum_{i=1}^n \sum_{l=1}^{c-1}\hat\beta_j(X_i)^2 &amp; \text{ if }x_j\in\mathcal{D}
   \end{array}
\right.
\end{equation*}\]</span></p></li>
<li class="fragment"><p>A bootstrap procedure provides the null distribution (<span class="citation" data-cites="RACINE:1997">Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">1997</a>)</span>, <span class="citation" data-cites="RACINE_HART_LI:2006">Racine, Hart, and Li (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2006</a>)</span>)</p></li>
</ul>
</section>
<section id="testing-h_0colonbeta_jx0-significance-1" class="slide level2 smaller center">
<h2>Testing <span class="math inline">\(H_0\colon\beta_j(x)=0\)</span> (Significance)</h2>
<p>A bootstrap <span class="math inline">\(\lambda\)</span>-test is used to test <span class="math inline">\(H_0\colon \beta_j(x)=0\,\forall\, x\)</span> (a.e.)</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1sigtestnp_4dd25c88bee0cc79d2f521bc04f32b29">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="fu">npsigtest</span>(ghat)</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="do">## </span></span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="do">## Kernel Regression Significance Test</span></span>
<span id="cb14-4"><a href="#cb14-4"></a><span class="do">## Type I Test with IID Bootstrap (399 replications, Pivot = TRUE, joint = FALSE)</span></span>
<span id="cb14-5"><a href="#cb14-5"></a><span class="do">## Explanatory variables tested for significance:</span></span>
<span id="cb14-6"><a href="#cb14-6"></a><span class="do">## female (1), married (2), educ (3), exper (4), tenure (5)</span></span>
<span id="cb14-7"><a href="#cb14-7"></a><span class="do">## </span></span>
<span id="cb14-8"><a href="#cb14-8"></a><span class="do">##                     female  married     educ    exper   tenure</span></span>
<span id="cb14-9"><a href="#cb14-9"></a><span class="do">## Bandwidth(s): 1.613829e-07 0.288114 5.148766 7.185586 28.00388</span></span>
<span id="cb14-10"><a href="#cb14-10"></a><span class="do">## </span></span>
<span id="cb14-11"><a href="#cb14-11"></a><span class="do">## Individual Significance Tests</span></span>
<span id="cb14-12"><a href="#cb14-12"></a><span class="do">## P Value: </span></span>
<span id="cb14-13"><a href="#cb14-13"></a><span class="do">## female  &lt; 2.22e-16 *** </span></span>
<span id="cb14-14"><a href="#cb14-14"></a><span class="do">## married 0.0050125 ** </span></span>
<span id="cb14-15"><a href="#cb14-15"></a><span class="do">## educ    &lt; 2.22e-16 *** </span></span>
<span id="cb14-16"><a href="#cb14-16"></a><span class="do">## exper   &lt; 2.22e-16 *** </span></span>
<span id="cb14-17"><a href="#cb14-17"></a><span class="do">## tenure  &lt; 2.22e-16 ***</span></span>
<span id="cb14-18"><a href="#cb14-18"></a><span class="do">## ---</span></span>
<span id="cb14-19"><a href="#cb14-19"></a><span class="do">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="counterfactuals" class="slide level2 center">
<h2>Counterfactuals</h2>
<ul>
<li class="fragment"><p>The R function <code>fitted(...)</code> extracts fitted values for each sample observation</p></li>
<li class="fragment"><p>Suppose we want fitted values for <em>specific value(s)</em> of the predictor(s)</p></li>
<li class="fragment"><p>We use the R function <code>predict(...,newdata=...)</code> where <code>newdata</code> points to a data frame containing <em>named and cast</em> <span class="math inline">\(X\)</span> values for which we want predictions</p></li>
<li class="fragment"><p>Let’s generate a data frame called <code>df</code> containing 1 row to generate the predicted log-wage for single males having median education, job tenure and job experience</p></li>
</ul>
</section>
<section id="counterfactuals-1" class="slide level2 center">
<h2>Counterfactuals</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1preddf_5fb783da0f8780f81a771e04dec7b752">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="fu">attach</span>(wage1)</span>
<span id="cb15-2"><a href="#cb15-2"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">female =</span> <span class="fu">factor</span>(<span class="st">"Male"</span>, <span class="at">levels=</span><span class="fu">levels</span>(female)),</span>
<span id="cb15-3"><a href="#cb15-3"></a>                 <span class="at">married =</span> <span class="fu">factor</span>(<span class="st">"Notmarried"</span>, <span class="at">levels=</span><span class="fu">levels</span>(married)),</span>
<span id="cb15-4"><a href="#cb15-4"></a>                 <span class="at">educ =</span> <span class="fu">median</span>(educ),</span>
<span id="cb15-5"><a href="#cb15-5"></a>                 <span class="at">tenure =</span> <span class="fu">median</span>(tenure),</span>
<span id="cb15-6"><a href="#cb15-6"></a>                 <span class="at">exper =</span> <span class="fu">median</span>(exper))</span>
<span id="cb15-7"><a href="#cb15-7"></a><span class="fu">head</span>(df)</span>
<span id="cb15-8"><a href="#cb15-8"></a><span class="do">##   female    married educ tenure exper</span></span>
<span id="cb15-9"><a href="#cb15-9"></a><span class="do">## 1   Male Notmarried   12      2  13.5</span></span>
<span id="cb15-10"><a href="#cb15-10"></a><span class="fu">predict</span>(ghat, <span class="at">newdata=</span>df)</span>
<span id="cb15-11"><a href="#cb15-11"></a><span class="do">## [1] 1.699062</span></span>
<span id="cb15-12"><a href="#cb15-12"></a><span class="do">## Or you could use ghat &lt;- npreg(...,newdata=df) and fitted(ghat) </span></span>
<span id="cb15-13"><a href="#cb15-13"></a><span class="do">## ghat &lt;- npreg(lwage ~ female + married + educ + exper + tenure, </span></span>
<span id="cb15-14"><a href="#cb15-14"></a><span class="do">##               data=wage1,</span></span>
<span id="cb15-15"><a href="#cb15-15"></a><span class="do">##               regtype="ll", </span></span>
<span id="cb15-16"><a href="#cb15-16"></a><span class="do">##               newdata=df)</span></span>
<span id="cb15-17"><a href="#cb15-17"></a><span class="do">## fitted(ghat)</span></span>
<span id="cb15-18"><a href="#cb15-18"></a><span class="do">## Note - If `df` had multiple rows we would get a prediction for each</span></span>
<span id="cb15-19"><a href="#cb15-19"></a><span class="do">## row (i.e., a vector of predictions)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section>
<section>
<section id="nonparametric-time-series-modelling" class="title-slide slide level1 center">
<h1>Nonparametric Time-Series Modelling</h1>

</section>
<section id="r-and-time-series-data-types" class="slide level2 smaller center">
<h2>R and Time-Series Data Types</h2>
<ul>
<li class="fragment"><p>In R, one can create time-series from numeric data representing data sampled at equispaced points in time</p></li>
<li class="fragment"><p>To do this, simply cast the data object as <code>ts()</code> (per the following example)</p></li>
<li class="fragment"><p>After this, R functions will act appropriately (e.g., <code>plot()</code> recognizes <code>ts()</code> so the horizontal axis is time, lines connect points, etc.)</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/tscasting_07a131d8da0978aaf890b9fb454f2690">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="do">## Example 1 - create a simple time series that begins on the 2nd Quarter of</span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="do">## 1959</span></span>
<span id="cb16-3"><a href="#cb16-3"></a>x <span class="ot">&lt;-</span> <span class="fu">ts</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="at">frequency =</span> <span class="dv">4</span>, <span class="at">start =</span> <span class="fu">c</span>(<span class="dv">1959</span>, <span class="dv">2</span>))</span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="fu">print</span>(x)</span>
<span id="cb16-5"><a href="#cb16-5"></a><span class="fu">plot</span>(x)</span>
<span id="cb16-6"><a href="#cb16-6"></a><span class="do">## Excample 2 - create simulated data from an ARIMA model that begins on the 2nd</span></span>
<span id="cb16-7"><a href="#cb16-7"></a><span class="do">## Quarter of 1959</span></span>
<span id="cb16-8"><a href="#cb16-8"></a>x <span class="ot">&lt;-</span> <span class="fu">ts</span>(<span class="fu">arima.sim</span>(<span class="at">n =</span> <span class="dv">10</span>, <span class="fu">list</span>(<span class="at">ar =</span> <span class="fu">c</span>(<span class="fl">0.8897</span>, <span class="sc">-</span><span class="fl">0.4858</span>), </span>
<span id="cb16-9"><a href="#cb16-9"></a>                               <span class="at">ma =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.2279</span>, <span class="fl">0.2488</span>)),</span>
<span id="cb16-10"><a href="#cb16-10"></a>                  <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fl">0.1796</span>)),</span>
<span id="cb16-11"><a href="#cb16-11"></a>        <span class="at">frequency =</span> <span class="dv">4</span>, <span class="at">start =</span> <span class="fu">c</span>(<span class="dv">1959</span>, <span class="dv">2</span>))</span>
<span id="cb16-12"><a href="#cb16-12"></a><span class="fu">print</span>(x)</span>
<span id="cb16-13"><a href="#cb16-13"></a><span class="fu">plot</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
</ul>
</section>
<section id="r-and-time-series-data-types-1" class="slide level2 smaller center">
<h2>R and Time-Series Data Types</h2>
<ul>
<li class="fragment"><p>The function <code>lag()</code> shifts the time base back by a given number of observations</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/tslag_d8bbdacdb40d252e3940020fb6689fde">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="do">## ldeaths is monthly deaths from bronchitis, emphysema and asthma in the UK,</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="do">## 1974–1979, both sexes</span></span>
<span id="cb17-3"><a href="#cb17-3"></a>ldeaths</span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="do">## lag(x, k=1, ...), note the sign of k: a series lagged by a positive k, i.e.,</span></span>
<span id="cb17-5"><a href="#cb17-5"></a><span class="do">## y_{t+k}</span></span>
<span id="cb17-6"><a href="#cb17-6"></a><span class="fu">lag</span>(ldeaths, <span class="dv">0</span>) <span class="co"># returns original series y_t</span></span>
<span id="cb17-7"><a href="#cb17-7"></a><span class="fu">lag</span>(ldeaths, <span class="sc">-</span><span class="dv">1</span>) <span class="co"># returns series lagged once y_{t-1}</span></span>
<span id="cb17-8"><a href="#cb17-8"></a><span class="fu">lag</span>(ldeaths, <span class="sc">-</span><span class="dv">2</span>) <span class="co"># returns series lagged twice y_{t-2}</span></span>
<span id="cb17-9"><a href="#cb17-9"></a><span class="do">## In matrix form compare to cbind(ldeaths,lag(ldeaths,-1),lag(ldeaths,-2))</span></span>
<span id="cb17-10"><a href="#cb17-10"></a><span class="fu">cbind</span>(ldeaths,<span class="fu">lag</span>(ldeaths,<span class="sc">-</span><span class="dv">1</span>),<span class="fu">lag</span>(ldeaths,<span class="sc">-</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
<li class="fragment"><p>The function <code>diff()</code> returns suitably lagged and iterated differences</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/tsdiff_442972af28b9af82e38e024cbab9663c">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="fu">diff</span>(ldeaths)</span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="do">## Manually compute first difference</span></span>
<span id="cb18-3"><a href="#cb18-3"></a>ldeaths[<span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(ldeaths)]<span class="sc">-</span>ldeaths[<span class="dv">1</span><span class="sc">:</span>(<span class="fu">length</span>(ldeaths)<span class="sc">-</span><span class="dv">1</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
<li class="fragment"><p>So, you can see that by casting a univariate numeric vector as a time series via <code>ts()</code> it inherits certain properties that make subsequent analysis easier than otherwise</p></li>
<li class="fragment"><p>The R package <code>np</code> supports <code>ts()</code> data for a range of functions (we will investigate univariate <em>nonlinear</em> kernel models alongside <em>linear</em> parametric models)</p></li>
</ul>
</section>
<section id="r-and-time-series-data-types-2" class="slide level2 center">
<h2>R and Time-Series Data Types</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment"><p>It may take some time to grow accustomed to R and its implementation of <code>ts()</code> objects</p></li>
<li class="fragment"><p>Not all functions support <code>ts()</code> objects as one might expect</p></li>
<li class="fragment"><p>In particular, <code>lm()</code> (linear regression) will not work as some might expect, beware!</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/tsRlag_65ad4599c09b28c543b5ccd66391596c">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="fu">library</span>(np)</span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="do">## Simulate a simple AR(1) process with lag coefficient 0.9</span></span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb19-4"><a href="#cb19-4"></a>y <span class="ot">&lt;-</span> <span class="fu">arima.sim</span>(<span class="at">n =</span> n, <span class="fu">list</span>(<span class="at">ar =</span> <span class="fu">c</span>(<span class="fl">0.9</span>)))</span>
<span id="cb19-5"><a href="#cb19-5"></a><span class="do">## Using lag() in lm() calls will fail (same result regardless of lag, R^2=1,</span></span>
<span id="cb19-6"><a href="#cb19-6"></a><span class="do">## lag() is totally ignored so you are regressing y_t on y_t though naturally </span></span>
<span id="cb19-7"><a href="#cb19-7"></a><span class="do">## you may not suspect that is what would happen in this case)</span></span>
<span id="cb19-8"><a href="#cb19-8"></a>ghat.ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span><span class="fu">lag</span>(y,<span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb19-9"><a href="#cb19-9"></a><span class="fu">summary</span>(ghat.ols)</span>
<span id="cb19-10"><a href="#cb19-10"></a><span class="do">## But if you manually regress numeric y_t on y_{t-1} you get the expected</span></span>
<span id="cb19-11"><a href="#cb19-11"></a><span class="do">## result (try also ar.ols(y, order.max = 1) or arima(y, order = c(1,0,0))). </span></span>
<span id="cb19-12"><a href="#cb19-12"></a><span class="do">## Here we regress (y_t,...,y_2) on (y_{t-1},...,y_1) using numeric vectors</span></span>
<span id="cb19-13"><a href="#cb19-13"></a>ghat.ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(y[<span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(y)]<span class="sc">~</span>y[<span class="dv">1</span><span class="sc">:</span>(<span class="fu">length</span>(y)<span class="sc">-</span><span class="dv">1</span>)])</span>
<span id="cb19-14"><a href="#cb19-14"></a><span class="fu">summary</span>(ghat.ols)</span>
<span id="cb19-15"><a href="#cb19-15"></a><span class="do">## npreg() does support ts() objects. In the example below we use the npreg()</span></span>
<span id="cb19-16"><a href="#cb19-16"></a><span class="do">## function to conduct simple OLS by noting that local linear regression with a</span></span>
<span id="cb19-17"><a href="#cb19-17"></a><span class="do">## large bandwidth is linear least squares, so the gradient will be a vector of</span></span>
<span id="cb19-18"><a href="#cb19-18"></a><span class="do">## constants equal to the least squares coefficient for lag(y,-1) - think of the</span></span>
<span id="cb19-19"><a href="#cb19-19"></a><span class="do">## lag() function and notation y_{t-1} or "why tee minus one" hence the -1 here</span></span>
<span id="cb19-20"><a href="#cb19-20"></a>ghat <span class="ot">&lt;-</span> <span class="fu">npreg</span>(y<span class="sc">~</span><span class="fu">lag</span>(y,<span class="sc">-</span><span class="dv">1</span>),<span class="at">regtype=</span><span class="st">"ll"</span>,<span class="at">bws=</span><span class="dv">10</span><span class="sc">^</span><span class="dv">5</span>,<span class="at">bandwidth.compute=</span><span class="cn">FALSE</span>,<span class="at">gradients=</span><span class="cn">TRUE</span>)</span>
<span id="cb19-21"><a href="#cb19-21"></a><span class="fu">tail</span>(<span class="fu">gradients</span>(ghat))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
</ul>
</div>
</div>
</div>
</section>
<section id="time-series-kernel-regression" class="slide level2 center">
<h2>Time Series Kernel Regression</h2>
<ul>
<li class="fragment"><p>The pointwise bias and variance formulas provided previously for kernel estimation were originally obtained in the i.i.d.&nbsp;setting</p></li>
<li class="fragment"><p>For <em>stationary</em> time series, the pointwise bias and variance expressions for density, conditional density, and conditional moment estimation are <em>identical</em> to those obtained for the i.i.d.&nbsp;case</p></li>
<li class="fragment"><p>For instance, for weakly dependent processes, <span class="math inline">\(\hat g(x)\)</span> has the same order MSE as for the i.i.d.&nbsp;case, and its asymptotic distribution is the same as that for the i.i.d.&nbsp;case</p></li>
<li class="fragment"><p>For the non-stationary case, see <span class="citation" data-cites="racine_2019">Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2019</a>)</span> for references and details</p></li>
</ul>
</section>
<section id="nonparametric-ar-models" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<ul>
<li class="fragment"><p>For stationary time series, bandwidth selection, estimation, and inference can proceed in general <em>without modification</em>; only the non-stationary case requires a separate treatment, which is to be expected</p></li>
<li class="fragment"><p>The function <code>npreg()</code> in the R package np supports time series objects and the R <code>lag()</code> function can be used to generate the desired lags for a series, as the following code chunk illustrates</p></li>
<li class="fragment"><p>The figure on the following slide presents a simulated time series and the local constant kernel estimate</p></li>
<li class="fragment"><p>The DGP is given by <span class="math inline">\(Y_t=0.9Y_{t-1}+\epsilon_t\)</span>, a linear stationary AR(1) process</p></li>
<li class="fragment"><p>Given the uncertainty regarding the appropriate lag order, we include four lags of <span class="math inline">\(Y_t\)</span> in the nonparametric model</p></li>
</ul>
</section>
<section id="nonparametric-ar-models-1" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/arimatsnpregcode_da77653b41bc5c06597f61ef2356d8ec">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="fu">library</span>(np)</span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="do">## Simulate data from a stationary univariate AR(1) process</span></span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb20-4"><a href="#cb20-4"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb20-5"><a href="#cb20-5"></a>y <span class="ot">&lt;-</span> <span class="fu">arima.sim</span>(<span class="at">n =</span> n, <span class="fu">list</span>(<span class="at">ar =</span> <span class="fu">c</span>(<span class="fl">0.9</span>)))</span>
<span id="cb20-6"><a href="#cb20-6"></a><span class="do">## Conduct local constant estimation of y on four lags</span></span>
<span id="cb20-7"><a href="#cb20-7"></a>ghat.<span class="fl">4.</span>lags <span class="ot">&lt;-</span> <span class="fu">npreg</span>(y<span class="sc">~</span><span class="fu">lag</span>(y,<span class="sc">-</span><span class="dv">1</span>)<span class="sc">+</span><span class="fu">lag</span>(y,<span class="sc">-</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">lag</span>(y,<span class="sc">-</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">lag</span>(y,<span class="sc">-</span><span class="dv">4</span>))</span>
<span id="cb20-8"><a href="#cb20-8"></a><span class="do">## Re-fit the model using only 1 lag</span></span>
<span id="cb20-9"><a href="#cb20-9"></a>ghat.<span class="fl">1.</span>lag <span class="ot">&lt;-</span> <span class="fu">npreg</span>(y<span class="sc">~</span><span class="fu">lag</span>(y,<span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb20-10"><a href="#cb20-10"></a><span class="do">## cbind(ts(fitted(ghat.4.lags),start=5),ts(fitted(ghat.1.lag),start=2))</span></span>
<span id="cb20-11"><a href="#cb20-11"></a><span class="do">## Plot the data and nonparametric fit</span></span>
<span id="cb20-12"><a href="#cb20-12"></a><span class="fu">plot</span>(y,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb20-13"><a href="#cb20-13"></a><span class="fu">lines</span>(<span class="fu">ts</span>(<span class="fu">fitted</span>(ghat.<span class="fl">4.</span>lags),<span class="at">start=</span><span class="dv">5</span>),<span class="at">col=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb20-14"><a href="#cb20-14"></a><span class="fu">points</span>(<span class="fu">ts</span>(<span class="fu">fitted</span>(ghat.<span class="fl">1.</span>lag),<span class="at">start=</span><span class="dv">2</span>),<span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb20-15"><a href="#cb20-15"></a><span class="fu">legend</span>(<span class="st">"topright"</span>,<span class="fu">c</span>(<span class="st">"Series"</span>,</span>
<span id="cb20-16"><a href="#cb20-16"></a>                    <span class="st">"npreg() including 4 lags"</span>,</span>
<span id="cb20-17"><a href="#cb20-17"></a>                    <span class="st">"npreg() including 1 lag"</span>),</span>
<span id="cb20-18"><a href="#cb20-18"></a>                    <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="cn">NA</span>),<span class="at">col=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,<span class="at">lwd=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="cn">NA</span>),<span class="at">pch=</span><span class="fu">c</span>(<span class="cn">NA</span>,<span class="cn">NA</span>,<span class="dv">1</span>),<span class="at">bty=</span><span class="st">"n"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="nonparametric-ar-models-2" class="slide level2 center">
<h2>Nonparametric AR Models</h2>

<img data-src="index_files/figure-revealjs/fig-arimatsnpreg-1.png" width="960" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;2: Local constant estimation of a stationary univariate AR(1) time series, <span class="math inline">\(Y_t=0.9Y_{t-1}+\epsilon_t\)</span></p></section>
<section id="nonparametric-ar-models-3" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/oahetagewpq_b63c81c8e1836e0881845fed856ca822">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a><span class="do">## Examine the model summary</span></span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="fu">summary</span>(ghat.<span class="fl">4.</span>lags)</span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="do">## </span></span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="do">## Regression Data: 96 training points, in 4 variable(s)</span></span>
<span id="cb21-5"><a href="#cb21-5"></a><span class="do">##               lag(y, -1) lag(y, -2) lag(y, -3) lag(y, -4)</span></span>
<span id="cb21-6"><a href="#cb21-6"></a><span class="do">## Bandwidth(s):  0.6519115   1.951533  149650850  265525833</span></span>
<span id="cb21-7"><a href="#cb21-7"></a><span class="do">## </span></span>
<span id="cb21-8"><a href="#cb21-8"></a><span class="do">## Kernel Regression Estimator: Local-Constant</span></span>
<span id="cb21-9"><a href="#cb21-9"></a><span class="do">## Bandwidth Type: Fixed</span></span>
<span id="cb21-10"><a href="#cb21-10"></a><span class="do">## Residual standard error: 0.8801438</span></span>
<span id="cb21-11"><a href="#cb21-11"></a><span class="do">## R-squared: 0.8245336</span></span>
<span id="cb21-12"><a href="#cb21-12"></a><span class="do">## </span></span>
<span id="cb21-13"><a href="#cb21-13"></a><span class="do">## Continuous Kernel Type: Second-Order Gaussian</span></span>
<span id="cb21-14"><a href="#cb21-14"></a><span class="do">## No. Continuous Explanatory Vars.: 4</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="nonparametric-ar-models-4" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<ul>
<li class="fragment"><p>Note how the bandwidths are large for lags that exceed 1, which arises because we are using the local constant estimator, cross-validated bandwidth selection, and a product kernel which jointly possess the ability to automatically remove irrelevant predictors without the need for pretesting <span class="citation" data-cites="HALL_LI_RACINE:2007">(<a href="#/references-scrollable" role="doc-biblioref" onclick="">Hall, Li, and Racine 2007</a>)</span></p></li>
<li class="fragment"><p>If <span class="math inline">\(h_k\to\infty\)</span>, then <span class="math inline">\(K((Y_{t-k}-y_{t-k})/h_k)\to K(0)\)</span>, thus <span class="math inline">\(\small \prod_{k=1}^4K((Y_{t-k}-y_{t-k})/h_k)=K((Y_{t-1}-y_{t-1})/h_1)K(0)^3\)</span> hence <span class="math display">\[\begin{equation*}
\small
\hat g(x)=\frac{\sum_{t=5}^tY_tK((Y_{t-1}-y_{t-1})/h_1)K(0)^3}{\sum_{t=5}^tK((Y_{t-1}-y_{t-1})/h_1)K(0)^3}
=\frac{\sum_{t=5}^tY_tK((Y_{t-1}-y_{t-1})/h_1)}{\sum_{t=5}^tK((Y_{t-1}-y_{t-1})/h_1)}
\end{equation*}\]</span></p></li>
<li class="fragment"><p>This then collapses to a one-predictor local constant kernel estimator, not a four-predictor estimator</p></li>
</ul>
</section>
<section id="nonparametric-ar-models-5" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<ul>
<li class="fragment"><p>If lags 2, 3, and 4 are irrelevant and if the cross-validated <span class="math inline">\(\hat h_2\)</span>, <span class="math inline">\(\hat h_3\)</span>, and <span class="math inline">\(\hat h_4\)</span> are large, then cross-validation has reduced a four-dimensional nonparametric estimator to a one-dimensional one possessing a (fast) one-dimensional rate of convergence</p></li>
<li class="fragment"><p>The intuition underlying this feature is straightforward</p></li>
<li class="fragment"><p>If a predictor is irrelevant but unknowingly included in a local constant regression, there is no bias arising from its presence</p></li>
<li class="fragment"><p>The estimator bias with respect to this predictor is the same regardless of the size of the bandwidth</p></li>
<li class="fragment"><p>However, the noise arising from the presence of this predictor increases the overall variability of the estimator</p></li>
<li class="fragment"><p>The variance falls the larger is the bandwidth associated with this predictor</p></li>
</ul>
</section>
<section id="nonparametric-ar-models-6" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<ul>
<li class="fragment"><p>Since cross-validation optimizes the square-error properties of the estimator, it is optimal to assign a large value of the bandwidth to this predictor since this does not affect bias but reduces overall variability, which essentially removes it from the resulting estimate as described above</p></li>
<li class="fragment"><p>Cross-validation can differentiate the relevant from the irrelevant predictors by oversmoothing the latter and optimally smoothing the former, and is one reason why we might not want to write-off the local constant estimator solely on the basis of the presence of boundary bias</p></li>
<li class="fragment"><p>In effect, local constant cross-validation is a dimensionality reduction procedure in the presence of irrelevant predictors, a feature not shared by its parametric counterparts</p></li>
</ul>
</section>
<section id="nonparametric-ar-models-7" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<ul>
<li class="fragment"><p>The upshot is that even though we included four lags of <span class="math inline">\(Y\)</span> due to uncertainty about how many lags to incorporate, the cross-validated local constant estimator has in effect <em>smoothed out</em> lags 2, 3, and 4</p></li>
<li class="fragment"><p>This results in an estimator that in effect includes only the relevant predictor, which happens to be the first lag of <span class="math inline">\(Y\)</span></p></li>
<li class="fragment"><p>Applying the consistent test for parameter relevance detailed earlier indicates that lags 2, 3, and 4 are not relevant predictors, which is the case</p></li>
<li class="fragment"><p>Refitting the model with only one lag produces the same fit which confirms that both estimates use only one predictor</p></li>
</ul>
</section>
<section id="nonparametric-ar-models-lag-inference" class="slide level2 center">
<h2>Nonparametric AR Models (Lag Inference)</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/arimatsnpregsig_0ab07cfc5301b654f3414a3bfe20143b">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="do">## Conduct a nonparametric significance test</span></span>
<span id="cb22-2"><a href="#cb22-2"></a><span class="fu">npsigtest</span>(ghat.<span class="fl">4.</span>lags)</span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="do">## </span></span>
<span id="cb22-4"><a href="#cb22-4"></a><span class="do">## Kernel Regression Significance Test</span></span>
<span id="cb22-5"><a href="#cb22-5"></a><span class="do">## Type I Test with IID Bootstrap (399 replications, Pivot = TRUE, joint = FALSE)</span></span>
<span id="cb22-6"><a href="#cb22-6"></a><span class="do">## Explanatory variables tested for significance:</span></span>
<span id="cb22-7"><a href="#cb22-7"></a><span class="do">## lag(y, -1) (1), lag(y, -2) (2), lag(y, -3) (3), lag(y, -4) (4)</span></span>
<span id="cb22-8"><a href="#cb22-8"></a><span class="do">## </span></span>
<span id="cb22-9"><a href="#cb22-9"></a><span class="do">##               lag(y, -1) lag(y, -2) lag(y, -3) lag(y, -4)</span></span>
<span id="cb22-10"><a href="#cb22-10"></a><span class="do">## Bandwidth(s):  0.6519115   1.951533  149650850  265525833</span></span>
<span id="cb22-11"><a href="#cb22-11"></a><span class="do">## </span></span>
<span id="cb22-12"><a href="#cb22-12"></a><span class="do">## Individual Significance Tests</span></span>
<span id="cb22-13"><a href="#cb22-13"></a><span class="do">## P Value: </span></span>
<span id="cb22-14"><a href="#cb22-14"></a><span class="do">## lag(y, -1) &lt; 2e-16 *** </span></span>
<span id="cb22-15"><a href="#cb22-15"></a><span class="do">## lag(y, -2) 0.46366  </span></span>
<span id="cb22-16"><a href="#cb22-16"></a><span class="do">## lag(y, -3) 0.96491  </span></span>
<span id="cb22-17"><a href="#cb22-17"></a><span class="do">## lag(y, -4) 0.89474 </span></span>
<span id="cb22-18"><a href="#cb22-18"></a><span class="do">## ---</span></span>
<span id="cb22-19"><a href="#cb22-19"></a><span class="do">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="nonparametric-ar-models-8" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<ul>
<li class="fragment"><p>The generalized local polynomial estimator <span class="citation" data-cites="HALL_RACINE:2015">(<a href="#/references-scrollable" role="doc-biblioref" onclick="">Hall and Racine 2015</a>)</span> uses cross-validation to select the bandwidth vector <em>and</em> polynomial degree vector, as opposed to the ad hoc choice of <code>regtype="lc"</code> or <code>regtype="ll"</code> (polynomials or order 0 or 1, respectively)</p></li>
<li class="fragment"><p>In the case of the linear time series AR(1) DGP used above with lag and polynomial degree uncertainty, we examine how it performs</p></li>
<li class="fragment"><p>We will also fit a correctly specified <em>linear parametric</em> AR(1) model and compare the first few fitted values</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/npglpregtsar1_ae8f3ab9e520188231b2576166cca98e">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="do">## npglpreg() does not support time series objects, so we manually</span></span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="do">## create the lagged series</span></span>
<span id="cb23-3"><a href="#cb23-3"></a><span class="fu">library</span>(crs)</span>
<span id="cb23-4"><a href="#cb23-4"></a><span class="fu">options</span>(<span class="at">crs.messages=</span><span class="cn">FALSE</span>)</span>
<span id="cb23-5"><a href="#cb23-5"></a>y.lag<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="cn">NA</span>,<span class="dv">1</span>),y[<span class="dv">1</span><span class="sc">:</span>(n<span class="dv">-1</span>)])</span>
<span id="cb23-6"><a href="#cb23-6"></a>y.lag<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="cn">NA</span>,<span class="dv">2</span>),y[<span class="dv">1</span><span class="sc">:</span>(n<span class="dv">-2</span>)])</span>
<span id="cb23-7"><a href="#cb23-7"></a>y.lag<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="cn">NA</span>,<span class="dv">3</span>),y[<span class="dv">1</span><span class="sc">:</span>(n<span class="dv">-3</span>)])</span>
<span id="cb23-8"><a href="#cb23-8"></a>y.lag<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="cn">NA</span>,<span class="dv">4</span>),y[<span class="dv">1</span><span class="sc">:</span>(n<span class="dv">-4</span>)])</span>
<span id="cb23-9"><a href="#cb23-9"></a>model.glp <span class="ot">&lt;-</span> <span class="fu">npglpreg</span>(y<span class="sc">~</span>y.lag<span class="fl">.1</span><span class="sc">+</span>y.lag<span class="fl">.2</span><span class="sc">+</span>y.lag<span class="fl">.3</span><span class="sc">+</span>y.lag<span class="fl">.4</span>,<span class="at">nmulti=</span><span class="dv">5</span>,<span class="at">degree.max=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
</ul>
</section>
<section id="nonparametric-ar-models-9" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/npglreuhaiteh_4386352cc472814502c8be6b73bcfad0">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="fu">summary</span>(model.glp)</span>
<span id="cb24-2"><a href="#cb24-2"></a><span class="do">## Call:</span></span>
<span id="cb24-3"><a href="#cb24-3"></a><span class="do">## npglpreg.formula(formula = y ~ y.lag.1 + y.lag.2 + y.lag.3 + </span></span>
<span id="cb24-4"><a href="#cb24-4"></a><span class="do">##     y.lag.4, nmulti = 5, degree.max = 5)</span></span>
<span id="cb24-5"><a href="#cb24-5"></a><span class="do">## </span></span>
<span id="cb24-6"><a href="#cb24-6"></a><span class="do">## Generalized Local Polynomial Kernel Regression</span></span>
<span id="cb24-7"><a href="#cb24-7"></a><span class="do">## </span></span>
<span id="cb24-8"><a href="#cb24-8"></a><span class="do">## Polynomial type: Bernstein</span></span>
<span id="cb24-9"><a href="#cb24-9"></a><span class="do">## Using (local) Seifert &amp; Gasser shrinkage for cross-validation</span></span>
<span id="cb24-10"><a href="#cb24-10"></a><span class="do">## There are 4 continuous predictors</span></span>
<span id="cb24-11"><a href="#cb24-11"></a><span class="do">## Bandwidth type: fixed</span></span>
<span id="cb24-12"><a href="#cb24-12"></a><span class="do">## Continuous kernel type: gaussian</span></span>
<span id="cb24-13"><a href="#cb24-13"></a><span class="do">## Continuous kernel order: 2</span></span>
<span id="cb24-14"><a href="#cb24-14"></a><span class="do">## Bandwidth for y.lag.1: 2236767 (scale factor = 1926099)</span></span>
<span id="cb24-15"><a href="#cb24-15"></a><span class="do">## Bandwidth for y.lag.2: 1507493 (scale factor = 1313654)</span></span>
<span id="cb24-16"><a href="#cb24-16"></a><span class="do">## Bandwidth for y.lag.3: 1510034 (scale factor = 1332174)</span></span>
<span id="cb24-17"><a href="#cb24-17"></a><span class="do">## Bandwidth for y.lag.4: 320288375 (scale factor = 284785113)</span></span>
<span id="cb24-18"><a href="#cb24-18"></a><span class="do">## Degree for y.lag.1: 1</span></span>
<span id="cb24-19"><a href="#cb24-19"></a><span class="do">## Degree for y.lag.2: 0</span></span>
<span id="cb24-20"><a href="#cb24-20"></a><span class="do">## Degree for y.lag.3: 0</span></span>
<span id="cb24-21"><a href="#cb24-21"></a><span class="do">## Degree for y.lag.4: 0</span></span>
<span id="cb24-22"><a href="#cb24-22"></a><span class="do">## Training observations: 96</span></span>
<span id="cb24-23"><a href="#cb24-23"></a><span class="do">## Multiple R-squared: 0.812</span></span>
<span id="cb24-24"><a href="#cb24-24"></a><span class="do">## Cross-validation score: 0.84938168</span></span>
<span id="cb24-25"><a href="#cb24-25"></a><span class="do">## Number of multistarts: 5</span></span>
<span id="cb24-26"><a href="#cb24-26"></a><span class="do">## Estimation time: 24.6 seconds</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="nonparametric-ar-models-10" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/npglreuhaitehsdfs_e339fad17ee385fb4145f882ce355b52">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="fu">summary</span>(model.lm)</span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="do">## </span></span>
<span id="cb25-3"><a href="#cb25-3"></a><span class="do">## Call:</span></span>
<span id="cb25-4"><a href="#cb25-4"></a><span class="do">## lm(formula = y ~ y.lag.1, subset = 5:n)</span></span>
<span id="cb25-5"><a href="#cb25-5"></a><span class="do">## </span></span>
<span id="cb25-6"><a href="#cb25-6"></a><span class="do">## Residuals:</span></span>
<span id="cb25-7"><a href="#cb25-7"></a><span class="do">##      Min       1Q   Median       3Q      Max </span></span>
<span id="cb25-8"><a href="#cb25-8"></a><span class="do">## -1.93603 -0.64311 -0.03501  0.67026  2.78012 </span></span>
<span id="cb25-9"><a href="#cb25-9"></a><span class="do">## </span></span>
<span id="cb25-10"><a href="#cb25-10"></a><span class="do">## Coefficients:</span></span>
<span id="cb25-11"><a href="#cb25-11"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb25-12"><a href="#cb25-12"></a><span class="do">## (Intercept) -0.05642    0.09469  -0.596    0.553    </span></span>
<span id="cb25-13"><a href="#cb25-13"></a><span class="do">## y.lag.1      0.90885    0.04511  20.146   &lt;2e-16 ***</span></span>
<span id="cb25-14"><a href="#cb25-14"></a><span class="do">## ---</span></span>
<span id="cb25-15"><a href="#cb25-15"></a><span class="do">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span id="cb25-16"><a href="#cb25-16"></a><span class="do">## </span></span>
<span id="cb25-17"><a href="#cb25-17"></a><span class="do">## Residual standard error: 0.9122 on 94 degrees of freedom</span></span>
<span id="cb25-18"><a href="#cb25-18"></a><span class="do">## Multiple R-squared:  0.812,  Adjusted R-squared:   0.81 </span></span>
<span id="cb25-19"><a href="#cb25-19"></a><span class="do">## F-statistic: 405.9 on 1 and 94 DF,  p-value: &lt; 2.2e-16</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="nonparametric-ar-models-11" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/npglreuhaitehsdfsps_d68aa17907f6061a4035f7bedb521817">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="do">## Examine the first few fitted values from the nonparametric and</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="do">## linear AR(1) models</span></span>
<span id="cb26-3"><a href="#cb26-3"></a><span class="fu">cbind</span>(<span class="fu">fitted</span>(model.glp),<span class="fu">fitted</span>(model.lm))[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,]</span>
<span id="cb26-4"><a href="#cb26-4"></a><span class="do">##          [,1]       [,2]</span></span>
<span id="cb26-5"><a href="#cb26-5"></a><span class="do">## 5  -1.0355583 -1.0355583</span></span>
<span id="cb26-6"><a href="#cb26-6"></a><span class="do">## 6  -0.4088536 -0.4088536</span></span>
<span id="cb26-7"><a href="#cb26-7"></a><span class="do">## 7   0.8985422  0.8985422</span></span>
<span id="cb26-8"><a href="#cb26-8"></a><span class="do">## 8   0.1420452  0.1420452</span></span>
<span id="cb26-9"><a href="#cb26-9"></a><span class="do">## 9   1.3060160  1.3060160</span></span>
<span id="cb26-10"><a href="#cb26-10"></a><span class="do">## 10  1.4750081  1.4750081</span></span>
<span id="cb26-11"><a href="#cb26-11"></a><span class="do">## 11  2.2657127  2.2657127</span></span>
<span id="cb26-12"><a href="#cb26-12"></a><span class="do">## 12  2.8703045  2.8703045</span></span>
<span id="cb26-13"><a href="#cb26-13"></a><span class="do">## 13  3.2328028  3.2328028</span></span>
<span id="cb26-14"><a href="#cb26-14"></a><span class="do">## 14  1.9558400  1.9558400</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="nonparametric-ar-models-12" class="slide level2 center">
<h2>Nonparametric AR Models</h2>
<ul>
<li class="fragment"><p>This method correctly selects the degree for the first lag (the AR(1) process is a linear process, so the degree 1 with a large bandwidth is appropriate)</p></li>
<li class="fragment"><p>It correctly selects a large bandwidth and a constant function (degree 0) for the remaining lags resulting in a globally linear model that is a function of the first lag of <span class="math inline">\(Y\)</span> only</p></li>
<li class="fragment"><p>Since the nonparametric model has effectively shrunk a four-dimensional object to what one would obtain using linear regression <em>and</em> knowing which lag to use, it has delivered a <span class="math inline">\(\sqrt{n}\)</span>-consistent estimator</p></li>
<li class="fragment"><p>The estimator, though fully nonparametric, is identical to what one would obtain if one knew a priori the true functional form and which predictors were relevant</p></li>
<li class="fragment"><p>In this example we obtain the <em>oracle</em> estimator (the oracle knows which predictors are relevant and the nature of the underlying DGP)</p></li>
</ul>
</section></section>
<section>
<section id="nonparametric-conditional-volatility-modelling" class="title-slide slide level1 center">
<h1>Nonparametric Conditional Volatility Modelling</h1>

</section>
<section id="conditional-variance-function-estimation" class="slide level2 center">
<h2>Conditional Variance Function Estimation</h2>
<ul>
<li class="fragment"><p>A conditional moment that is particularly popular in time series settings is the <em>second conditional moment about the conditional mean</em> or the <em>conditional variance</em> function, often denoted by <span class="math inline">\(\sigma^2(x)\)</span></p></li>
<li class="fragment"><p>By way of illustration, let <span class="math inline">\((Y_i,X_i)\)</span> be a strictly stationary process having the same marginal distribution as <span class="math inline">\((Y,X)\)</span>, and let <span class="math inline">\(g(x)=\operatorname{E}(Y|X=x)\)</span> and <span class="math inline">\(\sigma^2(x) = \operatorname{Var}(Y|X=x)\)</span>, <span class="math inline">\(\sigma^2(x) &gt; 0\)</span>, <span class="math inline">\(\varepsilon\sim(0,1)\)</span>, with <span class="math display">\[\begin{equation*}
Y_i = g(X_i) +\sigma(X_i)\varepsilon_i,\quad i=1,\dots,n
\end{equation*}\]</span></p></li>
<li class="fragment"><p>For <span class="math inline">\(Y_i=Y_t\)</span> and <span class="math inline">\(X_i=Y_{t-1}\)</span> this is a nonlinear autoregressive conditional heteroscedastic (ARCH) time series model, and <span class="math inline">\(\sigma^2(x)\)</span> is the so-called <em>volatility function</em> <span class="citation" data-cites="ENGLE:1982">(<a href="#/references-scrollable" role="doc-biblioref" onclick="">Engle 1982</a>)</span></p></li>
</ul>
</section>
<section id="local-linear-conditional-variance-estimation" class="slide level2 center">
<h2>Local Linear Conditional Variance Estimation</h2>
<ul>
<li class="fragment"><p><span class="citation" data-cites="FAN_YAO:1998">Fan and Yao (<a href="#/references-scrollable" role="doc-biblioref" onclick="">1998</a>)</span> consider a two-stage approach towards estimation of <span class="math inline">\(\sigma^2(x)\)</span></p></li>
<li class="fragment"><p>Their first stage uses local linear regression of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(X_i\)</span> which delivers the local linear (squared) residuals <span class="math inline">\(\hat r_i=(Y_i-\hat g(X_i))^2\)</span></p></li>
<li class="fragment"><p>Bandwidth selection proceeds without modification</p></li>
<li class="fragment"><p>Their second stage involves local linear regression of the squared residuals on <span class="math inline">\(X_i\)</span> (they consider only <span class="math inline">\(q=1\)</span> continuous predictors)</p></li>
<li class="fragment"><p>Even though <span class="math inline">\(g(x)\)</span> is unknown and must be estimated, the bias for <span class="math inline">\(\hat g(x)\)</span> with <span class="math inline">\(q\)</span> continuous predictors is of order <span class="math inline">\(O(\sum_{s=1}^q \hat h_s^2)\)</span>, but its contribution to <span class="math inline">\(\hat\sigma^2(x)\)</span> is only of order <span class="math inline">\(o(\sum_{s=1}^q \hat h_s^2)\)</span></p></li>
</ul>
</section>
<section id="conditional-variance-function-estimation-1" class="slide level2 center">
<h2>Conditional Variance Function Estimation</h2>
<ul>
<li class="fragment"><p>They demonstrate that <span class="math display">\[\begin{equation*}
\small
\sqrt{n\prod\nolimits_{j=1}^qh_j}\left( \hat\sigma^2(x) - \sigma^2(x) - \frac{\kappa_2}{2}\sum_{s=1}^q\frac{\partial^2\,\hat\sigma^2(x)}{\partial x^2}h_s^2\right)\to N(0, \Omega_x)
\end{equation*}\]</span></p></li>
<li class="fragment"><p>Here, <span class="math inline">\(\Omega_x = \kappa^q \sigma^4(x)\gamma^2(x)/f(x)\)</span>, where <span class="math inline">\(\gamma^2(x)=\operatorname{E}\left((\epsilon^2-1)^2|X=x\right)\)</span> and <span class="math inline">\(\epsilon=(Y-g(X))/\sigma(X)\)</span></p></li>
<li class="fragment"><p>The bias and variance expressions given in Theorem 1 of <span class="citation" data-cites="FAN_YAO:1998">Fan and Yao (<a href="#/references-scrollable" role="doc-biblioref" onclick="">1998</a>)</span> are exactly those which arise in the usual nonparametric regression analysis, considering the regression function to be <span class="math inline">\(\sigma^2(x)\)</span></p></li>
<li class="fragment"><p>In the bias of <span class="math inline">\(\hat\sigma^2(x)\)</span>, the contribution from the error caused by estimating <span class="math inline">\(g(x)\)</span> is of smaller order than <span class="math inline">\(h^2\)</span>, namely the order of the bias of <span class="math inline">\(\hat g(x)\)</span> itself</p></li>
</ul>
</section>
<section id="conditional-variance-function-estimation-2" class="slide level2 center">
<h2>Conditional Variance Function Estimation</h2>
<ul>
<li class="fragment"><p>We use the optimal bandwidth in <span class="math inline">\(\hat g(x)\)</span> (no undersmoothing is needed)</p></li>
<li class="fragment"><p>For the multivariate mixed-data case this then becomes <span class="math display">\[\begin{equation*}
\small
\sqrt{n\prod\nolimits_{j=1}^qh_j}\left( \hat\sigma^2(x) - \sigma^2(x)-\frac{\kappa_2}{2}\sum_{s=1}^q\frac{\partial^2\,\hat\sigma^2(x)}{\partial x^2}h_s^2-\sum_{s=1}^r \hat\lambda_s D_{s}(x)\right) \to N(0, \Omega_x)
\end{equation*}\]</span></p></li>
<li class="fragment"><p>Here, <span class="math inline">\(D_{s}(x) = \sum_{ v^d } ( {\bf 1}_s(v^d,x^d) \sigma^2(x^c,v^d) -\sigma^2(x))f(x^c,v^d)\)</span></p></li>
<li class="fragment"><p>The approach of <span class="citation" data-cites="FAN_YAO:1998">Fan and Yao (<a href="#/references-scrollable" role="doc-biblioref" onclick="">1998</a>)</span> is based on the local linear estimator, hence <span class="math inline">\(\hat\sigma^2(x)\)</span> is not guaranteed to be positive (it would if they used the local constant estimator since those weights are non-negative while the local linear weights can assume negative values)</p></li>
</ul>
</section>
<section id="conditional-variance-function-estimation-3" class="slide level2 center">
<h2>Conditional Variance Function Estimation</h2>
<ul>
<li class="fragment"><p>To overcome this limitation, <span class="citation" data-cites="CHEN_CHENG_PENG:2009">Chen, Cheng, and Peng (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2009</a>)</span> propose a local linear variant that a) is guaranteed to be positive and b) may perform better in the presence of heavy tailed distributions</p></li>
<li class="fragment"><p>Rewrite <span class="math inline">\(Y_i=g(X_i)+\sigma(X_i)\varepsilon_i\)</span> as <span class="math inline">\(\log r_i=\operatorname{v}(X_i)+\log(\varepsilon_i^2/d)\)</span>, <span class="math inline">\(\varepsilon\sim (0,1)\)</span>, where <span class="math inline">\(r_i=(Y_i-g(X_i))^2\)</span>, <span class="math inline">\(\operatorname{v}(X_i)=\log(d\sigma^2(x))\)</span> and <span class="math inline">\(d\)</span> satisfies <span class="math inline">\(\operatorname{E}\left(\log(\varepsilon_i^2/d)\right)=0\)</span></p></li>
<li class="fragment"><p>To avoid taking <span class="math inline">\(\log(0)\)</span> <span class="citation" data-cites="CHEN_CHENG_PENG:2009">Chen, Cheng, and Peng (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2009</a>)</span> nonparametrically regress <span class="math inline">\(\log(\hat r_i+n^{-1})\)</span> on <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>; denoting the resulting estimates <span class="math inline">\(\hat v(X_i)\)</span>, they estimate <span class="math inline">\(d\)</span> by <span class="math inline">\(\hat d=\left(n^{-1}\sum_{i=1}^n\hat r_i\exp(-\hat v(X_i))\right)^{-1}\)</span>, then compute <span class="math inline">\(\tilde\sigma^2(X_i)=\exp\left(\hat v(X_i)\right)/\hat d\)</span>, <span class="math inline">\(i=1,\dots,n\)</span></p></li>
<li class="fragment"><p>Related work includes <span class="citation" data-cites="YU_JONES:2004">Yu and Jones (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2004</a>)</span> for a local-likelihood approach and <span class="citation" data-cites="BROWN_LEVINE:2007">Brown and Levine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2007</a>)</span> and <span class="citation" data-cites="WANG_BROWN_CAI_LEVINE:2008">Wang et al. (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2008</a>)</span> for difference-based approaches</p></li>
</ul>
</section>
<section id="a-simulated-illustration-scrollable" class="slide level2 smaller center">
<h2>A Simulated Illustration (scrollable)</h2>
<div class="cell" data-layout-align="center" data-fig.asp="0.65" data-hash="index_cache/revealjs/fanyaosimcode_d68814fd0e0c123d9e74c17548fe880f">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="do">## Fan &amp; Yao's (1998) estimator, trim negative values of they occur</span></span>
<span id="cb27-2"><a href="#cb27-2"></a><span class="do">## Chen, Cheng &amp; Peng's (2009) estimator is always positive but my</span></span>
<span id="cb27-3"><a href="#cb27-3"></a><span class="do">## Monte Carlo simulations show it is less efficient for this DGP</span></span>
<span id="cb27-4"><a href="#cb27-4"></a><span class="fu">library</span>(np)</span>
<span id="cb27-5"><a href="#cb27-5"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb27-6"><a href="#cb27-6"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb27-7"><a href="#cb27-7"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n))</span>
<span id="cb27-8"><a href="#cb27-8"></a>sigma.var <span class="ot">&lt;-</span> <span class="fl">0.1</span><span class="sc">*</span>(.Machine<span class="sc">$</span>double.eps<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>x)<span class="sc">**</span><span class="dv">2</span></span>
<span id="cb27-9"><a href="#cb27-9"></a>dgp <span class="ot">&lt;-</span> <span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>x)</span>
<span id="cb27-10"><a href="#cb27-10"></a>y <span class="ot">&lt;-</span> dgp <span class="sc">+</span> <span class="fu">rnorm</span>(n,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma.var))</span>
<span id="cb27-11"><a href="#cb27-11"></a>model <span class="ot">&lt;-</span> <span class="fu">npreg</span>(y<span class="sc">~</span>x,<span class="at">regtype=</span><span class="st">"ll"</span>)</span>
<span id="cb27-12"><a href="#cb27-12"></a>r <span class="ot">&lt;-</span> <span class="fu">residuals</span>(model)<span class="sc">**</span><span class="dv">2</span></span>
<span id="cb27-13"><a href="#cb27-13"></a><span class="do">## Fan and Yao's (1998) estimator with trimming if needed</span></span>
<span id="cb27-14"><a href="#cb27-14"></a>var.fy <span class="ot">&lt;-</span> <span class="fu">fitted</span>(<span class="fu">npreg</span>(r<span class="sc">~</span>x,<span class="at">regtype=</span><span class="st">"ll"</span>))</span>
<span id="cb27-15"><a href="#cb27-15"></a>var.fy <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(var.fy<span class="sc">&lt;=</span><span class="dv">0</span>,.Machine<span class="sc">$</span>double.eps,var.fy)</span>
<span id="cb27-16"><a href="#cb27-16"></a><span class="do">## Chen, Cheng and Peng's (2009) estimator</span></span>
<span id="cb27-17"><a href="#cb27-17"></a>log.r <span class="ot">&lt;-</span> <span class="fu">log</span>(r<span class="sc">+</span><span class="dv">1</span><span class="sc">/</span>n) <span class="do">## Avoids log(0)</span></span>
<span id="cb27-18"><a href="#cb27-18"></a>V.hat <span class="ot">&lt;-</span> <span class="fu">fitted</span>(<span class="fu">npreg</span>(log.r<span class="sc">~</span>x,<span class="at">regtype=</span><span class="st">"ll"</span>))</span>
<span id="cb27-19"><a href="#cb27-19"></a>d.hat <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span><span class="fu">mean</span>(r<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span>V.hat))</span>
<span id="cb27-20"><a href="#cb27-20"></a>var.ccp <span class="ot">&lt;-</span> <span class="fu">exp</span>(V.hat)<span class="sc">/</span>d.hat</span>
<span id="cb27-21"><a href="#cb27-21"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb27-22"><a href="#cb27-22"></a><span class="fu">plot</span>(x,y,<span class="at">cex=</span>.<span class="dv">25</span>,<span class="at">col=</span><span class="st">"grey"</span>)</span>
<span id="cb27-23"><a href="#cb27-23"></a><span class="fu">lines</span>(x,dgp,<span class="at">col=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb27-24"><a href="#cb27-24"></a><span class="fu">lines</span>(x,<span class="fu">fitted</span>(model),<span class="at">col=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb27-25"><a href="#cb27-25"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>,<span class="fu">c</span>(<span class="st">"g(x)=sin(2 pi x)"</span>, </span>
<span id="cb27-26"><a href="#cb27-26"></a>                   <span class="st">"Nonparametric Estimate"</span>),</span>
<span id="cb27-27"><a href="#cb27-27"></a>       <span class="at">col=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,</span>
<span id="cb27-28"><a href="#cb27-28"></a>       <span class="at">lty=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,</span>
<span id="cb27-29"><a href="#cb27-29"></a>       <span class="at">lwd=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>),</span>
<span id="cb27-30"><a href="#cb27-30"></a>       <span class="at">bty=</span><span class="st">"n"</span>)</span>
<span id="cb27-31"><a href="#cb27-31"></a>ylim<span class="ot">=</span><span class="fu">c</span>(<span class="fu">min</span>(r),<span class="fu">quantile</span>(r,<span class="fl">0.95</span>))</span>
<span id="cb27-32"><a href="#cb27-32"></a><span class="fu">plot</span>(x,r,<span class="at">ylim=</span>ylim,<span class="at">cex=</span>.<span class="dv">25</span>,<span class="at">col=</span><span class="st">"grey"</span>)</span>
<span id="cb27-33"><a href="#cb27-33"></a><span class="fu">lines</span>(x,sigma.var,<span class="at">col=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb27-34"><a href="#cb27-34"></a><span class="fu">lines</span>(x,var.fy,<span class="at">col=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb27-35"><a href="#cb27-35"></a><span class="fu">lines</span>(x,var.ccp,<span class="at">col=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">4</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb27-36"><a href="#cb27-36"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>,<span class="fu">c</span>(<span class="st">"volatility=(2 pi x)^2/10"</span>,</span>
<span id="cb27-37"><a href="#cb27-37"></a>                   <span class="st">"Fan and Yao (1998)"</span>,</span>
<span id="cb27-38"><a href="#cb27-38"></a>                   <span class="st">"Chen et al. (2009)"</span>),</span>
<span id="cb27-39"><a href="#cb27-39"></a>       <span class="at">col=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb27-40"><a href="#cb27-40"></a>       <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>),</span>
<span id="cb27-41"><a href="#cb27-41"></a>       <span class="at">lwd=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>),</span>
<span id="cb27-42"><a href="#cb27-42"></a>       <span class="at">bty=</span><span class="st">"n"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="a-simulated-illustration" class="slide level2 smaller center">
<h2>A Simulated Illustration</h2>

<img data-src="index_files/figure-revealjs/fig-fanyaosim-1.png" width="960" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;3: Fan and Yao’s (1998) and Chen et al.’s (2009) conditional variance estimators</p></section></section>
<section id="nonparametric-panel-data-modelling" class="title-slide slide level1 center">
<h1>Nonparametric Panel Data Modelling</h1>
<aside class="notes">
<p>Random effects models are often attacked via GLS estimation, fixed effects by modelling the effects additively or fully nonparametrically</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section>
<section id="random-effects-one-way-error-component-model" class="title-slide slide level1 center">
<h1>Random Effects One-way Error Component Model</h1>

</section>
<section id="random-effects-models" class="slide level2 center">
<h2>Random Effects Models</h2>
<ul>
<li class="fragment"><p>Consider a nonparametric one-way error component model, <span class="math display">\[\begin{equation}\label{eqref:error_component_model}
Y_{it}=g(X_{it}) +\varepsilon_{it},
\end{equation}\]</span> where <span class="math inline">\(i=1,\ldots ,n\)</span>, <span class="math inline">\(t=1,\ldots ,T\)</span>, <span class="math inline">\(Y_{it}\)</span> is the endogenous variable, <span class="math inline">\(X_{it}=\left( X_{it1},\ldots ,X_{itq}\right)^{\mathrm{T}}\)</span> is a vector of <span class="math inline">\(q\)</span> exogenous variables, and <span class="math inline">\(g(\cdot)\)</span> is an unknown smooth function</p></li>
<li class="fragment"><p>Let <span class="math inline">\(\varepsilon_{it}\)</span> have a random effects specification, i.e., <span class="math display">\[\begin{equation*}
\varepsilon_{it}=u_{i}+v_{it},
\end{equation*}\]</span> where <span class="math inline">\(u_{i}\)</span> is i.i.d.&nbsp;<span class="math inline">\(\left( 0,\sigma_{u}^{2}\right)\)</span>, <span class="math inline">\(v_{it}\)</span> is i.i.d. <span class="math inline">\(\left( 0,\sigma_{v}^{2}\right)\)</span>, and <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(v_{it}\)</span> are uncorrelated for all <span class="math inline">\(i\)</span> and <span class="math inline">\(t\)</span></p></li>
<li class="fragment"><p>Let <span class="math inline">\(\varepsilon_{i}=\left( \varepsilon_{i1},\ldots,\varepsilon_{iT}\right) ^{\mathrm{T}}\)</span> be a <span class="math inline">\(T\times 1\)</span> vector</p></li>
</ul>
</section>
<section id="random-effects-models-1" class="slide level2 center">
<h2>Random Effects Models</h2>
<ul>
<li class="fragment"><p>Then <span class="math inline">\(\Sigma\equiv E\left(\varepsilon_{i}\varepsilon_{i}^{\mathrm{T}}\right)\)</span> takes the form <span class="math display">\[\begin{equation*}
\Sigma=\sigma_{v}^{2}I_{T}+\sigma_{u}^{2}1_{T}1_{T}^{\mathrm{T
}},
\end{equation*}\]</span> where <span class="math inline">\(I_{T}\)</span> is an identity matrix of dimension <span class="math inline">\(T\)</span> and <span class="math inline">\(1_{T}\)</span> is a <span class="math inline">\(T\times 1\)</span> column vector of ones</p></li>
<li class="fragment"><p>The covariance matrix for <span class="math inline">\(\varepsilon =\left(\varepsilon_{1}^{\mathrm{T}},\ldots ,\varepsilon_{n}^{\mathrm{T}}\right) ^{\mathrm{T}}\)</span> is <span class="math display">\[\begin{align*}
\Omega&amp;=E\left(\varepsilon \varepsilon^{\mathrm{T}}\right)=I_{N}\otimes \Sigma,\text{ and }
\Omega^{-1}=I_{N}\otimes \Sigma^{-1}
\end{align*}\]</span></p></li>
<li class="fragment"><p>By simple linear algebra, <span class="math inline">\(\Sigma^{-1}=\left( \Sigma_{tt^{\prime }}\right)_{t,t^{\prime }=1}^{T}=\Sigma_{1}I_{T}+\Sigma_{2}1_{T}1_{T}^{\mathrm{T}}\)</span> with <span class="math inline">\(\Sigma_{1}=\sigma_{v}^{-2}\)</span> and <span class="math inline">\(\Sigma_{2}=-\left( \sigma_{v}^{2}+\sigma_{u}^{2}T\right) ^{-1}\sigma_{u}^{2}\sigma_{v}^{-2}\)</span></p></li>
<li class="fragment"><p>Let <span class="math inline">\(A=\Omega^{-1/2}\)</span> be the square root of <span class="math inline">\(\Omega^{-1}\)</span> such that <span class="math inline">\(AA^{\mathrm{T}}=\Omega^{-1}\)</span> (use e.g.&nbsp;Cholesky decomposition, <span class="math inline">\(A\)</span> is a lower triangular matrix)</p></li>
</ul>
</section>
<section id="random-effects-models-2" class="slide level2 center">
<h2>Random Effects Models</h2>
<ul>
<li class="fragment"><p>Transform <span class="math inline">\(Y_{it}\)</span> so that the errors are i.i.d., i.e., <span class="math display">\[\begin{align*}
Y_{it}^*&amp;=\tau\Omega^{-1/2}Y_{it}+(I-\tau\Omega^{-1/2})g(X_{it})\cr
&amp;=g(X_{it})+\tau\Omega^{-1/2}\varepsilon_{it}
\end{align*}\]</span></p></li>
<li class="fragment"><p>This is obtained by adding and subtracting <span class="math inline">\(\tau\Omega^{-1/2}Y_{it}\)</span> from the right hand side of <span class="math inline">\(\eqref{eqref:error_component_model}\)</span>, moving <span class="math inline">\(-\tau\Omega^{-1/2}Y_{it}+\varepsilon_{it}\)</span> to the left hand side, then replacing <span class="math inline">\(\varepsilon_{it}\)</span> by <span class="math inline">\(Y_{it}-g(X_{it})\)</span> and simplifying</p></li>
<li class="fragment"><p>The two-step estimator <span class="citation" data-cites="SU_ULLAH:2007">(<a href="#/references-scrollable" role="doc-biblioref" onclick="">Su and Ullah 2007</a>)</span> involves a first-step pooled nonparametric estimator of Equation <span class="math inline">\(\eqref{eqref:error_component_model}\)</span>, then the first-step pooled residuals are used to estimate <span class="math inline">\(\Omega^{-1/2}\)</span>, <span class="math inline">\(g(X_{it})\)</span>, and <span class="math inline">\(\tau\)</span>, finally <span class="math inline">\(\hat Y_{it}^*\)</span> is nonparametrically regressed on <span class="math inline">\(X_{it}\)</span>, as follows</p></li>
<li class="fragment"><p>Let <span class="math inline">\(Y_{i}=(Y_{i1},\dots,Y_{iT})\)</span>, <span class="math inline">\(\bar\varepsilon_{i}=T^{-1}\sum_{t=1}^T \hat\varepsilon_{it}\)</span>, and <span class="math inline">\(\hat\varepsilon_{it}=Y_{it}-\hat g(X_{it})\)</span> where <span class="math inline">\(\hat g(X_{it})\)</span> is the first-step pooled kernel estimator of <span class="math inline">\(g(X_{it})\)</span></p></li>
</ul>
</section>
<section id="random-effects-models-3" class="slide level2 center">
<h2>Random Effects Models</h2>
<ul>
<li class="fragment"><p>Let <span class="math display">\[\begin{align*}
\hat\sigma^2_1&amp;=(T/n)\sum_{i=1}^n \bar\varepsilon_{i},\quad
\sigma^2_v=(n(T-1))^{-1}\sum_{i=1}^n\sum_{t=1}^T(\hat\varepsilon_{it}-\bar\varepsilon_{i})^2,\quad\text{and} \cr
\hat\sigma^2_u&amp;=T^{-1}(\hat\sigma^2_1-\hat\sigma^2_v)
\end{align*}\]</span></p></li>
<li class="fragment"><p>The estimator of the <span class="math inline">\(T\times T\)</span> matrix <span class="math inline">\(\Sigma\)</span> is obtained via <span class="math display">\[\begin{equation*}
\small
\widehat{\Sigma}=
\begin{pmatrix}
\hat\sigma^2_u+\hat\sigma^2_v&amp;\hat\sigma^2_u&amp;\hat\sigma^2_u&amp;\dots
&amp;\hat\sigma^2_u\cr
\hat\sigma^2_u&amp;\hat\sigma^2_u+\hat\sigma^2_v&amp;\hat\sigma^2_u&amp;\dots
&amp;\hat\sigma^2_u\cr
\vdots&amp;\hat\sigma^2_u&amp;\hat\sigma^2_u&amp;\vdots&amp; \hat\sigma^2_u\cr
\hat\sigma^2_u&amp;\hat\sigma^2_u&amp;\hat\sigma^2_u&amp;\dots&amp;
\hat\sigma^2_u+\hat\sigma^2_v
\end{pmatrix}
\end{equation*}\]</span></p></li>
</ul>
</section>
<section id="random-effects-models-4" class="slide level2 center">
<h2>Random Effects Models</h2>
<ul>
<li class="fragment"><p><span class="math inline">\(\widehat{\Sigma}\)</span> can be written as <span class="math display">\[\begin{equation*}
\widehat{\Sigma}=
(\hat\sigma^2_u+\hat\sigma^2_v)
\begin{pmatrix}
1&amp;c&amp;c&amp;\dots &amp;c\cr c&amp;1&amp;c&amp;\dots &amp;c\cr \vdots&amp;c&amp;c&amp;\vdots&amp; c\cr
c&amp;c&amp;c&amp;\dots&amp; 1
\end{pmatrix},
\end{equation*}\]</span> where <span class="math inline">\(c=\hat\sigma^2_u/(\hat\sigma^2_u+\hat\sigma^2_v)\)</span>, <span class="math inline">\(0\le c&lt;1\)</span></p></li>
<li class="fragment"><p>Racine &amp; Ullah (2015) suggest choosing <span class="math inline">\(\tau\)</span> via <span class="math inline">\(\hat\tau=(1.5-c)\sqrt{\hat\sigma^2_u+\sigma^2_v}\)</span>; see also <span class="citation" data-cites="MARTINS_FILHO_YAO:2009">Martins-Filho and Yao (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2009</a>)</span> who suggest <span class="math inline">\(\tau=\sigma_v/[1-\{1-\sqrt{1-d_T}\}/T]\)</span> where <span class="math inline">\(d_T=T\sigma^2_u/(\sigma^2_v+T\sigma^2_u)\)</span>, and <span class="citation" data-cites="RUCKSTUHL_WELSH_CARROLL:2000">Ruckstuhl, Welsh, and Carroll (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2000</a>)</span> who suggest <span class="math inline">\(\tau=\sigma_\varepsilon=\sqrt{\sigma^2_u+\sigma^2_v}\)</span></p></li>
</ul>
</section>
<section id="random-effects-models-5" class="slide level2 center">
<h2>Random Effects Models</h2>
<ul>
<li class="fragment"><p>The feasible transformed dependent variable is therefore <span class="math display">\[\begin{equation*}
\hat Y_{it}^*=\hat g(X_{it})+(1.5-c)\sqrt{\hat\sigma^2_u+\sigma^2_v}\widehat\Omega^{-1/2}\hat\varepsilon_{it}
\end{equation*}\]</span></p></li>
<li class="fragment"><p>The second-step random effects estimator is obtained by the nonparametric regression of <span class="math inline">\(\hat Y_{it}^*\)</span> on <span class="math inline">\(X_{it}\)</span>, where <span class="math inline">\(\hat Y_{it}^*\)</span> is obtained by replacing <span class="math inline">\(\widehat{\Sigma}\)</span>, <span class="math inline">\(g(X_{it})\)</span>, and <span class="math inline">\(\tau\)</span> with their first step estimates outlined above</p></li>
<li class="fragment"><p>For asymptotic results the bias in the first-step must be lower than in the second-step which is readily achieved by using a higher order kernel in the first-step than the second; see <span class="citation" data-cites="MARTINS_FILHO_YAO:2009">Martins-Filho and Yao (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2009</a>)</span></p></li>
<li class="fragment"><p>Cross-validation can be used to obtain the optimal bandwidths in each step</p></li>
</ul>
</section>
<section id="random-effects-models-6" class="slide level2 smaller center">
<h2>Random Effects Models</h2>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/reecm_07613aa8126f70f8619a7d9b86d3f63a">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="do">## Below is pseudo-code for generic data y, x (univariate) and </span></span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="do">## panel id (univariate) - we shall modify and use this for real</span></span>
<span id="cb28-3"><a href="#cb28-3"></a><span class="do">## data but it demonstrates all the nuances of the approach</span></span>
<span id="cb28-4"><a href="#cb28-4"></a><span class="fu">library</span>(np)</span>
<span id="cb28-5"><a href="#cb28-5"></a><span class="do">## Compute the pooled first-step estimator</span></span>
<span id="cb28-6"><a href="#cb28-6"></a>model.pooled <span class="ot">&lt;-</span> <span class="fu">npreg</span>(y<span class="sc">~</span>x,<span class="at">ckerorder=</span><span class="dv">4</span>)</span>
<span id="cb28-7"><a href="#cb28-7"></a><span class="do">## Compute Sigma.inv</span></span>
<span id="cb28-8"><a href="#cb28-8"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">residuals</span>(model.pooled)</span>
<span id="cb28-9"><a href="#cb28-9"></a>epsiloni.bar <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb28-10"><a href="#cb28-10"></a>i <span class="ot">&lt;-</span> sigmasq.v <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb28-11"><a href="#cb28-11"></a><span class="cf">for</span>(id <span class="cf">in</span> <span class="fu">unique</span>(id)) {</span>
<span id="cb28-12"><a href="#cb28-12"></a>  i <span class="ot">&lt;-</span> i<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb28-13"><a href="#cb28-13"></a>  epsiloni.bar[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(epsilon[id<span class="sc">==</span>id])</span>
<span id="cb28-14"><a href="#cb28-14"></a>  sigmasq.v <span class="ot">&lt;-</span> sigmasq.v <span class="sc">+</span> <span class="fu">sum</span>((epsilon[id<span class="sc">==</span>id]<span class="sc">-</span>epsiloni.bar[i])<span class="sc">**</span><span class="dv">2</span>)</span>
<span id="cb28-15"><a href="#cb28-15"></a>}</span>
<span id="cb28-16"><a href="#cb28-16"></a>sigmasq.one <span class="ot">&lt;-</span> (T<span class="sc">/</span>n)<span class="sc">*</span><span class="fu">sum</span>(epsiloni.bar<span class="sc">**</span><span class="dv">2</span>)</span>
<span id="cb28-17"><a href="#cb28-17"></a>sigmasq.v <span class="ot">&lt;-</span> sigmasq.v<span class="sc">/</span>(n<span class="sc">*</span>(T<span class="dv">-1</span>))</span>
<span id="cb28-18"><a href="#cb28-18"></a>sigmasq.u <span class="ot">&lt;-</span> (sigmasq.one<span class="sc">-</span>sigmasq.v)<span class="sc">/</span>T</span>
<span id="cb28-19"><a href="#cb28-19"></a>Sigma.inv <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>sigmasq.v)<span class="sc">*</span><span class="fu">diag</span>(T) <span class="sc">-</span></span>
<span id="cb28-20"><a href="#cb28-20"></a>  (sigmasq.u<span class="sc">/</span>sigmasq.v)<span class="sc">/</span>(sigmasq.v<span class="sc">+</span>sigmasq.u<span class="sc">*</span>T)<span class="sc">*</span><span class="fu">matrix</span>(<span class="dv">1</span>,T,T)</span>
<span id="cb28-21"><a href="#cb28-21"></a><span class="do">## Compute Omega.inv.sqrt via Cholesky decomposition</span></span>
<span id="cb28-22"><a href="#cb28-22"></a>Omega.inv.sqrt <span class="ot">&lt;-</span> <span class="fu">chol</span>(<span class="fu">kronecker</span>(<span class="fu">diag</span>(n),Sigma.inv))</span>
<span id="cb28-23"><a href="#cb28-23"></a><span class="do">## Compute c (for computing tau)</span></span>
<span id="cb28-24"><a href="#cb28-24"></a>c <span class="ot">&lt;-</span> sigmasq.u<span class="sc">/</span>(sigmasq.u<span class="sc">+</span>sigmasq.v)</span>
<span id="cb28-25"><a href="#cb28-25"></a><span class="do">## Compute y-star</span></span>
<span id="cb28-26"><a href="#cb28-26"></a>y.star <span class="ot">&lt;-</span> <span class="fu">fitted</span>(model.pooled) <span class="sc">+</span> </span>
<span id="cb28-27"><a href="#cb28-27"></a>  (<span class="fl">1.5</span><span class="sc">-</span>c)<span class="sc">*</span><span class="fu">sqrt</span>(sigmasq.u<span class="sc">+</span>sigmasq.v)<span class="sc">*</span>Omega.inv.sqrt<span class="sc">%*%</span><span class="fu">residuals</span>(model.pooled)</span>
<span id="cb28-28"><a href="#cb28-28"></a><span class="do">## Compute the second-step estimator</span></span>
<span id="cb28-29"><a href="#cb28-29"></a>model.re <span class="ot">&lt;-</span> <span class="fu">npreg</span>(y.star<span class="sc">~</span>x,<span class="at">ckerorder=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section>
<section>
<section id="fixed-effects-models" class="title-slide slide level1 center">
<h1>Fixed Effects Models</h1>

</section>
<section id="fixed-effects-one-way-error-component-model" class="slide level2 center">
<h2>Fixed Effects One-way Error Component Model</h2>
<ul>
<li class="fragment"><p>Nonparametric fixed effects models can be obtained by treating the cross-section identifier as an unordered factor</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/feecm_9ca8f415bc938481c83e805b7e1f48a6">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a><span class="do">## Below is pseudo-code for generic data y, x (univariate) and </span></span>
<span id="cb29-2"><a href="#cb29-2"></a><span class="do">## panel id (univariate) - we shall modify and use this for real</span></span>
<span id="cb29-3"><a href="#cb29-3"></a><span class="do">## data but it demonstrates all the nuances of the approach</span></span>
<span id="cb29-4"><a href="#cb29-4"></a><span class="fu">library</span>(np)</span>
<span id="cb29-5"><a href="#cb29-5"></a>model.fe <span class="ot">&lt;-</span> <span class="fu">npreg</span>(y<span class="sc">~</span>x<span class="sc">+</span><span class="fu">factor</span>(id))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
<li class="fragment"><p>One benefit of this approach is the ability of the model to automatically pool or partially pool the model, if appropriate</p></li>
<li class="fragment"><p>Nonparametric models with additive fixed effects (i.e., semiparametric) models have been considered by <span class="citation" data-cites="HENDERSON_CARROLL_LI:2008">Henderson, Carroll, and Li (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2008</a>)</span> and <span class="citation" data-cites="LEE_ROBINSON:2015">Lee and Robinson (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2015</a>)</span></p></li>
</ul>
</section>
<section id="airline-panel-illustration-scrollable" class="slide level2 smaller center">
<h2>Airline Panel Illustration (Scrollable)</h2>
<p>Consider the Airline panel dataset from the Ecdat package, construct nonparametric fixed and random effects models (the example below is scrollable, cut-and-paste and execute)</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/airline_fd4cf80b6d538083f7271099e7d77acd">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a><span class="fu">library</span>(Ecdat)</span>
<span id="cb30-2"><a href="#cb30-2"></a><span class="fu">data</span>(Airline)</span>
<span id="cb30-3"><a href="#cb30-3"></a>?Airline</span>
<span id="cb30-4"><a href="#cb30-4"></a><span class="fu">library</span>(np)</span>
<span id="cb30-5"><a href="#cb30-5"></a><span class="fu">options</span>(<span class="at">np.messages=</span><span class="cn">FALSE</span>)</span>
<span id="cb30-6"><a href="#cb30-6"></a><span class="fu">attach</span>(Airline)</span>
<span id="cb30-7"><a href="#cb30-7"></a></span>
<span id="cb30-8"><a href="#cb30-8"></a><span class="do">## Dependent variable is (log) cost, predictors (log) output, fuel price, load factor, year</span></span>
<span id="cb30-9"><a href="#cb30-9"></a>lcost <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">log</span>(cost))</span>
<span id="cb30-10"><a href="#cb30-10"></a>loutput <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">log</span>(output))</span>
<span id="cb30-11"><a href="#cb30-11"></a>lpf <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">log</span>(pf))</span>
<span id="cb30-12"><a href="#cb30-12"></a>lf <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(lf)</span>
<span id="cb30-13"><a href="#cb30-13"></a>year <span class="ot">&lt;-</span> <span class="fu">ordered</span>(year)</span>
<span id="cb30-14"><a href="#cb30-14"></a>airline <span class="ot">&lt;-</span> <span class="fu">factor</span>(airline)</span>
<span id="cb30-15"><a href="#cb30-15"></a></span>
<span id="cb30-16"><a href="#cb30-16"></a><span class="do">## Airline specific fixed effects</span></span>
<span id="cb30-17"><a href="#cb30-17"></a></span>
<span id="cb30-18"><a href="#cb30-18"></a>model.fe <span class="ot">&lt;-</span> <span class="fu">npreg</span>(lcost<span class="sc">~</span>loutput<span class="sc">+</span>lpf<span class="sc">+</span>lf<span class="sc">+</span>year<span class="sc">+</span>airline,</span>
<span id="cb30-19"><a href="#cb30-19"></a>                  <span class="at">regtype=</span><span class="st">"ll"</span>,</span>
<span id="cb30-20"><a href="#cb30-20"></a>                  <span class="at">bwmethod=</span><span class="st">"cv.aic"</span>,</span>
<span id="cb30-21"><a href="#cb30-21"></a>                  <span class="at">ukertype=</span><span class="st">"liracine"</span>,</span>
<span id="cb30-22"><a href="#cb30-22"></a>                  <span class="at">okertype=</span><span class="st">"liracine"</span>,</span>
<span id="cb30-23"><a href="#cb30-23"></a>                  <span class="at">gradients=</span><span class="cn">TRUE</span>)</span>
<span id="cb30-24"><a href="#cb30-24"></a></span>
<span id="cb30-25"><a href="#cb30-25"></a><span class="fu">summary</span>(model.fe)</span>
<span id="cb30-26"><a href="#cb30-26"></a><span class="fu">summary</span>(model.fe)</span>
<span id="cb30-27"><a href="#cb30-27"></a><span class="do">## Plot partial means</span></span>
<span id="cb30-28"><a href="#cb30-28"></a><span class="fu">plot</span>(model.fe,</span>
<span id="cb30-29"><a href="#cb30-29"></a>     <span class="at">plot.errors.method=</span><span class="st">"bootstrap"</span>,</span>
<span id="cb30-30"><a href="#cb30-30"></a>     <span class="at">plot.errors.boot.num=</span><span class="dv">25</span>,</span>
<span id="cb30-31"><a href="#cb30-31"></a>     <span class="at">common.scale=</span><span class="cn">FALSE</span>)</span>
<span id="cb30-32"><a href="#cb30-32"></a><span class="do">## Plot partial gradients</span></span>
<span id="cb30-33"><a href="#cb30-33"></a><span class="fu">plot</span>(model.fe,</span>
<span id="cb30-34"><a href="#cb30-34"></a>     <span class="at">plot.errors.method=</span><span class="st">"bootstrap"</span>,</span>
<span id="cb30-35"><a href="#cb30-35"></a>     <span class="at">plot.errors.boot.num=</span><span class="dv">25</span>,</span>
<span id="cb30-36"><a href="#cb30-36"></a>     <span class="at">gradients=</span><span class="cn">TRUE</span>,</span>
<span id="cb30-37"><a href="#cb30-37"></a>     <span class="at">common.scale=</span><span class="cn">FALSE</span>)</span>
<span id="cb30-38"><a href="#cb30-38"></a></span>
<span id="cb30-39"><a href="#cb30-39"></a><span class="do">## Random effects</span></span>
<span id="cb30-40"><a href="#cb30-40"></a></span>
<span id="cb30-41"><a href="#cb30-41"></a><span class="do">## Compute the pooled first-step estimator</span></span>
<span id="cb30-42"><a href="#cb30-42"></a></span>
<span id="cb30-43"><a href="#cb30-43"></a>id <span class="ot">&lt;-</span> airline</span>
<span id="cb30-44"><a href="#cb30-44"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">unique</span>(airline))</span>
<span id="cb30-45"><a href="#cb30-45"></a>T <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">unique</span>(year))</span>
<span id="cb30-46"><a href="#cb30-46"></a>model.pooled <span class="ot">&lt;-</span> <span class="fu">npreg</span>(lcost<span class="sc">~</span>loutput<span class="sc">+</span>lpf<span class="sc">+</span>lf<span class="sc">+</span>year,</span>
<span id="cb30-47"><a href="#cb30-47"></a>                      <span class="at">ckerorder=</span><span class="dv">4</span>,</span>
<span id="cb30-48"><a href="#cb30-48"></a>                      <span class="at">regtype=</span><span class="st">"ll"</span>,</span>
<span id="cb30-49"><a href="#cb30-49"></a>                      <span class="at">bwmethod=</span><span class="st">"cv.aic"</span>,</span>
<span id="cb30-50"><a href="#cb30-50"></a>                      <span class="at">ukertype=</span><span class="st">"liracine"</span>,</span>
<span id="cb30-51"><a href="#cb30-51"></a>                      <span class="at">okertype=</span><span class="st">"liracine"</span>)</span>
<span id="cb30-52"><a href="#cb30-52"></a><span class="do">## Compute Sigma.inv</span></span>
<span id="cb30-53"><a href="#cb30-53"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">residuals</span>(model.pooled)</span>
<span id="cb30-54"><a href="#cb30-54"></a>epsiloni.bar <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb30-55"><a href="#cb30-55"></a>i <span class="ot">&lt;-</span> sigmasq.v <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb30-56"><a href="#cb30-56"></a><span class="cf">for</span>(id <span class="cf">in</span> <span class="fu">unique</span>(id)) {</span>
<span id="cb30-57"><a href="#cb30-57"></a>  i <span class="ot">&lt;-</span> i<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb30-58"><a href="#cb30-58"></a>  epsiloni.bar[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(epsilon[id<span class="sc">==</span>id])</span>
<span id="cb30-59"><a href="#cb30-59"></a>  sigmasq.v <span class="ot">&lt;-</span> sigmasq.v <span class="sc">+</span> <span class="fu">sum</span>((epsilon[id<span class="sc">==</span>id]<span class="sc">-</span>epsiloni.bar[i])<span class="sc">**</span><span class="dv">2</span>)</span>
<span id="cb30-60"><a href="#cb30-60"></a>}</span>
<span id="cb30-61"><a href="#cb30-61"></a>sigmasq.one <span class="ot">&lt;-</span> (T<span class="sc">/</span>n)<span class="sc">*</span><span class="fu">sum</span>(epsiloni.bar<span class="sc">**</span><span class="dv">2</span>)</span>
<span id="cb30-62"><a href="#cb30-62"></a>sigmasq.v <span class="ot">&lt;-</span> sigmasq.v<span class="sc">/</span>(n<span class="sc">*</span>(T<span class="dv">-1</span>))</span>
<span id="cb30-63"><a href="#cb30-63"></a>sigmasq.u <span class="ot">&lt;-</span> (sigmasq.one<span class="sc">-</span>sigmasq.v)<span class="sc">/</span>T</span>
<span id="cb30-64"><a href="#cb30-64"></a>Sigma.inv <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>sigmasq.v)<span class="sc">*</span><span class="fu">diag</span>(T) <span class="sc">-</span></span>
<span id="cb30-65"><a href="#cb30-65"></a>  (sigmasq.u<span class="sc">/</span>sigmasq.v)<span class="sc">/</span>(sigmasq.v<span class="sc">+</span>sigmasq.u<span class="sc">*</span>T)<span class="sc">*</span><span class="fu">matrix</span>(<span class="dv">1</span>,T,T)</span>
<span id="cb30-66"><a href="#cb30-66"></a><span class="do">## Compute Omega.inv.sqrt via Cholesky decomposition</span></span>
<span id="cb30-67"><a href="#cb30-67"></a>Omega.inv.sqrt <span class="ot">&lt;-</span> <span class="fu">chol</span>(<span class="fu">kronecker</span>(<span class="fu">diag</span>(n),Sigma.inv))</span>
<span id="cb30-68"><a href="#cb30-68"></a><span class="do">## Compute c (for computing tau)</span></span>
<span id="cb30-69"><a href="#cb30-69"></a>c <span class="ot">&lt;-</span> sigmasq.u<span class="sc">/</span>(sigmasq.u<span class="sc">+</span>sigmasq.v)</span>
<span id="cb30-70"><a href="#cb30-70"></a><span class="do">## Compute y-star</span></span>
<span id="cb30-71"><a href="#cb30-71"></a>y.star <span class="ot">&lt;-</span> <span class="fu">fitted</span>(model.pooled) <span class="sc">+</span> </span>
<span id="cb30-72"><a href="#cb30-72"></a>  (<span class="fl">1.5</span><span class="sc">-</span>c)<span class="sc">*</span><span class="fu">sqrt</span>(sigmasq.u<span class="sc">+</span>sigmasq.v)<span class="sc">*</span>Omega.inv.sqrt<span class="sc">%*%</span><span class="fu">residuals</span>(model.pooled)</span>
<span id="cb30-73"><a href="#cb30-73"></a><span class="do">## Compute the second-step estimator</span></span>
<span id="cb30-74"><a href="#cb30-74"></a>model.re <span class="ot">&lt;-</span> <span class="fu">npreg</span>(y.star<span class="sc">~</span>loutput<span class="sc">+</span>lpf<span class="sc">+</span>lf<span class="sc">+</span>year,</span>
<span id="cb30-75"><a href="#cb30-75"></a>                  <span class="at">ckerorder=</span><span class="dv">2</span>,</span>
<span id="cb30-76"><a href="#cb30-76"></a>                  <span class="at">regtype=</span><span class="st">"ll"</span>,</span>
<span id="cb30-77"><a href="#cb30-77"></a>                  <span class="at">bwmethod=</span><span class="st">"cv.aic"</span>,</span>
<span id="cb30-78"><a href="#cb30-78"></a>                  <span class="at">ukertype=</span><span class="st">"liracine"</span>,</span>
<span id="cb30-79"><a href="#cb30-79"></a>                  <span class="at">okertype=</span><span class="st">"liracine"</span>)</span>
<span id="cb30-80"><a href="#cb30-80"></a></span>
<span id="cb30-81"><a href="#cb30-81"></a><span class="fu">summary</span>(model.re)</span>
<span id="cb30-82"><a href="#cb30-82"></a><span class="do">## Plot partial means</span></span>
<span id="cb30-83"><a href="#cb30-83"></a><span class="fu">plot</span>(model.re,</span>
<span id="cb30-84"><a href="#cb30-84"></a>     <span class="at">plot.errors.method=</span><span class="st">"bootstrap"</span>,</span>
<span id="cb30-85"><a href="#cb30-85"></a>     <span class="at">plot.errors.boot.num=</span><span class="dv">25</span>,</span>
<span id="cb30-86"><a href="#cb30-86"></a>     <span class="at">common.scale=</span><span class="cn">FALSE</span>)</span>
<span id="cb30-87"><a href="#cb30-87"></a><span class="do">## Plot partial gradients</span></span>
<span id="cb30-88"><a href="#cb30-88"></a><span class="fu">plot</span>(model.re,</span>
<span id="cb30-89"><a href="#cb30-89"></a>     <span class="at">plot.errors.method=</span><span class="st">"bootstrap"</span>,</span>
<span id="cb30-90"><a href="#cb30-90"></a>     <span class="at">plot.errors.boot.num=</span><span class="dv">25</span>,</span>
<span id="cb30-91"><a href="#cb30-91"></a>     <span class="at">gradients=</span><span class="cn">TRUE</span>,</span>
<span id="cb30-92"><a href="#cb30-92"></a>     <span class="at">common.scale=</span><span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section>
<section>
<section id="semiparametric-models" class="title-slide slide level1 center">
<h1>Semiparametric Models</h1>

</section>
<section id="overview-of-semiparametric-modelling" class="slide level2 center">
<h2>Overview of Semiparametric Modelling</h2>
<ul>
<li class="fragment"><p>Semiparametric models are a compromise between fully nonparametric and fully parametric specifications</p></li>
<li class="fragment"><p>They are used to reduce the dimension of the nonparametric component in order to mitigate the curse of dimensionality</p></li>
<li class="fragment"><p>They have proven to be quite popular in applied settings and tend to be simpler to interpret than fully nonparametric models</p></li>
<li class="fragment"><p>However, they rely on parametric assumptions and can therefore be misspecified and inconsistent, just like their parametric counterparts</p></li>
<li class="fragment"><p>We shall restrict attention to regression-type models, and we consider three popular methods, namely, the partially linear, single index, and varying coefficient specifications</p></li>
</ul>
</section>
<section id="robinsons-partially-linear-model" class="slide level2 center">
<h2>Robinson’s Partially Linear Model</h2>
<ul>
<li class="fragment"><p>The partially linear model <span class="citation" data-cites="ROBINSON:1988">(<a href="#/references-scrollable" role="doc-biblioref" onclick="">Robinson 1988</a>)</span> is one of the simplest semiparametric models used in practice</p></li>
<li class="fragment"><p>A semiparametric partially linear model is given by <span class="math display">\[\begin{equation*}
  Y_i = X_i'\beta + g(Z_i) + u_i, \quad i=1,\dots ,n
\end{equation*}\]</span></p></li>
<li class="fragment"><p><span class="math inline">\(X_i\)</span> is a <span class="math inline">\(p\times 1\)</span> vector of regressors, <span class="math inline">\(\beta\)</span> is a <span class="math inline">\(p\times 1\)</span> vector of unknown parameters</p></li>
<li class="fragment"><p><span class="math inline">\(Z_i\in \mathbb{R}^q\)</span></p></li>
<li class="fragment"><p>The functional form of <span class="math inline">\(g(\cdot)\)</span> is not specified</p></li>
<li class="fragment"><p>The finite dimensional parameter <span class="math inline">\(\beta\)</span> constitutes the parametric part of the model and the unknown function <span class="math inline">\(g(\cdot)\)</span> the nonparametric part</p></li>
</ul>
</section>
<section id="robinsons-partially-linear-model-1" class="slide level2 center">
<h2>Robinson’s Partially Linear Model</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment"><p>The goal is to consistently estimate <span class="math inline">\(\beta\)</span></p></li>
<li class="fragment"><p>Identification conditions are required in order to identify the parameter vector <span class="math inline">\(\beta\)</span></p></li>
<li class="fragment"><p>In particular, <span class="math inline">\(X\)</span> cannot contain a constant (i.e.&nbsp;<span class="math inline">\(\beta\)</span> cannot contain an intercept) because, if there were an intercept, say <span class="math inline">\(\alpha\)</span>, it could not be identified separately from the unknown function <span class="math inline">\(g(\cdot)\)</span></p></li>
<li class="fragment"><p>To see this note that, for any constant <span class="math inline">\(c\neq 0\)</span>, <span class="math display">\[\begin{equation*}
\alpha + g(z) = [\alpha + c] + [g(z) - c] \equiv \alpha_\text{new} + g_\text{new}(z),
\end{equation*}\]</span> thus the sum of the new intercept and the new <span class="math inline">\(g(\cdot)\)</span> function is observationally equivalent to the sum of the old ones</p></li>
<li class="fragment"><p>Since the functional form of <span class="math inline">\(g(\cdot)\)</span> is not specified, this immediately tells us that an intercept term cannot be identified in a partially linear model</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="robinsons-partially-linear-model-2" class="slide level2 center">
<h2>Robinson’s Partially Linear Model</h2>
<ul>
<li class="fragment"><p>Robinson’s insight is to first eliminate the unknown function <span class="math inline">\(g(\cdot)\)</span></p></li>
<li class="fragment"><p>Taking expectations conditional on <span class="math inline">\(Z_i\)</span>, we get <span class="math display">\[\begin{equation*}
  \operatorname{E}(Y_i|Z_i) = \operatorname{E}(X_i|Z_i)'\beta + g(Z_i)
\end{equation*}\]</span></p></li>
<li class="fragment"><p>Subtracting this from the partially linear model yields <span class="math display">\[\begin{equation*}
  Y_i - \operatorname{E}(Y_i|Z_i) = \left(X_i-\operatorname{E}(X_i|Z_i)\right)'\beta + u_i
\end{equation*}\]</span></p></li>
<li class="fragment"><p>Defining <span class="math inline">\(\tilde Y_i = Y_i - \operatorname{E}(Y_i|Z_i)\)</span>, <span class="math inline">\(\tilde X_i = X_i - \operatorname{E}(X_i|Z_i)\)</span>, and appling least squares, we get <span class="math display">\[\begin{equation*}
  \hat \beta_\text{inf} = \left[
    \sum_{i=1}^n \tilde X_i \tilde X_i' \right]^{-1} \sum_{i=1}^n
  \tilde X_i\tilde Y_i
\end{equation*}\]</span></p></li>
</ul>
</section>
<section id="robinsons-partially-linear-model-3" class="slide level2 center">
<h2>Robinson’s Partially Linear Model</h2>
<ul>
<li class="fragment"><p>Of course, <span class="math inline">\(\hat \beta_\text{inf}\)</span> is infeasible (<span class="math inline">\(\operatorname{E}(Y_i|Z_i)\)</span> and <span class="math inline">\(\operatorname{E}(X_i|Z_i)\)</span> are unknown)</p></li>
<li class="fragment"><p>Robinson replaces these unknown moments with nonparametric estimates</p></li>
<li class="fragment"><p>Under standard regularity conditions, Robinson shows that <span class="math display">\[\begin{equation*}
  \sqrt{n}(\hat \beta - \beta)\stackrel{d}{\rightarrow }
  N\left(0, \Phi^{-1}\Psi \Phi^{-1}\right),
\end{equation*}\]</span> provided that <span class="math inline">\(\Phi\)</span> is positive definite, where <span class="math display">\[\begin{equation*}
  \Phi =  \operatorname{E}[\tilde X_i\tilde X_i'], \Psi = \operatorname{E}[\sigma^2(X_i,Z_i) \tilde
  X_i\tilde X_i']\text{ and }\tilde X_i =
  X_i-\operatorname{E}(X_i|Z_i)
\end{equation*}\]</span></p></li>
<li class="fragment"><p><span class="citation" data-cites="ROBINSON:1988">Robinson (<a href="#/references-scrollable" role="doc-biblioref" onclick="">1988</a>)</span> treated the continuous only <span class="math inline">\(Z\)</span> case, while <span class="citation" data-cites="GAO_LIU_RACINE:2015">Gao, Liu, and Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2015</a>)</span> consider the mixed-data <span class="math inline">\(Z\)</span> case</p></li>
</ul>
</section>
<section id="robinsons-partially-linear-model-4" class="slide level2 center">
<h2>Robinson’s Partially Linear Model</h2>
<ul>
<li class="fragment"><p>Suppose that we again consider Wooldridge’s <code>wage1</code> dataset</p></li>
<li class="fragment"><p>First we estimate a linear model that is quadratic in experience</p></li>
<li class="fragment"><p>Then we assume that the researcher is unwilling to specify the nature of the relationship between <code>exper</code> and <code>lwage</code></p></li>
<li class="fragment"><p>The researcher hence relegates <code>exper</code> to the nonparametric part of a semiparametric partially linear model</p></li>
<li class="fragment"><p>The following presents a summary first from the linear parametric specification and then from the semiparametric partially linear specification</p></li>
</ul>
</section>
<section id="robinsons-partially-linear-model-5" class="slide level2 smaller center">
<h2>Robinson’s Partially Linear Model</h2>
<ul>
<li class="fragment"><p>First, suppose that we model a standard earnings equation using a simple common parametric specification</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1lm_7597383e9df7ffd16df67aa8a31b25ed">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="fu">library</span>(np)</span>
<span id="cb31-2"><a href="#cb31-2"></a><span class="fu">data</span>(wage1)</span>
<span id="cb31-3"><a href="#cb31-3"></a>model.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(lwage<span class="sc">~</span>female<span class="sc">+</span>married<span class="sc">+</span>educ<span class="sc">+</span>tenure<span class="sc">+</span>exper<span class="sc">+</span><span class="fu">I</span>(exper<span class="sc">^</span><span class="dv">2</span>),<span class="at">data=</span>wage1)</span>
<span id="cb31-4"><a href="#cb31-4"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">coef</span>(model.lm),<span class="at">col.names=</span><span class="st">"Coefficient"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">0.1811615</td>
</tr>
<tr class="even">
<td style="text-align: left;">femaleMale</td>
<td style="text-align: right;">0.2911303</td>
</tr>
<tr class="odd">
<td style="text-align: left;">marriedNotmarried</td>
<td style="text-align: right;">-0.0564494</td>
</tr>
<tr class="even">
<td style="text-align: left;">educ</td>
<td style="text-align: right;">0.0798322</td>
</tr>
<tr class="odd">
<td style="text-align: left;">tenure</td>
<td style="text-align: right;">0.0160739</td>
</tr>
<tr class="even">
<td style="text-align: left;">exper</td>
<td style="text-align: right;">0.0300995</td>
</tr>
<tr class="odd">
<td style="text-align: left;">I(exper^2)</td>
<td style="text-align: right;">-0.0006012</td>
</tr>
</tbody>
</table>
</div>
</div></li>
<li class="fragment"><p>The <span class="math inline">\(R^2\)</span> for this model is 0.436</p></li>
</ul>
</section>
<section id="robinsons-partially-linear-model-6" class="slide level2 smaller center">
<h2>Robinson’s Partially Linear Model</h2>
<ul>
<li class="fragment"><p>Next, we consider the partially linear specification</p></li>
<li class="fragment"><p>Note that, for identification purposes, there is no intercept term</p></li>
<li class="fragment"><p>Also, there is no coefficient for <code>exper</code> as it has been relegated to the nonparametric component of the model</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1pl_021b1711e6e9d7ce8429482944008d06">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a><span class="fu">library</span>(np)</span>
<span id="cb32-2"><a href="#cb32-2"></a><span class="fu">data</span>(wage1)</span>
<span id="cb32-3"><a href="#cb32-3"></a>model.pl <span class="ot">&lt;-</span> <span class="fu">npplreg</span>(lwage<span class="sc">~</span>female<span class="sc">+</span>married<span class="sc">+</span>educ<span class="sc">+</span>tenure<span class="sc">|</span>exper,</span>
<span id="cb32-4"><a href="#cb32-4"></a>                    <span class="at">data=</span>wage1)</span>
<span id="cb32-5"><a href="#cb32-5"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">coef</span>(model.pl),<span class="at">digits=</span><span class="dv">7</span>,<span class="at">col.names=</span><span class="st">"Coefficient"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">female</td>
<td style="text-align: right;">0.2861456</td>
</tr>
<tr class="even">
<td style="text-align: left;">married</td>
<td style="text-align: right;">-0.0383323</td>
</tr>
<tr class="odd">
<td style="text-align: left;">educ</td>
<td style="text-align: right;">0.0788131</td>
</tr>
<tr class="even">
<td style="text-align: left;">tenure</td>
<td style="text-align: right;">0.0161654</td>
</tr>
</tbody>
</table>
</div>
</div></li>
<li class="fragment"><p>It is interesting to compare this model with the linear model in terms of in-sample fit (<span class="math inline">\(R^2\)</span> for this model is 0.452 versus <span class="math inline">\(R^2\)</span> = 0.436 for the parametric model)</p></li>
</ul>
</section>
<section id="varying-coefficient-models" class="slide level2 center">
<h2>Varying Coefficient Models</h2>
<ul>
<li class="fragment"><p>Consider a more general semiparametric regression model, the so-called <em>semiparametric smooth coefficient</em> model</p></li>
<li class="fragment"><p>This is one of the most flexible models I am aware of</p></li>
<li class="fragment"><p>It nests linear, partially linear, and locally linear models as special cases</p></li>
<li class="fragment"><p>This model also nests a local linear model as a special case</p></li>
<li class="fragment"><p>The smooth coefficient model is given by <span class="math display">\[\begin{equation*}
  Y_i = \alpha(Z_i) + X_i'\beta(Z_i) + u_i,
\end{equation*}\]</span> where <span class="math inline">\(\beta(z)\)</span> is a vector of unspecified smooth functions of <span class="math inline">\(z\)</span></p></li>
<li class="fragment"><p>When <span class="math inline">\(\beta(z)=\beta_0\)</span>, this collapses to the <em>partially linear</em> model (i.e.&nbsp;<span class="math inline">\(g(z)=\alpha(z)\)</span>)</p></li>
<li class="fragment"><p>When <span class="math inline">\(X=Z\)</span>, this collapses to the <em>local linear</em> model</p></li>
</ul>
</section>
<section id="varying-coefficient-models-1" class="slide level2 center">
<h2>Varying Coefficient Models</h2>
<ul>
<li class="fragment"><p>We abuse notation slightly and express the model more compactly as <span class="math display">\[\begin{equation*}
\small
  Y_i  =  X_i'\beta(Z_i) + u_i
\end{equation*}\]</span></p></li>
<li class="fragment"><p><span class="math inline">\(X_i\)</span> is <span class="math inline">\(p\times 1\)</span> and can incorporate a constant, <span class="math inline">\(\beta(z)\)</span> is a <span class="math inline">\(p\times 1\)</span> dimensional function of <span class="math inline">\(z\)</span>, and <span class="math inline">\(z\)</span> is of dimension <span class="math inline">\(q\)</span></p></li>
<li class="fragment"><p>Premultiplying by <span class="math inline">\(X_i\)</span> and taking expectations conditional on <span class="math inline">\(Z_i\)</span> leads to <span class="math inline">\(\operatorname{E}[X_iY_i|Z_i] =\operatorname{E}[X_iX_i'|Z_i]\beta(Z_i)\)</span>, yielding <span class="math display">\[\begin{equation*}
\small
  \beta(z) = \left[\operatorname{E}(X_iX_i'|z) \right]^{-1}
  \operatorname{E}[X_iY_i|z]
\end{equation*}\]</span></p></li>
<li class="fragment"><p>This suggests the following local constant least squares estimator for <span class="math inline">\(\beta(z)\)</span>: <span class="math display">\[\begin{equation*}
\small
  \hat \beta(z) = \left[ \sum_{j=1}^n X_j X_j'
    K\left(\frac{Z_j-z}{h} \right) \right]^{-1}
  \sum_{j=1}^n X_j Y_j K\left(\frac{Z_j-z}{h} \right)
\end{equation*}\]</span></p></li>
</ul>
</section>
<section id="varying-coefficient-models-2" class="slide level2 center">
<h2>Varying Coefficient Models</h2>
<ul>
<li class="fragment"><p>Under regularity conditions given in <span class="citation" data-cites="LI_HUANG_LI_FU:2002">Li et al. (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2002</a>)</span>, we have <span class="math display">\[\begin{equation*}
  \sqrt{ nh_1\dots h_q } \left(\hat \beta(z) - \beta(z)
    - \sum_{s=1}^q h^2_s B_s(z)\right)
  \stackrel{d}{\rightarrow} N(0, \Omega_z)
\end{equation*}\]</span></p></li>
<li class="fragment"><p>This follows when <span class="math inline">\(M_z\stackrel{\text{def}}{=} f_z(z)\operatorname{E}[X_i X_i'|Z_i=z]\)</span> is positive definite where <span class="math inline">\(B_s(z) = \kappa_2 M_z^{-1}\operatorname{E}[ X_i X_i' \{ \beta_s(z)f_s(X_i,Z_i)/f(X_i|Z_i=z)\)</span> + <span class="math inline">\(f_z(Z_i) \beta_{ss}(Z_i)/2 \} |z]\)</span>, <span class="math inline">\(\kappa_2 = \int k(v)v^2dv\)</span>, <span class="math inline">\(\beta_s(z) = \partial \beta(z)/\partial z_s\)</span>, <span class="math inline">\(\beta_{ss}(z) = \partial^2 \beta(z)/\partial z_s^2\)</span>, <span class="math inline">\(\Omega_z = M_z^{-1} V_z M_z^{-1}\)</span>, <span class="math inline">\(V_z = \kappa^q f_z(z)\operatorname{E}[ X_i X_i' \sigma^2(X_i,Z_i) |Z_i=z]\)</span>, and <span class="math inline">\(\sigma^2(X_i,Z_i) =\operatorname{E}(u_i^2|X_i,Z_i)\)</span></p></li>
<li class="fragment"><p>The categorical only <span class="math inline">\(Z\)</span> case is treated in <span class="citation" data-cites="LI_OUYANG_RACINE:2013">Ouyang, Li, and Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2013</a>)</span> while the mixed data case is treated in <span class="citation" data-cites="LI_RACINE:2010">Li and Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2010</a>)</span></p></li>
</ul>
</section>
<section id="varying-coefficient-models-3" class="slide level2 center">
<h2>Varying Coefficient Models</h2>
<ul>
<li class="fragment"><p>Suppose that the researcher is unwilling to presume that the coefficients associated with the continuous variables do not vary with respect to the categorical variables <code>female</code> and <code>married</code></p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1scoef_329a194c16ea65ae9c7e02140817bb40">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="fu">library</span>(np)</span>
<span id="cb33-2"><a href="#cb33-2"></a><span class="fu">data</span>(wage1)</span>
<span id="cb33-3"><a href="#cb33-3"></a>model.scoef <span class="ot">&lt;-</span> <span class="fu">npscoef</span>(lwage<span class="sc">~</span>educ<span class="sc">+</span>tenure<span class="sc">+</span>exper<span class="sc">+</span>expersq<span class="sc">|</span>female<span class="sc">+</span>married,</span>
<span id="cb33-4"><a href="#cb33-4"></a>                       <span class="at">data=</span>wage1,</span>
<span id="cb33-5"><a href="#cb33-5"></a>                       <span class="at">betas=</span><span class="cn">TRUE</span>)</span>
<span id="cb33-6"><a href="#cb33-6"></a><span class="fu">summary</span>(model.scoef)</span>
<span id="cb33-7"><a href="#cb33-7"></a><span class="do">## </span></span>
<span id="cb33-8"><a href="#cb33-8"></a><span class="do">## Smooth Coefficient Model</span></span>
<span id="cb33-9"><a href="#cb33-9"></a><span class="do">## Regression data: 526 training points, in 2 variable(s)</span></span>
<span id="cb33-10"><a href="#cb33-10"></a><span class="do">## </span></span>
<span id="cb33-11"><a href="#cb33-11"></a><span class="do">##                    female   married</span></span>
<span id="cb33-12"><a href="#cb33-12"></a><span class="do">## Bandwidth(s): 0.001824223 0.1343076</span></span>
<span id="cb33-13"><a href="#cb33-13"></a><span class="do">## </span></span>
<span id="cb33-14"><a href="#cb33-14"></a><span class="do">## Bandwidth Type: Fixed</span></span>
<span id="cb33-15"><a href="#cb33-15"></a><span class="do">## </span></span>
<span id="cb33-16"><a href="#cb33-16"></a><span class="do">## Residual standard error: 0.3834084</span></span>
<span id="cb33-17"><a href="#cb33-17"></a><span class="do">## R-squared: 0.4787092</span></span>
<span id="cb33-18"><a href="#cb33-18"></a><span class="do">## </span></span>
<span id="cb33-19"><a href="#cb33-19"></a><span class="do">## Unordered Categorical Kernel Type: Aitchison and Aitken</span></span>
<span id="cb33-20"><a href="#cb33-20"></a><span class="do">## No. Unordered Categorical Explanatory Vars.: 2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
</ul>
</section>
<section id="varying-coefficient-models-4" class="slide level2 smaller center">
<h2>Varying Coefficient Models</h2>
<ul>
<li class="fragment"><p>Consider the average derivatives from the varying coefficient model</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1scoeftab_80e8b8a20e2f8aa747ffe3e00f43ec2c">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1"></a><span class="do">## The smooth coefficients are vectors, one for each predictor in X (they "vary"</span></span>
<span id="cb34-2"><a href="#cb34-2"></a><span class="do">## with Z), so we compute the means of the columns of these coefficients, one for</span></span>
<span id="cb34-3"><a href="#cb34-3"></a><span class="do">## each X predictor</span></span>
<span id="cb34-4"><a href="#cb34-4"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">colMeans</span>(<span class="fu">coef</span>(model.scoef)),<span class="at">digits=</span><span class="dv">7</span>,<span class="at">col.names=</span><span class="st">"Avg. Coefficient"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Avg. Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Intercept</td>
<td style="text-align: right;">0.3402132</td>
</tr>
<tr class="even">
<td style="text-align: left;">educ</td>
<td style="text-align: right;">0.0786505</td>
</tr>
<tr class="odd">
<td style="text-align: left;">tenure</td>
<td style="text-align: right;">0.0142986</td>
</tr>
<tr class="even">
<td style="text-align: left;">exper</td>
<td style="text-align: right;">0.0300507</td>
</tr>
<tr class="odd">
<td style="text-align: left;">expersq</td>
<td style="text-align: right;">-0.0005951</td>
</tr>
</tbody>
</table>
</div>
</div></li>
<li class="fragment"><p>The average derivatives are comparable in magnitude (except for the intercept), however, the smooth coefficient model performs better than the parametric specification in terms of in-sample fit (<span class="math inline">\(R^2\)</span> = 0.479 versus <span class="math inline">\(R^2\)</span> = 0.436 for the parametric model)</p></li>
</ul>
</section>
<section id="single-index-models" class="slide level2 center">
<h2>Single-Index Models</h2>
<ul>
<li class="fragment"><p>A semiparametric single index model is of the form <span class="math display">\[\begin{equation*}
  Y = g(X'\beta_0) + u,
\end{equation*}\]</span> where <span class="math inline">\(Y\)</span> is the dependent variable, <span class="math inline">\(X\in \mathbb{R}^q\)</span> is the vector of explanatory variables, <span class="math inline">\(\beta_0\)</span> is the <span class="math inline">\(q\times 1\)</span> vector of unknown parameters, and <span class="math inline">\(u\)</span> is an error term satisfying <span class="math inline">\(\operatorname{E}(u|X)=0\)</span></p></li>
<li class="fragment"><p>The term <span class="math inline">\(x'\beta_0\)</span> is called a <em>single index</em> because it is a scalar (a single index) even though <span class="math inline">\(x\)</span> is a vector</p></li>
<li class="fragment"><p>The functional form of <span class="math inline">\(g(\cdot)\)</span> is unknown to the researcher</p></li>
<li class="fragment"><p>This model is semiparametric in nature since the functional form of the linear index is specified while <span class="math inline">\(g(\cdot)\)</span> is left unspecified</p></li>
</ul>
</section>
<section id="identification" class="slide level2 center">
<h2>Identification</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment"><p>For semiparametric single index models, identification of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(g(\cdot)\)</span> requires that</p>
<ul>
<li class="fragment"><p><span class="math inline">\(x\)</span> should not contain a constant (intercept)</p></li>
<li class="fragment"><p><span class="math inline">\(x\)</span> must contain at least one continuous variable</p></li>
<li class="fragment"><p>Moreover, <span class="math inline">\(||\beta_0|| =1\)</span> or <span class="math inline">\(\beta_{01}=1\)</span> (i.e.&nbsp;we need some normalization of <span class="math inline">\(\beta\)</span>)</p></li>
<li class="fragment"><p><span class="math inline">\(g(\cdot)\)</span> is differentiable and is not a constant function on the support of <span class="math inline">\(x'\beta_0\)</span></p></li>
<li class="fragment"><p>For the discrete components of <span class="math inline">\(x\)</span>, varying the values of the discrete variables will not divide the support of <span class="math inline">\(x'\beta_0\)</span> into disjoint subsets</p></li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
<section id="ichimuras-method-continuous-y" class="slide level2 center">
<h2>Ichimura’s Method (Continuous <span class="math inline">\(y\)</span>)</h2>
<ul>
<li class="fragment"><p>We compute <span class="math inline">\(\beta\)</span> by estimating <span class="math inline">\(g(x'\beta)\)</span> using a local constant estimator and minimizing the following objective function (<span class="citation" data-cites="ICHIMURA:1993">Ichimura (<a href="#/references-scrollable" role="doc-biblioref" onclick="">1993</a>)</span>, <span class="citation" data-cites="HARDLE_HALL_ICHIMURA:1993">Härdle, Hall, and Ichimura (<a href="#/references-scrollable" role="doc-biblioref" onclick="">1993</a>)</span>): <span class="math display">\[\begin{equation*}
  \label{single_index:eq:hat beta Ich}
  S_n(\beta,h) = \sum_{i=1}^n
  \left[Y_i - \hat g_{-i}(X_i'\beta)\right]^2 w(X_i)\mathbf{1}(X_i\in
  A_n)
\end{equation*}\]</span></p></li>
<li class="fragment"><p><span class="math inline">\(\hat g_{-i}(X_i'\beta)\)</span> is a leave-one-out local constant kernel estimator and <span class="math inline">\(h\)</span> the scalar bandwidth for the index <span class="math inline">\(x'\beta\)</span></p></li>
<li class="fragment"><p><span class="math inline">\(w(X_i)\)</span> is a nonnegative weight function</p></li>
<li class="fragment"><p><span class="math inline">\(\mathbf{1}(\cdot)\)</span> is the usual indicator function</p></li>
<li class="fragment"><p>That is, <span class="math inline">\({\bf 1}(X_i\in A_n)\)</span> is a trimming function which equals one if <span class="math inline">\(X_i\in A_n\)</span>, zero otherwise</p></li>
</ul>
</section>
<section id="ichimuras-method-continuous-y-1" class="slide level2 center">
<h2>Ichimura’s Method (Continuous <span class="math inline">\(y\)</span>)</h2>
<ul>
<li class="fragment"><p>It can be shown that <span class="math display">\[\begin{equation*}
  \small
  \sqrt{n} (\hat \beta - \beta_0) \stackrel{d}{\rightarrow} N(0, \Omega_I)
\end{equation*}\]</span></p></li>
<li class="fragment"><p>Note that <span class="math inline">\(\Omega_{I} = V^{-1}\Sigma V^{-1}\)</span>, where <span class="math display">\[\begin{equation*}
  \small
  \Sigma = \operatorname{E}\Big\{ w(X_i)\sigma^2(X_i) \left(g^{(1)}_i\right)^2
  \left(X_i - \operatorname{E}_A(X_i|X_i'\beta_0)\right)
  \left(X_i - \operatorname{E}_A(X_i|X_i'\beta_0)\right)' \Big\}
\end{equation*}\]</span></p></li>
<li class="fragment"><p>Note further that <span class="math inline">\(g^{(1)}_i = [ \partial g(v)/\partial v]|_{v=X_i'\beta_0}\)</span>, <span class="math inline">\(\operatorname{E}_A(X_i|v) = \operatorname{E}(X_i|x_A'\beta_0 =v)\)</span> with <span class="math inline">\(x_A\)</span> having the distribution of <span class="math inline">\(X_i\)</span> conditional on <span class="math inline">\(X_i\in A_{\delta}\)</span></p></li>
<li class="fragment"><p>Finally, note that <span class="math display">\[\begin{equation*}
  \small
  V = \operatorname{E}\left[ w(X_i) \left(g^{(1)}_i\right)^2 \left(X_i - \operatorname{E}_A(X_i|X_i'\beta_0)\right)
    \left(X_i - \operatorname{E}_A(X_i|X_i'\beta_0)\right)'\right]
\end{equation*}\]</span></p></li>
</ul>
</section>
<section id="ichimuras-method-continuous-y-2" class="slide level2 smaller center">
<h2>Ichimura’s Method (Continuous <span class="math inline">\(y\)</span>)</h2>
<ul>
<li class="fragment"><p>We revisit Wooldridge’s wage1 dataset and estimate a sigle-index model using Ichimura’s method</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1index_faddbe8c809e639970e51590bbf648ed">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a><span class="fu">library</span>(np)</span>
<span id="cb35-2"><a href="#cb35-2"></a><span class="fu">data</span>(wage1)</span>
<span id="cb35-3"><a href="#cb35-3"></a>model.index <span class="ot">&lt;-</span> <span class="fu">npindex</span>(lwage<span class="sc">~</span>educ<span class="sc">+</span></span>
<span id="cb35-4"><a href="#cb35-4"></a>                       tenure<span class="sc">+</span></span>
<span id="cb35-5"><a href="#cb35-5"></a>                       exper<span class="sc">+</span></span>
<span id="cb35-6"><a href="#cb35-6"></a>                       expersq<span class="sc">+</span></span>
<span id="cb35-7"><a href="#cb35-7"></a>                       female<span class="sc">+</span></span>
<span id="cb35-8"><a href="#cb35-8"></a>                       married,</span>
<span id="cb35-9"><a href="#cb35-9"></a>                       <span class="at">method=</span><span class="st">"ichimura"</span>,</span>
<span id="cb35-10"><a href="#cb35-10"></a>                       <span class="at">data=</span>wage1)</span>
<span id="cb35-11"><a href="#cb35-11"></a><span class="fu">summary</span>(model.index)</span>
<span id="cb35-12"><a href="#cb35-12"></a><span class="do">## </span></span>
<span id="cb35-13"><a href="#cb35-13"></a><span class="do">## Single Index Model</span></span>
<span id="cb35-14"><a href="#cb35-14"></a><span class="do">## Regression Data: 526 training points, in 6 variable(s)</span></span>
<span id="cb35-15"><a href="#cb35-15"></a><span class="do">## </span></span>
<span id="cb35-16"><a href="#cb35-16"></a><span class="do">##       educ      tenure      exper       expersq    female     married</span></span>
<span id="cb35-17"><a href="#cb35-17"></a><span class="do">## Beta:    1 0.004673803 0.02084886 -0.0004225395 0.3691865 -0.07438324</span></span>
<span id="cb35-18"><a href="#cb35-18"></a><span class="do">## Bandwidth: 0.09557031</span></span>
<span id="cb35-19"><a href="#cb35-19"></a><span class="do">## Kernel Regression Estimator: Local-Constant</span></span>
<span id="cb35-20"><a href="#cb35-20"></a><span class="do">## </span></span>
<span id="cb35-21"><a href="#cb35-21"></a><span class="do">## Residual standard error: 0.3750512</span></span>
<span id="cb35-22"><a href="#cb35-22"></a><span class="do">## R-squared: 0.504711</span></span>
<span id="cb35-23"><a href="#cb35-23"></a><span class="do">## </span></span>
<span id="cb35-24"><a href="#cb35-24"></a><span class="do">## Continuous Kernel Type: Second-Order Gaussian</span></span>
<span id="cb35-25"><a href="#cb35-25"></a><span class="do">## No. Continuous Explanatory Vars.: 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
<li class="fragment"><p>It is interesting to compare this with the linear, partially linear, and varying coefficient specifications in terms of in-sample fit (<span class="math inline">\(R^2\)</span> = 0.452 for the partially linear model, <span class="math inline">\(R^2\)</span> = 0.479 for the smooth coefficient model, <span class="math inline">\(R^2\)</span> = 0.436 for the parametric model)</p></li>
</ul>
</section>
<section id="klein-spadys-method-binary-y" class="slide level2 center">
<h2>Klein &amp; Spady’s Method (binary <span class="math inline">\(y\)</span>)</h2>
<ul>
<li class="fragment"><p>For binary outcomes, <span class="citation" data-cites="KLEIN_SPADY:1993">Klein and Spady (<a href="#/references-scrollable" role="doc-biblioref" onclick="">1993</a>)</span> suggested estimating <span class="math inline">\(\beta\)</span> by maximum likelihood methods using a local constant estimator of <span class="math inline">\(g(x'\beta)\)</span></p></li>
<li class="fragment"><p>The estimated log-likelihood function is <span class="math display">\[\begin{equation*}
  {\mathcal L}(\beta,h) = \sum_i (1-Y_i) \ln(1 - \hat g_{-i}(X_i'\beta)) + \sum_i Y_i
  \ln[ \hat g_{-i}(X_i'\beta)]
\end{equation*}\]</span></p></li>
<li class="fragment"><p>Maximizing with respect to <span class="math inline">\(\beta\)</span> and <span class="math inline">\(h\)</span> leads to the semiparametric maximum likelihood estimator of <span class="math inline">\(\beta\)</span>, say <span class="math inline">\(\hat \beta_{KS}\)</span> (here <span class="math inline">\(h\)</span> is the smoothing parameter for the scalar index)</p></li>
<li class="fragment"><p>Like Ichimura’s estimator, maximization is performed numerically by solving the necessary first order conditions</p></li>
</ul>
</section>
<section id="klein-spadys-method-binary-y-1" class="slide level2 center">
<h2>Klein &amp; Spady’s Method (binary <span class="math inline">\(y\)</span>)</h2>
<ul>
<li class="fragment"><p>Klein &amp; Spady showed that <span class="math inline">\(\hat \beta_{KS}\)</span> is <span class="math inline">\(\sqrt{n}\)</span>-consistent and has an asymptotic normal distribution given by <span class="math display">\[\begin{equation*}
  \sqrt{n}(\hat \beta_{KS} - \beta) \stackrel{d}{\rightarrow} N(0, \Omega_{KS})
\end{equation*}\]</span></p></li>
<li class="fragment"><p>Note that <span class="math display">\[\begin{equation*}
  \Omega_{KS} = \left[ \operatorname{E}\left\{
      \frac{ \partial P}{\partial \beta}
      \left(\frac{\partial P}{\partial \beta}\right)'\left[\frac{1}{P(1-P)}\right] \right\} \right]^{-1}
\end{equation*}\]</span></p></li>
<li class="fragment"><p>Also, <span class="math inline">\(P=P(\epsilon &lt;x'\beta)=F_{\epsilon|x}(x'\beta)\)</span>, where <span class="math inline">\(F_{\epsilon|x}(\cdot)\)</span> is the CDF of <span class="math inline">\(\epsilon_i\)</span> conditional on <span class="math inline">\(X_i=x\)</span></p></li>
</ul>
</section>
<section id="klein-spadys-method-binary-y-2" class="slide level2 center">
<h2>Klein &amp; Spady’s Method (binary <span class="math inline">\(y\)</span>)</h2>
<ul>
<li class="fragment"><p>We consider the birthwt data (<span class="math inline">\(y\)</span> is 0/1) in the MASS package and compute a Logit and a semiparametric index model</p></li>
<li class="fragment"><p>We then compare their confusion matrices and assess their classification ability</p></li>
<li class="fragment"><p>Variables are defined as follows:</p>
<div style="font-size: 70%;">
<ul>
<li class="fragment"><p><code>low</code> indicator of birth weight less than 2.5kg</p></li>
<li class="fragment"><p><code>smoke</code> smoking status during pregnancy</p></li>
<li class="fragment"><p><code>race</code> mother’s race (‘1’ = white, ‘2’ = black, ‘3’ = other)</p></li>
<li class="fragment"><p><code>ht</code> history of hypertension</p></li>
<li class="fragment"><p><code>ui</code> presence of uterine irritability</p></li>
<li class="fragment"><p><code>ftv</code> number of physician visits during the first trimester</p></li>
<li class="fragment"><p><code>age</code> mother’s age in years</p></li>
<li class="fragment"><p><code>lwt</code> mother’s weight in pounds at last menstrual period</p></li>
</ul>
</div></li>
<li class="fragment"><p>Note that all variables other than <code>age</code> and <code>lwt</code> are categorical in nature</p></li>
</ul>
</section>
<section id="klein-spadys-method-binary-y-3" class="slide level2 smaller center">
<h2>Klein &amp; Spady’s Method (binary <span class="math inline">\(y\)</span>)</h2>
<ul>
<li class="fragment"><p>First, consider the parametric Logit model’s confusion matrix</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1logit_281ca2a81ee203bf8f2a48f7ae50bddb">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb36-2"><a href="#cb36-2"></a><span class="fu">data</span>(<span class="st">"birthwt"</span>)</span>
<span id="cb36-3"><a href="#cb36-3"></a>model.logit <span class="ot">&lt;-</span> <span class="fu">glm</span>(low<span class="sc">~</span><span class="fu">factor</span>(smoke)<span class="sc">+</span></span>
<span id="cb36-4"><a href="#cb36-4"></a>                   <span class="fu">factor</span>(race)<span class="sc">+</span></span>
<span id="cb36-5"><a href="#cb36-5"></a>                   <span class="fu">factor</span>(ht)<span class="sc">+</span></span>
<span id="cb36-6"><a href="#cb36-6"></a>                   <span class="fu">factor</span>(ui)<span class="sc">+</span></span>
<span id="cb36-7"><a href="#cb36-7"></a>                   <span class="fu">ordered</span>(ftv)<span class="sc">+</span></span>
<span id="cb36-8"><a href="#cb36-8"></a>                   age<span class="sc">+</span></span>
<span id="cb36-9"><a href="#cb36-9"></a>                   lwt,</span>
<span id="cb36-10"><a href="#cb36-10"></a>                   <span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span>logit),<span class="at">data=</span>birthwt)</span>
<span id="cb36-11"><a href="#cb36-11"></a>cm.logit <span class="ot">&lt;-</span> <span class="fu">with</span>(birthwt,<span class="fu">table</span>(low, <span class="fu">ifelse</span>(<span class="fu">fitted</span>(model.logit)<span class="sc">&gt;</span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)))</span>
<span id="cb36-12"><a href="#cb36-12"></a>ccr.logit <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(cm.logit))<span class="sc">/</span><span class="fu">sum</span>(cm.logit)</span>
<span id="cb36-13"><a href="#cb36-13"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(cm.logit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">0</th>
<th style="text-align: right;">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0</td>
<td style="text-align: right;">119</td>
<td style="text-align: right;">11</td>
</tr>
<tr class="even">
<td style="text-align: left;">1</td>
<td style="text-align: right;">34</td>
<td style="text-align: right;">25</td>
</tr>
</tbody>
</table>
</div>
</div></li>
<li class="fragment"><p>The correct classification ratio is CCR = 0.762 (the in-sample correct classification ratio is the sum of the diagonals of the confusion matrix divided by the number of observations)</p></li>
</ul>
</section>
<section id="klein-spadys-method-binary-y-4" class="slide level2 smaller center">
<h2>Klein &amp; Spady’s Method (binary <span class="math inline">\(y\)</span>)</h2>
<ul>
<li class="fragment"><p>Next, consider the semiparametric single index model’s confusion matrix</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/revealjs/wage1indexks_ae2a83c948f5ec1f8aab3262ea28b6e6">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb37-2"><a href="#cb37-2"></a><span class="fu">data</span>(birthwt)</span>
<span id="cb37-3"><a href="#cb37-3"></a><span class="fu">library</span>(np)</span>
<span id="cb37-4"><a href="#cb37-4"></a>model.index <span class="ot">&lt;-</span> <span class="fu">npindex</span>(low<span class="sc">~</span><span class="fu">factor</span>(smoke)<span class="sc">+</span></span>
<span id="cb37-5"><a href="#cb37-5"></a>                       <span class="fu">factor</span>(race)<span class="sc">+</span></span>
<span id="cb37-6"><a href="#cb37-6"></a>                       <span class="fu">factor</span>(ht)<span class="sc">+</span></span>
<span id="cb37-7"><a href="#cb37-7"></a>                       <span class="fu">factor</span>(ui)<span class="sc">+</span></span>
<span id="cb37-8"><a href="#cb37-8"></a>                       <span class="fu">ordered</span>(ftv)<span class="sc">+</span></span>
<span id="cb37-9"><a href="#cb37-9"></a>                       age<span class="sc">+</span></span>
<span id="cb37-10"><a href="#cb37-10"></a>                       lwt,</span>
<span id="cb37-11"><a href="#cb37-11"></a>                       <span class="at">method=</span><span class="st">"kleinspady"</span>,</span>
<span id="cb37-12"><a href="#cb37-12"></a>                       <span class="at">data=</span>birthwt)</span>
<span id="cb37-13"><a href="#cb37-13"></a>cm.index <span class="ot">&lt;-</span> <span class="fu">with</span>(birthwt,<span class="fu">table</span>(low, <span class="fu">ifelse</span>(<span class="fu">fitted</span>(model.index)<span class="sc">&gt;</span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)))</span>
<span id="cb37-14"><a href="#cb37-14"></a>ccr.index <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(cm.index))<span class="sc">/</span><span class="fu">sum</span>(cm.index)</span>
<span id="cb37-15"><a href="#cb37-15"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(cm.index)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">0</th>
<th style="text-align: right;">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0</td>
<td style="text-align: right;">119</td>
<td style="text-align: right;">11</td>
</tr>
<tr class="even">
<td style="text-align: left;">1</td>
<td style="text-align: right;">22</td>
<td style="text-align: right;">37</td>
</tr>
</tbody>
</table>
</div>
</div></li>
<li class="fragment"><p>In terms of in-sample fit, the semiparametric model outperforms the parametric Logit model (CCR = 0.825 versus CCR = 0.762 for the Logit model)</p></li>
</ul>
</section>
<section id="semiparametric-model-summary" class="slide level2 center">
<h2>Semiparametric Model Summary</h2>
<ul>
<li class="fragment"><p>Semiparametric regression models are very popular in applied settings</p></li>
<li class="fragment"><p>They exist to circumvent the <em>curse of dimensionality</em></p></li>
<li class="fragment"><p>They are also popular because they contain scalar parameters that are easy to interpret</p></li>
<li class="fragment"><p>However, realize that since part of the model is parametric and must be specified prior to estimation, these models too can be misspecified, just like their parametric counterparts</p></li>
<li class="fragment"><p>Nonetheless, they are useful models that ought to be part of the applied econometrician’s toolkit</p></li>
</ul>
</section></section>
<section>
<section id="quarto-technical-reproducible-documents" class="title-slide slide level1 center">
<h1>Quarto: Technical, Reproducible Documents</h1>

</section>
<section id="a-sub-optimal-workflow" class="slide level2 center">
<h2>A Sub-Optimal Workflow</h2>
<ul>
<li class="fragment"><p>The old way of doing research is to run your analyses using R, then to manually copy figures and tables into a word processor (e.g., MS Word) or typesetter (e.g., <span class="math inline">\(\rm\TeX\)</span>)</p></li>
<li class="fragment"><p>So you end up maintaining separate code, figures, and narrative files and being their go-between coordinator</p></li>
<li class="fragment"><p>And you have pre-committed to an output format!</p></li>
<li class="fragment"><p>Then you find a code or data error and need to redo figures, tables, etc., or need to change output formats</p></li>
<li class="fragment"><p>This is a brutally inefficient and error prone process because it requires a go-between (i.e., you!)</p></li>
</ul>
</section>
<section id="a-streamlined-workflow" class="slide level2 center">
<h2>A Streamlined Workflow</h2>
<ul>
<li class="fragment"><p>Quarto places your code and narrative into the same file</p></li>
<li class="fragment"><p>Relieved of coordination duties, new possibilities emerge</p>
<ul>
<li class="fragment"><p>you now <em>automatically</em> refer to analytical results</p></li>
<li class="fragment"><p>you now <em>conditionally</em> describe analytical results</p></li>
</ul></li>
<li class="fragment"><p>If code or data is updated, figures and tables are automatically updated in your output document</p></li>
<li class="fragment"><p>Perhaps best of all, your Quarto file can be <em>rendered</em> into any imaginable output format seamlessly via <a href="https://pandoc.org">Pandoc</a></p></li>
<li class="fragment"><p>(<a href="https://quarto.org">Quarto</a>, <a href="https://github.com">GitHub</a> and <a href="https://rstudio.github.io/renv">renv</a> are all supported in <a href="https://posit.co/products/open-source/rstudio/">RStudio</a>)</p></li>
</ul>
</section>
<section id="rstudio-quarto-editor-interface" class="slide level2 center">
<h2>RStudio Quarto Editor Interface</h2>

<img data-src="images/rstudio_quarto.png" class="r-stretch"></section></section>
<section id="show-and-tell-time-time-to-play" class="title-slide slide level1 center">
<h1>Show and Tell Time <br> Time to Play!</h1>

</section>

<section id="summary" class="title-slide slide level1 center">
<h1>Summary</h1>
<ul>
<li class="fragment"><p>We have barely scratched the surface and have skipped technical details</p></li>
<li class="fragment"><p>However, you now have some perspective and have access to modern tools</p></li>
<li class="fragment"><p>You can consult <span class="citation" data-cites="racine_2019">Racine (<a href="#/references-scrollable" role="doc-biblioref" onclick="">2019</a>)</span> if you wish to go deeper (detailed proofs, R code, examples etc.)</p></li>
<li class="fragment"><p>Many of the links in these slides may also be useful</p></li>
<li class="fragment"><p>You may contact me at <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a></p></li>
<li class="fragment"><p>It has been my pleasure to guide you!</p></li>
</ul>
</section>

<section id="references-scrollable" class="title-slide slide level1 smaller scrollable">
<h1>References (scrollable)</h1>
<div class="footer footer-default">
<p>Workshop | J. Racine</p>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-BROWN_LEVINE:2007" class="csl-entry" role="listitem">
Brown, Lawrence D., and Michael Levine. 2007. <span>“Variance Estimation in Nonparametric Regression via the Difference Sequence Method.”</span> <em>The Annals of Statistics</em> 35 (5): 2219–32.
</div>
<div id="ref-CHEN_CHENG_PENG:2009" class="csl-entry" role="listitem">
Chen, Lu-Hung, Ming-Yen Cheng, and Liang Peng. 2009. <span>“Conditional Variance Estimation in Heteroscedastic Regression Models.”</span> <em>Journal of Statistical Planning and Inference</em> 139 (2): 236–45.
</div>
<div id="ref-CRAVEN_WAHBA:1979" class="csl-entry" role="listitem">
Craven, P., and G. Wahba. 1979. <span>“Smoothing Noisy Data with Spline Functions.”</span> <em>Numerische Mathematik</em> 13: 377–403.
</div>
<div id="ref-ENGLE:1982" class="csl-entry" role="listitem">
Engle, R. F. 1982. <span>“Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of UK Inflation.”</span> <em>Econometrica</em> 50: 987–1008.
</div>
<div id="ref-FAN_YAO:1998" class="csl-entry" role="listitem">
Fan, J., and Q. Yao. 1998. <span>“Efficient Estimation of Conditional Variance Functions in Stochastic Regression.”</span> <em>Biometrika</em> 85: 645–60.
</div>
<div id="ref-FOX:2011" class="csl-entry" role="listitem">
Fox, John, and Sandford Weisberg. 2011. <em>An <span>R</span> Companion to Applied Regression</em>. Second. Thousand Oaks <span>CA</span>: Sage. <a href="http://socserv.socsci.mcmaster.ca/jfox/Books/Companion">http://socserv.socsci.mcmaster.ca/jfox/Books/Companion</a>.
</div>
<div id="ref-GAO_LIU_RACINE:2015" class="csl-entry" role="listitem">
Gao, Q., L. Liu, and J. S. Racine. 2015. <span>“A Partially Linear Kernel Estimator for Categorical Data.”</span> <em>Econometric Reviews</em> 34 (6–10): 958–77.
</div>
<div id="ref-HALL_LI_RACINE:2007" class="csl-entry" role="listitem">
Hall, P. G., Q. Li, and J. S. Racine. 2007. <span>“Nonparametric Estimation of Regression Functions in the Presence of Irrelevant Regressors.”</span> <em>The Review of Economics and Statistics</em> 89: 784–89.
</div>
<div id="ref-HALL_RACINE:2015" class="csl-entry" role="listitem">
Hall, P. G., and J. S. Racine. 2015. <span>“Infinite-Order Cross-Validated Local Polynomial Regression.”</span> <em>Journal of Econometrics</em> 185: 510–25.
</div>
<div id="ref-HARDLE_HALL_ICHIMURA:1993" class="csl-entry" role="listitem">
Härdle, W., P. G. Hall, and H. Ichimura. 1993. <span>“Optimal Smoothing in Single Index Models.”</span> <em>Annals of Statistics</em> 21: 157–78.
</div>
<div id="ref-np" class="csl-entry" role="listitem">
Hayfield, T., and J. S. Racine. 2008. <span>“Nonparametric Econometrics: The <span class="nocase">np</span> Package.”</span> <em>Journal of Statistical Software</em> 27 (5): 1–32. <a href="https://doi.org/10.18637/jss.v027.i05">https://doi.org/10.18637/jss.v027.i05</a>.
</div>
<div id="ref-HENDERSON_CARROLL_LI:2008" class="csl-entry" role="listitem">
Henderson, D. J., R. J. Carroll, and Q. Li. 2008. <span>“Nonparametric Estimation and Testing of Fixed Effects Panel Data Models.”</span> <em>Journal of Econometrics</em> 144: 257–75.
</div>
<div id="ref-HURVICH_SIMONOFF_TSAI:1998" class="csl-entry" role="listitem">
Hurvich, C. M., J. S. Simonoff, and C. L. Tsai. 1998. <span>“Smoothing Parameter Selection in Nonparametric Regression Using an Improved <span>A</span>kaike Information Criterion.”</span> <em>Journal of the Royal Statistical Society Series B</em> 60: 271–93.
</div>
<div id="ref-ICHIMURA:1993" class="csl-entry" role="listitem">
Ichimura, H. 1993. <span>“Semiparametric Least Squares (<span>SLS</span>) and Weighted <span>SLS</span> Estimation of Single-Index Models.”</span> <em>Journal of Econometrics</em> 58: 71–120.
</div>
<div id="ref-KLEIN_SPADY:1993" class="csl-entry" role="listitem">
Klein, R. W., and R. H. Spady. 1993. <span>“An Efficient Semiparametric Estimator for Binary Response Models.”</span> <em>Econometrica</em> 61: 387–421.
</div>
<div id="ref-LEE_ROBINSON:2015" class="csl-entry" role="listitem">
Lee, Jungyoon, and Peter M. Robinson. 2015. <span>“Panel Nonparametric Regression with Fixed Effects.”</span> <em>Journal of Econometrics</em>.
</div>
<div id="ref-LI_HUANG_LI_FU:2002" class="csl-entry" role="listitem">
Li, Q., C. J. Huang, D. Li, and T. T. Fu. 2002. <span>“Semiparametric Smooth Coefficient Models.”</span> <em>Journal of Business and Economics Statistics</em> 20: 412–22.
</div>
<div id="ref-LI_RACINE:2010" class="csl-entry" role="listitem">
Li, Q., and J. S. Racine. 2010. <span>“Smooth Varying-Coefficient Estimation and Inference for Qualitative and Quantitative Data.”</span> <em>Econometric Theory</em> 26: 1607–37.
</div>
<div id="ref-MARTINS_FILHO_YAO:2009" class="csl-entry" role="listitem">
Martins-Filho, Carlos, and Feng Yao. 2009. <span>“<span class="nocase">Nonparametric regression estimation with general parametric error covariance</span>.”</span> <em>Journal of Multivariate Analysis</em> 100 (3): 309–33.
</div>
<div id="ref-LI_OUYANG_RACINE:2013" class="csl-entry" role="listitem">
Ouyang, D., Q. Li, and J. S. Racine. 2013. <span>“Categorical Semiparametric Varying Coefficient Models.”</span> <em>Journal of Applied Econometrics</em> 28 (3): 551–79.
</div>
<div id="ref-RACINE:1997" class="csl-entry" role="listitem">
Racine, J. S. 1997. <span>“Consistent Significance Testing for Nonparametric Regression.”</span> <em>Journal of Business and Economic Statistics</em> 15 (3): 369–79.
</div>
<div id="ref-10.2307/41337225" class="csl-entry" role="listitem">
———. 2012. <span>“RStudio: A Platform-Independent <span>IDE</span> for <span>R</span> and <span>Sweave</span>.”</span> <em>Journal of Applied Econometrics</em> 27 (1): 167–72. <a href="http://www.jstor.org/stable/41337225">http://www.jstor.org/stable/41337225</a>.
</div>
<div id="ref-racine_2019" class="csl-entry" role="listitem">
———. 2019. <em>An Introduction to the Advanced Theory and Practice of Nonparametric Econometrics: A Replicable Approach Using <span>R</span></em>. Cambridge University Press. <a href="https://doi.org/10.1017/9781108649841">https://doi.org/10.1017/9781108649841</a>.
</div>
<div id="ref-RACINE_HART_LI:2006" class="csl-entry" role="listitem">
Racine, J. S., J. D. Hart, and Q. Li. 2006. <span>“Testing the Significance of Categorical Predictor Variables in Nonparametric Regression Models.”</span> <em>Econometric Reviews</em> 25: 523–44.
</div>
<div id="ref-ROBINSON:1988" class="csl-entry" role="listitem">
Robinson, P. M. 1988. <span>“Root-n Consistent Semiparametric Regression.”</span> <em>Econometrica</em> 56: 931–54.
</div>
<div id="ref-RUCKSTUHL_WELSH_CARROLL:2000" class="csl-entry" role="listitem">
Ruckstuhl, A., A. H. Welsh, and R. J. Carroll. 2000. <span>“Nonparametric Function Estimation of the Relationship Between Two Repeatedly Measured Variables.”</span> <em>Statistica Sinica</em> 10: 51–71.
</div>
<div id="ref-SU_ULLAH:2007" class="csl-entry" role="listitem">
Su, Liangjun, and Aman Ullah. 2007. <span>“<span class="nocase">More efficient estimation of nonparametric panel data models with random effects</span>.”</span> <em>Economics Letters</em> 96 (3): 375–80.
</div>
<div id="ref-WANG_BROWN_CAI_LEVINE:2008" class="csl-entry" role="listitem">
Wang, Lie, Lawrence D. Brown, T. Tony Cai, and Michael Levine. 2008. <span>“Effect of Mean on Variance Function Estimation in Nonparametric Regression.”</span> <em>The Annals of Statistics</em> 36 (2): 646–64.
</div>
<div id="ref-YU_JONES:2004" class="csl-entry" role="listitem">
Yu, Keming, and M. C. Jones. 2004. <span>“Likelihood-Based Local Linear Estimation of the Conditional Variance Functio.”</span> <em>Journal of the American Statistical Association</em> 99: 139–44.
</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="index_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="index_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="index_files/libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="index_files/libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="index_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="index_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="index_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="index_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="index_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'multiplex': {"secret":null,"id":"775d824ec7014bd2","url":"https://reveal-multiplex.glitch.me/"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
              // target, if specified
              link.setAttribute("target", "_blank");
          }
        }
    });
    </script>
    

</body></html>