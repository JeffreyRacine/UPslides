
# Welcome!

- The website with install info for R, RStudio, and $\rm\TeX$ is

  <https://jeffreyracine.github.io/UPworkshop>

- The GitHub repository with example code is

  <https://github.com/JeffreyRacine/UPworkshop>

- Don't be shy, feel free to ask questions.

- Let's get going!

## Welcome!

- You will be guided through basic nonparametric kernel methods using R

- You will also, if you are patient, be introduced to recently released tools for conducting reproducible research

- No background knowledge of either nonparametric analysis or the R programming language is required

- All software is available for *free* and is *open source*

- For more detailed descriptions see  @racine_2019, @10.2307/41337225, and @np

# Overview

- I hope that you leave this workshop armed with a modern set of data-analytic tools

- Let's briefly discuss the following:

  - <a href="https://www.r-project.org/about.html">What is R?</a>
  
  - <a href="https://posit.co/products/open-source/rstudio/">What is RStudio?</a>
  
  - <a href="https://en.wikipedia.org/wiki/Kernel_regression">What is Kernel Regression?</a>
  
  - <a href="https://quarto.org">What is Quarto?</a>
  
# Background <br> Data Types

## R and Data Types

::: callout-important
*Pre-cast* data *prior to analysis* so R functions can do their job
:::

- It is *crucial* to understand different data types in R

- There are three R functions we will use: `numeric()`, `factor()`, and `ordered()`

- These correspond to data that are *numbers*, *unordered categories*, and *ordered categories*, respectively

- Pay careful attention to the following example

## R and Data Types {.smaller}

- Below is an example of how we *cast* our data in R *prior* to analyzing it

- Copy and paste this into a new R file in RStudio, then run it line-by-line

  ```{r datatype}
  #| echo: true
  #| eval: false
  ## Generate some data: sex (unordered categorical), income (ordered categorical),
  ## and height (numeric)
  n <- 100
  sex <- sample(c("Female","Male"),n,replace=TRUE,prob=c(.4,.6))
  income <- sample(c("Low","Middle","High"),n,replace=TRUE,prob=c(.3,.5,.2))
  height <- rnorm(n,mean=150,sd=20)
  ## Note - by default these variables may not be the data types we desire
  class(sex);class(income);class(height)
  ## income is already numeric(), but sex and height are character()
  ## sex is categorical and unordered, so cast as type factor()
  sex <- factor(sex)
  ## income is categorical and ordered, but we need to ensure intended order (it
  ## will assume alphabetical ordering otherwise). Suppose you ignore it - let's
  ## see what happens when we just cast as type ordered() using defaults
  income <- ordered(income)
  ## The levels are in alphabetical order, which we don't want
  levels(income)
  ## We shall reorder the ordered factor levels as intended using levels=...
  income <- ordered(income,levels=c("Low","Middle","High"))
  levels(income)
  ## Check data types again
  class(sex);class(income);class(height)
  ## Note that with integers the default ordered() works fine
  x <- sample(c(2,5,4,3,1),n,replace=TRUE)
  x <- ordered(x)
  levels(x)
  ```
  
::: {.notes}
Pull up RStudio and patiently explain the interface then run the example - don't waste time but realize this could be their first exposure to R/RStudio.
:::

## RStudio R Editor Interface
  
![](images/rstudio_rcode.png)

# Nonparametric Essentials <br> Parametric vs. Nonparametric

## Parametric or Nonparametric?

- Suppose we need to estimate the density $f(x)$ of some numeric random variable $X$
  
- Suppose that we naively presume that our data is generated from the normal parametric family of distributions, namely
    \begin{equation*}
      f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}
      \exp\left\{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right\}
    \end{equation*}

- This distributional family depends on two unknown parameters, $\mu$ and $\sigma$, which can be estimated given data

- That is, given estimates of the mean and standard deviation, then assuming the data is generated from this family *prior to estimation* we can construct an estimate of the density

## Parametric or Nonparametric? {.smaller}

Let's simulate some data where we know the distribution that generated the sample (we don't have this luxury in applied settings), then estimate the density using the true density function (which is not known in applied settings)

```{r  parnpeval}
#| eval: true
#| echo: true
## Let's simulate a random numeric sample from the normal distribution
set.seed(42)
n <- 1000
x <- rnorm(n)
## Let's sort the data so we can graph x versus dnorm(x,...) using lines (type="l")
x <- sort(x)
## Since we simulated the data, let's plot the true, known, parametric density
## (we can't do this with actual data because the density of such data is, in
## general, unknown)
plot(x,dnorm(x,mean=mean(x),sd=sd(x)),type="l",ylab="Parametric Density Estimate",xlab="X")
```

## Parametric or Nonparametric?

- The R function `shapiro.test(x)` tests for normality
  
  ```{r shapiro}
  pander::pander(shapiro.test(x))
  ```

- This is *simulated normal* data, and we fail to reject the null

  ::: callout-warning
  What if we got the null density wrong (i.e., what if we rejected the null)?
  :::

## Parametric or Nonparametric?

- Now let's test normality for actual data, `eruptions` (we will discuss this data in detail shortly)

  ```{r shapiroeruptions}
  data(faithful)
  with(faithful,pander::pander(shapiro.test(eruptions)))
  ```

  ::: callout-warning
  Oh oh... I guess we rule out the normal parametric model... what next?
  :::

## Parametric or Nonparametric?

- Actual data ($X_i$, $i=1,\dots,n$) is rarely drawn from the simple parametric distributions used in *sampling theory*

- Sampling theory describes *summary statistics*, e.g., *averages* of actual data, or *averages* of squared deviations, such as 
  \begin{align*}
  \hat\mu&=\frac{1}{n}\sum_{i=1}^n X_i\quad\text{ or }\quad\hat\sigma^2=\frac{1}{n-1}\sum_{i=1}^n (X_i -\hat\mu)^2&
  \end{align*}

- But we need to estimate the *density of the actual data* (i.e., $f(x)$), not the *density of some summary statistic* (i.e., $f(\hat\mu)$)

## Parametric or Nonparametric?

- Below is a nonparametric density estimate and data *rug* for the `eruptions` data (it is bi-modal *and* asymmetric - we will discuss this estimator shortly)

  ```{r densityeruptions}
  plot(density(faithful$eruptions),main="")
  rug(faithful$eruptions)
  ```

## Parametric or Nonparametric?

- Below is a nonparametric and *normal* parametric estimate (recall the normal parametric model has been *rejected*)

  ```{r densityeruptionscomp}
  plot(density(faithful$eruptions),main="")
  with(faithful,lines(density(eruptions)$x,dnorm(density(eruptions)$x,mean=mean(eruptions),sd=sd(eruptions)),col=2,lty=2))
  rug(faithful$eruptions)
  legend("topleft",c("Nonparametric","Parametric"),lty=c(1,2),col=c(1,2),bty="n")
  ```

## Parametric or Nonparametric?

- Here lies the crux of the parametric problem

- We write down some parametric model that is drawn from a *dense* space of functions (their number is *uncountable*)

- One very important assumption is *this parametric model is the true model*, and all model properties (unbiasedness, consistency etc.) depend *crucially* on this being the case

- But if you are serious, you immediately *test* your model for correct specification

- What if the parametric model is rejected, as it often is?
  
# Nonparametric Essentials <br> Numeric Data
  
## Numeric Data

- Parametric methods require the user to make *very strong assumptions* about the *data generating process* (DGP) 

- We just considered density estimation, but the exact same issue plagues all parametric analysis (i.e., regression, etc.)

- We will consider, instead, nonparametric *kernel* estimators

- A *kernel* is simply a weight function, which for *numeric* data we denote by $K(Z_i)$ where $Z_i=(x-X_i)/h$

- The kernel function assigns *higher* weight to observations close to $x$ (i.e., small $Z_i$) than to those lying further away (i.e., large $Z_i$)

- A kernel estimator is *nonparametric* since it does not presume the data is from some known parametric family prior to estimation

## Non-Smooth or Smooth?

- Our choice of kernel functions affects the nonparametric estimate

- Below are non-smooth and smooth nonparametric density estimates for the *numeric* variable `eruptions` (we will define each estimator shortly)

  ```{r histdensityeruptions}
  hist(faithful$eruptions,prob=TRUE,main="",xlab="Eruptions",breaks=20,xlim=c(1.25,5.5))
  with(faithful,lines(density(eruptions)$x,fitted(npudens(tdat=eruptions,edat=density(eruptions)$x))))
  rug(faithful$eruptions)
  ```

## Non-Smooth or Smooth?

- The non-smooth nonparametric histogram density estimator is
  \begin{equation*} 
  f_H(x)=\frac{1}{nh}\sum_{i=1}^n\mathbf{1}(X_i\text{ is in the same bin as }x) 
  \end{equation*} 
  
- This estimator has drawbacks, including

  - it is not particularly *efficient* in the statistical sense 
  
  - the estimator's discontinuity presents obstacles if derivatives are required ($df_H(x)/dx$ is 0 or undefined)
  

##  Smooth Univariate Density Estimation

- Consider a symmetric smooth kernel function that satisfies $K(z)\ge 0$ and $\int_{-\infty}^{\infty}K(z)\,dz=1$

- The smooth nonparametric kernel density estimator is
  \begin{equation*}
  \hat f(x)=\frac{1}{nh}\sum_{i=1}^n K\left(\frac{x-X_i}{h}\right)
  \end{equation*}
  
- Note that we can easily switch between non-smooth and smooth estimators by simply changing the kernel function used (see the `ckertype=` option in the `np` package)

- The smooth estimator is dominant in applied settings, though *boundary bias* can be an issue (simple corrections exist, see `npuniden.boundary()` in the `np` package)

## Smooth Univariate Density Estimation

::: callout-important
- The kernel function $K(z)$ is relatively unimportant (it imparts *smoothness* on the estimate, where smooth functions are *continuously differentiable* functions)

- The bandwidth $h$ is crucial (it governs the bias-variance trade-off)
:::

- *Closeness* is determined by a *bandwidth*, denoted $h$

- We choose $h$ to minimize *square error risk* and trade off *bias* for *variance* for the sample at hand

- To accomplish this we use *data-driven* methods for selecting $h$ (e.g., least-squares cross-validation and likelihood cross-validation)

## Smooth Univariate Density Estimation

- The pointwise bias (based on a *second order kernel*, i.e., $\kappa_2\ne0$) is 
    \begin{equation*}
      \operatorname{bias}\hat f(x)\approx \frac{h^2}{2}f^{''}(x)\kappa_2,\qquad \kappa_2=\int z^2K(z)\,dz<\infty
    \end{equation*}

- Note that the bias falls as $h$ decreases

- The pointwise variance, used to construct asymptotic confidence intervals by replacing $f(x)$ with $\hat f(x)$, is
    \begin{equation*}
      \operatorname{var}\hat f(x)\approx\frac{f(x)\kappa}{nh},\qquad \kappa=\int K^2(z)dz
    \end{equation*}
    
-  Note that the variance rises as $h$ decreases

## Smooth Univariate Density Estimation

- The integrated mean square error, denoted IMSE, aggregates
    the pointwise mean square error over the domain of the density
    yielding a global error measure, and can be defined as
    \begin{align*}
      \operatorname{IMSE}{\hat f(x)} &= \int \operatorname{mse}{\hat f(x)} dx\cr
     &=\int \operatorname{var}{\hat f(x)}\, dx + \int \left\{\operatorname{bias}{\hat f(x)}\right\}^2dx
    \end{align*}

- The IMSE is used to derive optimal (but infeasible) bandwidths

- The IMSE also helps assess data-driven bandwidth selection methods (i.e., how close to optimal is a data-driven method)

## Smooth Univariate Density Estimation

- We obtain the infeasible optimal bandwidth balancing bias and variance by minimizing IMSE with respect to $h$, i.e.
    \begin{align*}
        h_{opt}&=\kappa^{1/5} \kappa_2^{-2/5}\Phi^{-1/5}n^{-1/5}\cr
        &=\left\{\frac{\int K^2(z)dz}{\left(\int
              z^2K(z)dz\right)^2\int
            \left\{f^{''}(x)\right\}^2dx}\right\}^{1/5} n^{-1/5}\cr
        &=cn^{-1/5}
    \end{align*}

- Note that the constant $c$ depends on $f^{''}(x)$ and $K(z)$ (the latter we know and can compute, the former is unknown)

## Data-Driven Bandwidth Selection

- Likelihood cross validation is a popular data-driven method that  chooses $h$ to *maximize* the (leave-one-out) log likelihood function
    \begin{equation*}
      \mathcal{L} = \log\, L = \sum_{i=1}^n \log\, \hat f_{-i}(X_i)
    \end{equation*}

- Here, $\hat f_{-i}(X_i)$ is the leave-one-out kernel estimator of $f(X_i)$ that uses all points except $X_i$ to construct the
    density estimate, that is,
    \begin{equation*}
      \hat f_{-i}(X_i)=\frac{1}{(n-1)h}\sum_{j=1,j\ne i}^n K\left(\frac{X_i-X_j}{h}\right)
    \end{equation*}
    
- See the option `bwmethod="cv.ml"` in the R package np

## Data-Driven Bandwidth Selection

- Least squares cross validation is another data-driven method based on the principle of choosing $h$ to *minimize* the integrated square error of the resulting estimate

- The integrated squared difference between $\hat f$ and $f$, i.e.,  $\int (\hat f(x) - f(x))^2\, dx$, is
    \begin{equation*}
     \int \hat f(x)^2\, dx - 2\int \hat f(x) f(x)\, dx + \int f(x)^2\, dx
    \end{equation*}

- We can insert the formula for $\hat f(x)$ into these three terms, adjust for bias (delete-one trick), and obtain an objective function that can be minimized using numerical search algorithms

- See the option `bwmethod="cv.ls"` in the R package np

## Smooth Univariate Density Estimation

```{r npudenseruptions}
#| echo: true
library(np)
data(faithful)
fhat <- npudens(~eruptions,data=faithful)
plot(fhat,neval=250,plot.errors.method="bootstrap")
```

## Smooth Joint Density Estimation

- So far we have considered univariate densities ($X\in\mathbb{R}^1$)

- Let $X\in \mathbb{R}^q$ denote a numeric vector of dimension $q$

- Let $f(x)=f(x_1,x_2,\dots,x_q)$ denote a joint PDF evaluated at $x=(x_1,x_2,\dots,x_q)'$

- Let $\{X_1,X_2,\dots,X_n\}$ represent $n$ draws of a numeric random vector $X$, where the $i$th draw is denoted by $X_i=(X_{i1},X_{i2},\dots,X_{iq})$

## Smooth Joint Density Estimation

- The multivariate kernel density estimator is
\begin{equation*}
\hat f(x)=\frac{1}{nh_1\dots h_q}\sum_{i=1}^n K\left(\frac{x_1-X_{i1}}{h_1},\dots,\frac{x_q-X_{iq}}{h_q}\right)
\end{equation*}
  
- $K(\cdot)$ is a *multivariate* kernel, and one popular and tractable choice of kernel function is the product kernel given by the simple product of univariate kernels,
    \begin{equation*}
      K\left(\frac{x_1-X_{i1}}{h_1}\right)\times\dots\times K\left(\frac{x_q-X_{iq}}{h_q}\right)
    \end{equation*}

- Typically this the *product* of the same univariate kernel we already used - we have lots of flexibility here

## Smooth Joint Density Estimation

- In the multivariate case, we can show that
    \begin{equation*}
      \operatorname{bias}\hat f(x) \approx \sum_{s=1}^q \frac{h_s^2}{2}f_{ss}(x)\kappa_2
    \end{equation*}
     based on a *second order kernel* ($0< \kappa_2 = \int z^2 K(z)\, dz<\infty$) where $f_{ss}(x)$ is the second-order derivative of $f(x)$ with respect to $x_s$
     
- We can also show that 
    \begin{equation*}
      \operatorname{var} \hat f(x)\approx \frac{f(x)\kappa^q}{n\prod_{s=1}^qh_s}
    \end{equation*}
    
- Bias falls and variance increases as $h_s$ decreases, $s=1,\dots,q$ (just as when $q=1$, the univariate case)

## Smooth Joint Density Estimation

- The smooth multivariate estimator is a straightforward extension of the smooth univariate estimator

- The main differences to be aware of are that

    - there is one bandwidth to be computed for each variable (i.e., $h_1,h_2,\dots,h_q$)
    
    - the rate of convergence depends on the number of numeric variables involved (the "curse-of-dimensionality")
  
- We use data-driven methods for bandwidth selection when $q>1$ (e.g., likelihood and least-squares cross-validation) 

- Since optimal bandwidths typically differ across variables ($h_1\ne h_2$, etc.) we conduct $q$-dimensional numerical optimization

## Smooth Joint Density Estimation

```{r npudenseruptionswaiting}
#| echo: true
library(np)
data(faithful)
fhat <- npudens(~eruptions+waiting,data=faithful)
plot(fhat,theta=330,xtrim=-0.05,neval=75,view="fixed",main="")
```

# Nonparametric Essentials <br> Categorical Data

## Categorical Variables

- We considered the density of continuously distributed random variables $X\in\mathbb{R}$, but we also deal with the probability function of categorical variables $X\in\mathcal{D}$ where $\mathcal{D}$ is a discrete set

- We might presume a parametric model for the probability function of $X\in\mathcal{D}$ but we face exactly the same issue we faced before

- The non-smooth categorical counterpart to the histogram is called the *frequency* or *empirical* probability estimator and, like the histogram, has some drawbacks

## Univariate Probability Estimation

- Let $X\in\mathcal{D}=\{0,1,\dots,c-1\}$ be categorical

- $X$ can be *unordered* (`factor()` in R) or *ordered* (`ordered()` in R)

- The  nonparametric *frequency* estimator of $p(x)$ is
\begin{align*}
  p_n(x)&=\frac{\# X_i\text{ equal to }x}{n}\\
  &=\frac{1}{n}\sum_{i=1}^n \mathbf{1}(X_i=x)
\end{align*}

## Univariate Probability Estimation

- The *unordered* nonparametric kernel estimator of $p(x)$ is
\begin{equation*}
  \hat p(x)=\frac{1}{n}\sum_{i=1}^n L(X_i,x,\lambda)
\end{equation*}

- $L(\cdot)$ is an *unordered* kernel function given by
\begin{equation*}
  L(X_i,x,\lambda)=\left\{
    \begin{array}{ll}
      1-\lambda & \mbox{ if } X_i=x\\
      \lambda/(c-1) & \mbox{ otherwise}
    \end{array}
  \right.
\end{equation*}

- $\lambda$ is a *smoothing parameter* (counterpart to *bandwidth* $h$) that can be selected using the same data-driven methods already considered

## Univariate Probability Estimation

- The pointwise bias is 
    \begin{equation*}
      \operatorname{bias}\hat p(x)=\lambda\left\{\frac{1-cp(x)}{c-1}\right\}
    \end{equation*}

- Note that the bias falls as $\lambda$ decreases

- The pointwise variance, used to construct asymptotic confidence intervals by replacing $p(x)$ with $\hat p(x)$, is
    \begin{equation*}
      \operatorname{var}\hat p(x)=\frac{p(x)(1-p(x))}{n}\left(1-\lambda\frac{c}{(c-1)}\right)^2
    \end{equation*}
    
-  Note that the variance rises as $\lambda$ decreases

## Example - Unordered Probability

```{r npudensfactor}
#| echo: true
n <- 250
set.seed(42)
sex <- sample(c("Female","Male"),n,replace=TRUE,prob=c(.4,.6))
sex <- factor(sex)
phat <- npudens(~sex)
plot(phat,plot.errors.method="bootstrap")
```

## Univariate Probability Estimation

- Let $X\in\mathcal{D}=\{0,1,\dots,c-1\}$, $c\ge 2$, be *ordered*

- The *ordered* nonparametric kernel estimator of $p(x)$ is
\begin{equation*}
  \hat p(x)=\frac{1}{n}\sum_{i=1}^n l(X_i,x,\lambda)
\end{equation*}

- $l(X_i,x,\lambda)$ is an *ordered* kernel function given by
\begin{equation*}
l(X_i,x,\lambda)=\frac{\lambda^{d_{xi}}}{\Lambda_i}
\end{equation*}

- See @racine_2019 for technical details (e.g., bias and variance formulas)

## Example - Ordered Probability

```{r npudensordered}
#| echo: true
n <- 250
set.seed(42)
income <- sample(c("Low","Middle","High"),n,replace=TRUE,prob=c(.3,.5,.2))
income <- ordered(income,levels=c("Low","Middle","High"))
phat <- npudens(~income)
plot(phat,plot.errors.method="bootstrap")
```

# Nonparametric Essentials <br> Mixed Data

## Mixed Data Density Estimation

- Statisticians know that $f(x)$ and $p(x)$ are both called *density* functions (the difference lies in their *measure* - the latter uses *counting* measure), so we adopt $f(x)$ 

- Suppose you have a joint density defined over mixed data types, say, one numeric ($X^c\in\mathbb{R}$) and one unordered ($X^d\in\mathcal{D}$ with cardinality $c$)

- We would like to model their joint density function $f(x)=f(x^c,x^d)$, where the superscripts $^c$ and $^d$ denote continuous and discrete data types, respectively, and where $x=(x^c,x^d)\in\mathbb{R}\times\mathcal{D}$

## Mixed Data Density Estimation

- The kernel estimator of $f(x^c,x^d)$ is 
\begin{equation*}
  \hat f(x^c,x^d)=\frac{1}{n}\sum_{i=1}^n \frac{1}{h}K\left(\frac{X_i^c-x^c}{h}\right)L(X_i^d,x^d,\lambda)
\end{equation*}

- $K(\cdot)$ and $L(\cdot)$ are (univariate) *numeric* and *unordered* kernel functions, respectively

- If you had one numeric and one *ordered* variable you would use the ordered kernel $l(X_i^d,x^d,\lambda)$ above

- Now let's consider the *general* multivariate mixed data density case

## Mixed Data Density Estimation

-  In general multivariate settings, the probability density function $f(x)$ might use some combination of $q$ numeric, $r$ unordered, and $s$ ordered variable types ($x$ and $X_i$ are $q+r+s$-vectors)

- The (product) kernel for estimating the joint density function is
\begin{equation*}
\prod_{j=1}^qh^{-1}_jK\left(\frac{x^c_j-X^c_{ij}}{h_j}\right)\prod_{j=1}^r L(X^u_{ij},x^u_j,\lambda^u_j)\prod_{j=1}^s l(X^o_{ij},x^o_j,\lambda^o_j)
\end{equation*}

- All we are doing here is using the *appropriate kernel function* for each data type

## Mixed Data Density Estimation

- Let $\gamma=(h_1,\dots,h_q,\lambda^u_1,\dots,\lambda^u_r,\lambda^o_1,\dots,\lambda^o_s)$

- Call the product kernel function on the previous slide the *generalized* kernel, i.e., let the expression be written as
\begin{equation*}
K_\gamma(X_i,x)=\prod_{j=1}^q[\dots]\prod_{j=1}^r[\dots]\prod_{j=1}^s[\dots]
\end{equation*}

- With $x$ and $X_i$ being $q+r+s$ vectors, we write $\hat f(x)$ as 
\begin{equation*}
\hat f(x)=\frac{1}{n}\sum_{i=1}^nK_{\gamma}(X_i,x)
\end{equation*}

- See @racine_2019 for technical details (variance is the same multivariate formula given on a previous slide, use that for CIs)

## Mixed Data Density Estimation

- The key point is that *once you cast your data*, then the R functions in the `np` package know *exactly* what to do 

- They *automatically* use the appropriate kernel for the appropriate data type (numeric, factor, ordered)

- The data-driven bandwidth methods adjust automatically to the data type

- Methods for inference (confidence intervals, significance testing in regression) do the same

- Let's consider a quick example

## Example - Mixed Data Density

- We use Wooldridge's `wage1` data and consider two variables, one numeric (`lwage`) and one ordered (`numdep`)

- `lwage` is the logarithm of an individual's average hourly earnings, and `numdep` is their number of dependents

- The number of observations in each *cell* is tabulated in @tbl-wage1mixedtable

  ```{r wage1mixedtable}
  #| label: tbl-wage1mixedtable
  #| tbl-cap: Counts of number of dependants present in 526 households by cell
  library(np)
  library(plot3D)
  data(wage1)
  knitr::kable(with(wage1,t(data.frame(numdep=sort(unique(numdep)),counts=as.numeric(table(numdep))))),
               booktabs=TRUE,
               linesep="")
  ```
  
- We estimate $\hat f(lwage,numdep)$ and plot it in @fig-wage1mixeddensity

## Example - Mixed Data Density

- Below is the R code to estimate and plot the joint density (we use the `plot3D` package and reformat the data to render this plot)

  ```{r wage1mixeddensitycode}
  #| echo: true
  #| eval: false
  library(np)
  library(plot3D)
  data(wage1)
  numdep.seq <- with(wage1,sort(unique(numdep)))
  lwage.seq <- with(wage1,seq(min(lwage),max(lwage),length=50))
  wage1.eval <- expand.grid(numdep=ordered(numdep.seq),lwage=lwage.seq)
  bw <- npudensbw(~lwage+ordered(numdep),data=wage1)
  fhat <- fitted(npudens(bws=bw,newdata=wage1.eval))
  ## Hack since scatter3D converts ordered 0-6 to numeric 1-7
  scatter3D(as.numeric(wage1.eval[,1])-1,wage1.eval[,2],fhat,
            ylab="Log-Wage",
            xlab="Number of Dependants",
            zlab="Joint Density",
            ticktype="detailed",
            angle=15,
            box=TRUE,
            type="h",
            grid=TRUE,
            col="blue",
            colkey=FALSE)
  ```

## Example - Mixed Data Density

```{r wage1mixeddensity}
#| label: fig-wage1mixeddensity
#| fig-cap: Mixed-data bivariate kernel density estimate for the joint PDF of lwage (numeric) and numdeps (ordered)
data(wage1)
bw <- npudensbw(~lwage+ordered(numdep),data=wage1)
numdep.seq <- with(wage1,sort(unique(numdep)))
lwage.seq <- with(wage1,seq(min(lwage),max(lwage),length=50))
wage1.eval <- expand.grid(numdep=ordered(numdep.seq),lwage=lwage.seq)
fhat <- fitted(npudens(bws=bw,newdata=wage1.eval))
## Hack since scatter3D converts ordered 0-6 to numeric 1-7
scatter3D(as.numeric(wage1.eval[,1])-1,wage1.eval[,2],fhat,
          ylab="Log-Wage",
          xlab="Number of Dependants",
          zlab="Joint Density",
          ticktype="detailed",
          angle=15,
          box=TRUE,
          type="h",
          grid=TRUE,
          col="blue",
          colkey=FALSE)
```

